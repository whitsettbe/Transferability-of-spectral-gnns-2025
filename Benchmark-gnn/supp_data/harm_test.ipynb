{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b4e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..')) # or the path to your source code\n",
    "sys.path.insert(0, module_path)\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6917dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] Loading dataset ZINC...\n",
      "train, test, val sizes : 10000 1000 1000\n",
      "[I] Finished loading.\n",
      "[I] Data load time: 5.5135s\n"
     ]
    }
   ],
   "source": [
    "from data.data import LoadData\n",
    "\n",
    "dataset = LoadData(\"ZINC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44912ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0be05cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_atom_type = dataset.num_atom_type\n",
    "hidden_dim = 106  # Updated to match the new configuration\n",
    "embedding = nn.Embedding(num_atom_type, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19d7db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset.train, batch_size=len(dataset.train), shuffle=True, collate_fn=dataset.collate)\n",
    "test_loader = DataLoader(dataset.test, batch_size=len(dataset.test), shuffle=False, collate_fn=dataset.collate)\n",
    "val_loader = DataLoader(dataset.val, batch_size=len(dataset.val), shuffle=False, collate_fn=dataset.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24b17616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_info_hash(num_nodes, edge_index):\n",
    "    edge_tuple = tuple(sorted(zip(edge_index[0].tolist(), edge_index[1].tolist())))\n",
    "    return hash((num_nodes, edge_tuple))\n",
    "\n",
    "def graph_hash(g):\n",
    "    # Create hash from edges and number of nodes\n",
    "    edges = g.edges()\n",
    "    num_nodes = g.num_nodes()\n",
    "    return graph_info_hash(num_nodes, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71be8b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a precomputed eigenvector dataset using metis_import\n",
    "from supp_data.molecules import metis_import\n",
    "eig_dict = dict()\n",
    "metis_import(eig_dict, graph_info_hash,\n",
    "             {\"train\": \"supp_data/molecules/zinc_train_rec_full.csv\",\n",
    "              \"test\": \"supp_data/molecules/zinc_test_rec_full.csv\",\n",
    "              \"val\": \"supp_data/molecules/zinc_val_rec_full.csv\"},\n",
    "             num_eigs=15,\n",
    "             fixMissingPhi1=False,\n",
    "             extraOrtho=True)\n",
    "\n",
    "# eigenvectors are stored as nxe (technically, stored as (evals, evecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "981f613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eigenvectors(g, normalized_laplacian=False, eigval_norm=\"\", num_eigs=64, eigmod=\"\"):\n",
    "    # NOTE: this uses L, not the normalized laplacian (maybe should be normalized?)\n",
    "    adj = g.adjacency_matrix().to_dense()\n",
    "    if normalized_laplacian:\n",
    "        d_12 = torch.pow(adj.sum(dim=1), -0.5).view(1,-1)\n",
    "        laplacian = torch.eye(adj.size(0)) - d_12 * adj * d_12.T\n",
    "    else:\n",
    "        laplacian = torch.diag(adj.sum(dim=1)) - adj\n",
    "    torch_eigenvalues, torch_eigenvectors = torch.eig(laplacian, eigenvectors=True)\n",
    "\n",
    "    # Sort on real component of eigenvalues, and remove complex part (should be 0)\n",
    "    sort_indices = torch_eigenvalues[:, 0].argsort()\n",
    "    torch_eigenvectors = torch_eigenvectors[:, sort_indices]\n",
    "    torch_eigenvalues = torch_eigenvalues[:,0][sort_indices]\n",
    "\n",
    "    if eigval_norm == \"scale(-1,1)_all\":\n",
    "        torch_eigenvalues = torch_eigenvalues / torch_eigenvalues.max()\n",
    "        torch_eigenvalues = 2 * torch_eigenvalues - 1\n",
    "    elif eigval_norm == \"scale(0,2)_all\":\n",
    "        torch_eigenvalues = torch_eigenvalues / torch_eigenvalues.max()\n",
    "        torch_eigenvalues = 2 * torch_eigenvalues\n",
    "\n",
    "    # Remove high-end spectrum\n",
    "    torch_eigenvectors = torch_eigenvectors[:, :num_eigs].to(g.device)\n",
    "    torch_eigenvalues = torch_eigenvalues[:num_eigs].to(g.device)\n",
    "    \n",
    "    if eigval_norm == \"scale(0,2)_sub\":\n",
    "        # Scale the eigenvalues to [0, 2] range\n",
    "        torch_eigenvalues = torch_eigenvalues / torch_eigenvalues.max()\n",
    "        torch_eigenvalues = 2 * torch_eigenvalues\n",
    "    \n",
    "    # Replace the eigenvectors with a random orthonormal basis for the eigenspace, if requested\n",
    "    if eigmod == \"rand_basis\":\n",
    "        # Generate random vectors\n",
    "        r = torch.randn((torch_eigenvectors.size(0), num_eigs), device=g.device)\n",
    "        # Project into the eigenspace\n",
    "        r = torch_eigenvectors @ (torch_eigenvectors.T @ r)\n",
    "        # Orthonormalize\n",
    "        for i in range(r.size(1)):\n",
    "            for j in range(i):\n",
    "                r[:, i] -= torch.dot(r[:, i], r[:, j]) * r[:, j]\n",
    "            norm = torch.norm(r[:, i])\n",
    "            if norm > 0:\n",
    "                r[:, i] = r[:, i] / norm\n",
    "        torch_eigenvectors = r\n",
    "                \n",
    "        # Generate eigenvalues as Rayleigh quotients\n",
    "        quots = torch.einsum('ze,nz,ne->e', r, laplacian, r)\n",
    "        if eigval_norm == \"scale(-1,1)_all\":\n",
    "            quots = quots / quots.max()\n",
    "            quots = 2 * quots - 1\n",
    "        elif eigval_norm in [\"scale(0,2)_all\", \"scale(0,2)_sub\"]:\n",
    "            quots = quots / quots.max()\n",
    "            quots = 2 * quots\n",
    "        torch_eigenvalues = quots[:num_eigs]\n",
    "    \n",
    "    # Pad with trailing zeros\n",
    "    if torch_eigenvectors.size(1) < num_eigs:\n",
    "        num_missing = num_eigs - torch_eigenvectors.size(1)\n",
    "        vec_padding = torch.zeros((torch_eigenvectors.size(0), num_missing), device=g.device)\n",
    "        val_padding = torch.zeros((num_missing,), device=g.device)\n",
    "        torch_eigenvectors = torch.cat((torch_eigenvectors, vec_padding), dim=1)\n",
    "        torch_eigenvalues = torch.cat((torch_eigenvalues, val_padding), dim=0)\n",
    "\n",
    "    # Save the computation results and return\n",
    "    return (torch_eigenvalues.detach(), torch_eigenvectors.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dcd3451d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsw38/.conda/envs/benchmark_gnn_2/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/bsw38/.conda/envs/benchmark_gnn_2/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "regr = svm.SVR()\n",
    "for iter, (batch_graphs, batch_targets) in enumerate(train_loader):\n",
    "    batch_x = batch_graphs.ndata['feat']\n",
    "    batch_e = batch_graphs.edata['feat']\n",
    "    \n",
    "    batch_x = embedding(batch_x)\n",
    "    \n",
    "    # split the batch by graph (each entry is nxf)\n",
    "    batch_x = batch_x.split(tuple(batch_graphs.batch_num_nodes()))\n",
    "    graphs = dgl.unbatch(batch_graphs)\n",
    "    \n",
    "    # transform to eigencomponents (exf)\n",
    "    #print(eig_dict[graph_hash(g)][1].shape)\n",
    "    #evecs = [eig_dict[graph_hash(g)][1] for g in graphs]\n",
    "    evecs = [get_eigenvectors(g, num_eigs=15)[1] for g in graphs]\n",
    "    batch_x = [evec.T @ x for evec, x in zip(evecs, batch_x)]\n",
    "    \n",
    "    # regress on the graphs, flattening each set of features\n",
    "    batch_x = [x.flatten() for x in batch_x]\n",
    "    batch_x = torch.stack(batch_x, dim=0)\n",
    "    regr.fit(batch_x.detach().numpy(), batch_targets.detach().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "80c851e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter, (batch_graphs, batch_targets) in enumerate(test_loader):\n",
    "    batch_x = batch_graphs.ndata['feat']\n",
    "    batch_e = batch_graphs.edata['feat']\n",
    "    \n",
    "    batch_x = embedding(batch_x)\n",
    "    \n",
    "    # split the batch by graph (each entry is nxf)\n",
    "    batch_x = batch_x.split(tuple(batch_graphs.batch_num_nodes()))\n",
    "    graphs = dgl.unbatch(batch_graphs)\n",
    "    \n",
    "    # transform to eigencomponents (exf)\n",
    "    #print(eig_dict[graph_hash(g)][1].shape)\n",
    "    #evecs = [eig_dict[graph_hash(g)][1] for g in graphs]\n",
    "    evecs = [get_eigenvectors(g, num_eigs=15)[1] for g in graphs]\n",
    "    batch_x = [evec.T @ x for evec, x in zip(evecs, batch_x)]\n",
    "    \n",
    "    # regress on the graphs, flattening each set of features\n",
    "    batch_x = [x.flatten() for x in batch_x]\n",
    "    batch_x = torch.stack(batch_x, dim=0)\n",
    "    \n",
    "    predictions = regr.predict(batch_x.detach().numpy())\n",
    "    targets = batch_targets.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d3e9d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = predictions - targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27c06549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8087695249993212"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(errs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4467d294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

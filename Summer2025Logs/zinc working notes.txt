old ds, lr=0.01 instead of lr=0.001 (seemed to be learning, just slowly)

and another running with same lr on zinc_big (not cmd-line parametrized yet)

now going zinc_big lr=0.001 with 300 continuous features, no gpu

lr=0.001, zinc subset, single input feature

dropout=0.5

biases=True [17: 55646672]

group_by='feat' [16: 55650406]

switched to train on mse, eval on mae [17: 55650983]

skip inverse transform [16: 55652099]

brief experiment: training stdev of predictions drops to 0.1 after 3 epochs (signal is ~10^0) so... it's just predicting 0

weight loss by 1+abs(y)*(100-epoch) [17: 55655967]

torch_detach_eigs [17: 55667574]

updatePeriod 10 > 100 [did poorly - stdev dropped pretty rapidly]

Augmenting the MLP model used in https://arxiv.org/pdf/2402.10793v2 [18: 55685991]

same but with 16 layers instead of 4 [18: 55697837]

and 4 layers, but with random instead of eigenvectors [19: 55700450]

now with basic kway partitions [19: 55730221]

{seed was 41 above}

seed 42: GT [18: 55735263], random [19: 55735335], kway [20: 55735415]
seed 43: GT [18: 55755576], random [19: 55755611], kway [20: 55755634]
seed 44: GT [21: 55755657], random [22: 55755731], kway [23: 55755732]

Base run of ChebNet on ZINC [24: 55772391]
ChebNet modified for eigenvectors, parallel features [24: 55774435]

benchmarked MLP, seed 41, orthognormalized random vectors [23: 55799558]

ChebNet 0.25 dropout [24: 55799839]

rerun ChebNet, no dropout, with eigenvectors explicitly detached [24: 55815600]

Eigenvalue-based filter layer [25: 55817790]
Same, but without activation at the end of the kernel construction [25: 55840082]
Building entries of filter matrix as parallel MLPs, each acting on only one value [26: 55841133]
Same, corrected to have bias initialized as 0 [26: 55843679]
Now trying 3-term polynomial filtering in the spectral domain [25: 55845187]

just trying... polynomial for NORMALIZED Laplacian [26: 55848748]

standard Laplacian, polynomial vector as layer (allows feature mixing) [26: 55857091]
same but with the normalized Laplacian, shifting eigenvalues into -1..1 [25: 55858597]
normalized Laplacian, scaling into -1..1, Chebyshev vectors [27: 55859166]
standard Laplacian, eigenvalues -1..1, Chebyshev vectors [28: 55863463]
same as 55859166 but with Chebyshev for 0..2 [28: 55892519]
same but with 40 eigenvectors [28: 55907368]

basic ChebNet, seed 42 [28: 55909350]
basic ChebNet, seed 41, no residual [27: 55909380]

ChebAugmented (i.e. also receives an arbitrary lambda filter on low-freq components) [29: 55920338]

normalized, Chebyshev 0..2, 40 eigenvectors, fixed to include biases now (adding spatially) and FIXED OPTIMIZER [28: 55931094]
\same but with no residual (also 8 cpus, 8GB per) [28: 55937047]
 \same but with only 15 eigenvectors [27: 55937055]
  \same but with standard Laplacian [30: 55937069]
\same but with k=2 (like chebnet, which apparently I forgot) [28: 55937687]
 \same but with fixed parameter initialization to match chebnet Linear [28: 55947000]
  \same but with no residual [28: 55959235]
   \same but with seed 42 [28: 55974585]
  \same but with standard Laplacian [28: 55980132]
   \same but with "post-normalization" of signals [28: 55984699] (.43)
    \same but with k=10 [28: 55987834]
     \same but with l1_reg=0.5e-4, l2_reg=0.5e-3 [28: 55994319]
      \same but with 15 eigs [27: 55994513]
     \same but with l1_reg=1e-6, l2_reg=1e-4, sparse_vec [30: 56030853] (0.432/0.352)
      \same but with 15 eigs [31: 56033018]
      \same but with fixed initialization [30: 56074331] (0.467/0.391)
       \same but with 15 eigs [31: 56074362] (0.482/0.383)
    \same but with rational filter (k=4) [32: 56074291] (0.414/0.331)
     \same but with 15 eigs [33: 56074309] (0.472/0.376)
     \same but with metis recursive harmonics [33: 56472692] (0.667/0.593)
      \same but gen_reg reduced 5->1 [33: 56540140] (0.676/0.601)
      \same but with 15 eigs [32: 56540487] (0.802/0.716)
       \same but with gen_reg reduced 5->1 [31: 56540519] (0.751/0.674)
       \same but with normalized Laplacian for Rayleigh quotients, no rescaling [30: 56541353] (0.694/0.636)
        \same but rescaling of the eig subset (fixed bug in scale(0,2)_all) [32: 56574172] (0.808/0.744)
        \same but gen_reg reduced 5->1 [33: 56573437] (0.694/0.627)
         \same but explicitly orthogonalizing metis vectors [33: 56597183] (0.679/0.622)
     \same but with 15 random low vectors, gen_reg 5->1, normalized laplacian [32: 56599720] (0.603/0.518)
     \same but with metis recursive harmonics (fixed Laplacian calculation), normalized Laplacian [33: 56613335] (0.657/0.568)
      \same but with standard Laplacian, scale02_all, postnormalization, no extra orthogonalization [32: 56619240] (0.630/0.454)
     \same but with metis kway, standard Laplacian, postnorm, scale02_sub, no extra ortho [33: 56621668] (0.704/0.489)
      \same but with normalized laplacian [31: 56621679] (0.684/0.558)
     \same but with ChebAugmented, normalized laplacian, metis 15 [34: 56637298] (0.650/0.583)
      \same but with metis k-way [35: 56637304] (0.688/0.563)
     \same but with regularization 1, normalized, 15 eigs [33: 56653398] (0.457/0.363)
     \exactly the same, full metis recursive, 64-bit (no extraOrtho) [32: 57072305] (0.59/0.432 incomplete)
      \same but no extra phi1 [46: 57116249] (0.558/0.393)
       \same but reg 0.5 instead of 0.05 [46: 57142744] (crashed)
       \same but reg 2.0 [47: 57142762] (0.555/0.444)
       \same but reg 8.0 [46: 57160499] (0.580/0.462)
       \same but reg 32.0 [47: 57160509] (timeout; 0.640/0.507)
       \same but with haar wavelets [48: 57117423] (0.558/0.441)
        \same but reg 0.1 instead of 0.05 [49: 57117475] (0.544/0.414)
        \same but reg 0.5 [48: 57142808] (0.555/0.432)
        \same but reg 2.0 [49: 57142821] (crashed)
        \same but reg 8.0 [48: 57160526] (0.552/0.451)
        \same but reg 32.0 [49: 57160555 - CHANGED IN .sh!] (timeout; 0.816/0.505)
       \same but only bottom 16 eigenvectors (properly ordered), no reg [47: 57629127] (0.584/0.413)
        \same but with normalized Laplacian directly [47: 57638916] (0.647/0.558)
        \same but with subset scaled to 0..2 [47: 57650029] (0.589/0.434)
        \same but with no residual [48: 57650107] (0.605/0.418)

full GT chebyshev filter, standard laplacian postnorm, cheb02, scale02_all [50: 57666520] (0.427/0.347)
\same but fixed double precision [50: 57696011] (0.427/0.357)
\same but partial spectrum (16), still scale02_all [51: 57695869] (0.463/0.381)
 \same but metis (changed in .sh!) [52: 57666742] (0.569/0.427)
 \same but random basis (no extra orthogonalization, but forgot to normalize) [50: 57722082] (0.719/0.563)
  \same but with orthogonalization [51: 57722153] (0.581/0.449)
  \same but just with normalization [50: 57723601] (0.711/0.525)
\same but metis (changed in .sh!) [52: 57722193] (0.541/0.412)
\same but random basis (not orthogonalized) [51: 57723656] (0.689/0.518)
 \same but orthogonalized [52: 57723702] (0.632/0.536)

poly_mlp filter (3x100), k=6, standard Laplacian postnorm, scale02_all, 16 eigenvectors, metis [50: 57931988] (.604/.440)
\same but k=2, 0x2 [50: 57955708] (0.595/0.446)
 \same but only 4 cpus, down from 8 (changed in .sh, because where else?) [51: 57955726] (0.595/0.446 - expected)
  \same but 0x100 [51: 57978952] (0.591/0.455)
   \same but no residual [50: 58016592] (.641/.492)
   \same but hidden dimension 53<106 [51: 58016613] (.603/.502)
   \same but l2 reg 5e-4 [52: 58017822] (.595/.458)
    \same but also l1 reg 1e-5 [51: 58043947] (.590/.425)
   \same but l1 reg 1e-5 [50: 58043899] (.593/.459)
   \same but scale02_sub [52: 58044029] (.610/.393)
    \same but l1 reg 1e-5, l2 reg 5e-4 [50: 58084966] (.616/.535)
    \same but l1 reg 1e-4, l2 reg 5e-3 [51: 58085010] (.688/.627)
    \same but 3x100, k=4 [52: 58085016] (.602/.456)
   \same but batch_norm False [53: 58044145] (.582/.510)
  \same but 1x100 [50: 58008088] (.613/.439)
  \same but 2x100 [51: 58008199] (.598/.440)
  \same but 3x100 [52: 58008210] (.602/.449)
  
cheb02 filter, k=4, standard w/ postnorm, scale02_sub, 16 eigs, metis, L=16 [50: 58310989] (.642/.333)
\same but k=2 [51: 58311102] (.591/.371)
\same but poly_mlp 3x100 [52: 58322127] (.609/.315)
\same but l1 reg 1e-5 [50: 58322346] (.630/.358)
\same but l2 reg 0.5e-3 [51: 58322399] (.643/.340)
\same but l1 reg 1e-4 [50: 58344147] (0.633/.448)
\same but l2 reg 0.5e-2 [51: 58360568] (.641/.438)
 \same but also l1 reg 1e-4 [51: 58429821] (.624/.492)
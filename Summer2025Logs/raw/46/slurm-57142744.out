I'm echoing to stdout
I'm echoing to stderr
My JobID is 57142744
I have 8 CPUs on node r108u25n01
Using backend: pytorch
cuda not available
[I] Loading dataset ZINC...
train, test, val sizes : 10000 1000 1000
[I] Finished loading.
[I] Data load time: 5.0382s
Dataset: ZINC,
Model: EigvalFilters

params={'seed': 41, 'epochs': 1000, 'batch_size': 128, 'init_lr': 0.001, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 5, 'min_lr': 1e-05, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 48}

net_params={'L': 4, 'hidden_dim': 106, 'out_dim': 106, 'residual': True, 'readout': 'mean', 'k': 4, 'in_feat_dropout': 0.0, 'dropout': 0.0, 'graph_norm': True, 'batch_norm': True, 'self_loop': False, 'subtype': 'rational_vec', 'normalized_laplacian': False, 'post_normalized': True, 'eigval_norm': 'scale(0,2)_all', 'num_eigs': 64, 'bias_mode': 'spatial', 'eigval_hidden_dim': 5, 'eigval_num_hidden_layer': 3, 'l1_reg': 0.0, 'l2_reg': 0.0, 'gen_reg': 0.5, 'eigmod': 'import_csv', 'eigInFiles': {'train': 'supp_data/molecules/zinc_train_rec_full.csv', 'test': 'supp_data/molecules/zinc_test_rec_full.csv', 'val': 'supp_data/molecules/zinc_val_rec_full.csv'}, 'fixMissingPhi1': False, 'extraOrtho': False, 'doublePrecision': True, 'device': device(type='cpu'), 'gpu_id': 0, 'batch_size': 128, 'biases': False, 'num_atom_type': 28, 'num_bond_type': 4, 'total_param': -1}


Total Parameters: -1


Training Graphs:  10000
Validation Graphs:  1000
Test Graphs:  1000
  0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [09:36<?, ?it/s, lr=0.001, test_MAE=1.47, time=577, train_MAE=1.38, train_loss=1.4, val_MAE=1.38, val_loss=1.4]Epoch 0:   0%|          | 1/1000 [09:36<160:06:45, 576.98s/it, lr=0.001, test_MAE=1.47, time=577, train_MAE=1.38, train_loss=1.4, val_MAE=1.38, val_loss=1.4]Epoch 1:   0%|          | 1/1000 [09:36<160:06:45, 576.98s/it, lr=0.001, test_MAE=1.47, time=577, train_MAE=1.38, train_loss=1.4, val_MAE=1.38, val_loss=1.4]Epoch 1:   0%|          | 1/1000 [19:33<160:06:45, 576.98s/it, lr=0.001, test_MAE=1.47, time=596, train_MAE=1.01, train_loss=1.03, val_MAE=1.37, val_loss=1.39]Epoch 1:   0%|          | 2/1000 [19:33<161:32:30, 582.72s/it, lr=0.001, test_MAE=1.47, time=596, train_MAE=1.01, train_loss=1.03, val_MAE=1.37, val_loss=1.39]Epoch 2:   0%|          | 2/1000 [19:33<161:32:30, 582.72s/it, lr=0.001, test_MAE=1.47, time=596, train_MAE=1.01, train_loss=1.03, val_MAE=1.37, val_loss=1.39]Epoch 2:   0%|          | 2/1000 [29:39<161:32:30, 582.72s/it, lr=0.001, test_MAE=0.928, time=607, train_MAE=0.788, train_loss=0.806, val_MAE=0.853, val_loss=0.871]Epoch 2:   0%|          | 3/1000 [29:39<163:23:03, 589.95s/it, lr=0.001, test_MAE=0.928, time=607, train_MAE=0.788, train_loss=0.806, val_MAE=0.853, val_loss=0.871]Epoch 3:   0%|          | 3/1000 [29:39<163:23:03, 589.95s/it, lr=0.001, test_MAE=0.928, time=607, train_MAE=0.788, train_loss=0.806, val_MAE=0.853, val_loss=0.871]Epoch 3:   0%|          | 3/1000 [39:50<163:23:03, 589.95s/it, lr=0.001, test_MAE=0.823, time=610, train_MAE=0.7, train_loss=0.718, val_MAE=0.75, val_loss=0.769]   Epoch 3:   0%|          | 4/1000 [39:50<164:54:53, 596.08s/it, lr=0.001, test_MAE=0.823, time=610, train_MAE=0.7, train_loss=0.718, val_MAE=0.75, val_loss=0.769]Epoch 4:   0%|          | 4/1000 [39:50<164:54:53, 596.08s/it, lr=0.001, test_MAE=0.823, time=610, train_MAE=0.7, train_loss=0.718, val_MAE=0.75, val_loss=0.769]Epoch 4:   0%|          | 4/1000 [50:00<164:54:53, 596.08s/it, lr=0.001, test_MAE=1.16, time=611, train_MAE=0.674, train_loss=0.693, val_MAE=1.12, val_loss=1.14]Epoch 4:   0%|          | 5/1000 [50:00<165:57:22, 600.44s/it, lr=0.001, test_MAE=1.16, time=611, train_MAE=0.674, train_loss=0.693, val_MAE=1.12, val_loss=1.14]Epoch 5:   0%|          | 5/1000 [50:00<165:57:22, 600.44s/it, lr=0.001, test_MAE=1.16, time=611, train_MAE=0.674, train_loss=0.693, val_MAE=1.12, val_loss=1.14]Epoch 5:   0%|          | 5/1000 [1:00:20<165:57:22, 600.44s/it, lr=0.001, test_MAE=0.769, time=620, train_MAE=0.669, train_loss=0.688, val_MAE=0.71, val_loss=0.729]Epoch 5:   1%|          | 6/1000 [1:00:20<167:24:42, 606.32s/it, lr=0.001, test_MAE=0.769, time=620, train_MAE=0.669, train_loss=0.688, val_MAE=0.71, val_loss=0.729]Epoch 6:   1%|          | 6/1000 [1:00:20<167:24:42, 606.32s/it, lr=0.001, test_MAE=0.769, time=620, train_MAE=0.669, train_loss=0.688, val_MAE=0.71, val_loss=0.729]Epoch 6:   1%|          | 6/1000 [1:10:30<167:24:42, 606.32s/it, lr=0.001, test_MAE=0.751, time=609, train_MAE=0.633, train_loss=0.653, val_MAE=0.692, val_loss=0.711]Epoch 6:   1%|          | 7/1000 [1:10:30<167:29:03, 607.19s/it, lr=0.001, test_MAE=0.751, time=609, train_MAE=0.633, train_loss=0.653, val_MAE=0.692, val_loss=0.711]Epoch 7:   1%|          | 7/1000 [1:10:30<167:29:03, 607.19s/it, lr=0.001, test_MAE=0.751, time=609, train_MAE=0.633, train_loss=0.653, val_MAE=0.692, val_loss=0.711]Epoch 7:   1%|          | 7/1000 [1:20:38<167:29:03, 607.19s/it, lr=0.001, test_MAE=0.8, time=609, train_MAE=0.633, train_loss=0.653, val_MAE=0.743, val_loss=0.763]  Epoch 7:   1%|          | 8/1000 [1:20:38<167:25:49, 607.61s/it, lr=0.001, test_MAE=0.8, time=609, train_MAE=0.633, train_loss=0.653, val_MAE=0.743, val_loss=0.763]Epoch 8:   1%|          | 8/1000 [1:20:38<167:25:49, 607.61s/it, lr=0.001, test_MAE=0.8, time=609, train_MAE=0.633, train_loss=0.653, val_MAE=0.743, val_loss=0.763]Epoch 8:   1%|          | 8/1000 [1:30:47<167:25:49, 607.61s/it, lr=0.001, test_MAE=0.936, time=609, train_MAE=0.631, train_loss=0.651, val_MAE=0.862, val_loss=0.881]Epoch 8:   1%|          | 9/1000 [1:30:48<167:23:49, 608.10s/it, lr=0.001, test_MAE=0.936, time=609, train_MAE=0.631, train_loss=0.651, val_MAE=0.862, val_loss=0.881]Epoch 9:   1%|          | 9/1000 [1:30:48<167:23:49, 608.10s/it, lr=0.001, test_MAE=0.936, time=609, train_MAE=0.631, train_loss=0.651, val_MAE=0.862, val_loss=0.881]Epoch 9:   1%|          | 9/1000 [1:41:00<167:23:49, 608.10s/it, lr=0.001, test_MAE=1.04, time=612, train_MAE=0.61, train_loss=0.63, val_MAE=0.909, val_loss=0.929]   Epoch 9:   1%|          | 10/1000 [1:41:00<167:33:19, 609.29s/it, lr=0.001, test_MAE=1.04, time=612, train_MAE=0.61, train_loss=0.63, val_MAE=0.909, val_loss=0.929]Epoch 10:   1%|          | 10/1000 [1:41:00<167:33:19, 609.29s/it, lr=0.001, test_MAE=1.04, time=612, train_MAE=0.61, train_loss=0.63, val_MAE=0.909, val_loss=0.929]Epoch 10:   1%|          | 10/1000 [1:51:11<167:33:19, 609.29s/it, lr=0.001, test_MAE=0.74, time=612, train_MAE=0.605, train_loss=0.625, val_MAE=0.694, val_loss=0.715]Epoch 10:   1%|          | 11/1000 [1:51:11<167:34:30, 609.98s/it, lr=0.001, test_MAE=0.74, time=612, train_MAE=0.605, train_loss=0.625, val_MAE=0.694, val_loss=0.715]Epoch 11:   1%|          | 11/1000 [1:51:11<167:34:30, 609.98s/it, lr=0.001, test_MAE=0.74, time=612, train_MAE=0.605, train_loss=0.625, val_MAE=0.694, val_loss=0.715]Epoch 11:   1%|          | 11/1000 [2:01:24<167:34:30, 609.98s/it, lr=0.001, test_MAE=0.681, time=613, train_MAE=0.6, train_loss=0.62, val_MAE=0.631, val_loss=0.652]  Epoch 11:   1%|          | 12/1000 [2:01:24<167:37:12, 610.76s/it, lr=0.001, test_MAE=0.681, time=613, train_MAE=0.6, train_loss=0.62, val_MAE=0.631, val_loss=0.652]Epoch 12:   1%|          | 12/1000 [2:01:24<167:37:12, 610.76s/it, lr=0.001, test_MAE=0.681, time=613, train_MAE=0.6, train_loss=0.62, val_MAE=0.631, val_loss=0.652]Epoch 12:   1%|          | 12/1000 [2:11:38<167:37:12, 610.76s/it, lr=0.001, test_MAE=0.672, time=615, train_MAE=0.604, train_loss=0.625, val_MAE=0.621, val_loss=0.642]Epoch 12:   1%|▏         | 13/1000 [2:11:38<167:46:16, 611.93s/it, lr=0.001, test_MAE=0.672, time=615, train_MAE=0.604, train_loss=0.625, val_MAE=0.621, val_loss=0.642]Epoch 13:   1%|▏         | 13/1000 [2:11:38<167:46:16, 611.93s/it, lr=0.001, test_MAE=0.672, time=615, train_MAE=0.604, train_loss=0.625, val_MAE=0.621, val_loss=0.642]Epoch 13:   1%|▏         | 13/1000 [2:21:44<167:46:16, 611.93s/it, lr=0.001, test_MAE=0.65, time=606, train_MAE=0.607, train_loss=0.627, val_MAE=0.599, val_loss=0.62]  Epoch 13:   1%|▏         | 14/1000 [2:21:44<167:06:55, 610.16s/it, lr=0.001, test_MAE=0.65, time=606, train_MAE=0.607, train_loss=0.627, val_MAE=0.599, val_loss=0.62]Epoch 14:   1%|▏         | 14/1000 [2:21:44<167:06:55, 610.16s/it, lr=0.001, test_MAE=0.65, time=606, train_MAE=0.607, train_loss=0.627, val_MAE=0.599, val_loss=0.62]Epoch 14:   1%|▏         | 14/1000 [2:31:48<167:06:55, 610.16s/it, lr=0.001, test_MAE=0.759, time=603, train_MAE=0.589, train_loss=0.61, val_MAE=0.727, val_loss=0.747]Epoch 14:   2%|▏         | 15/1000 [2:31:48<166:23:32, 608.13s/it, lr=0.001, test_MAE=0.759, time=603, train_MAE=0.589, train_loss=0.61, val_MAE=0.727, val_loss=0.747]Epoch 15:   2%|▏         | 15/1000 [2:31:48<166:23:32, 608.13s/it, lr=0.001, test_MAE=0.759, time=603, train_MAE=0.589, train_loss=0.61, val_MAE=0.727, val_loss=0.747]Epoch 15:   2%|▏         | 15/1000 [2:41:48<166:23:32, 608.13s/it, lr=0.001, test_MAE=0.673, time=600, train_MAE=0.588, train_loss=0.609, val_MAE=0.641, val_loss=0.662]Epoch 15:   2%|▏         | 16/1000 [2:41:48<165:32:45, 605.66s/it, lr=0.001, test_MAE=0.673, time=600, train_MAE=0.588, train_loss=0.609, val_MAE=0.641, val_loss=0.662]Epoch 16:   2%|▏         | 16/1000 [2:41:48<165:32:45, 605.66s/it, lr=0.001, test_MAE=0.673, time=600, train_MAE=0.588, train_loss=0.609, val_MAE=0.641, val_loss=0.662]Epoch 16:   2%|▏         | 16/1000 [2:51:54<165:32:45, 605.66s/it, lr=0.001, test_MAE=0.725, time=606, train_MAE=0.596, train_loss=0.617, val_MAE=0.665, val_loss=0.686]Epoch 16:   2%|▏         | 17/1000 [2:51:54<165:23:36, 605.71s/it, lr=0.001, test_MAE=0.725, time=606, train_MAE=0.596, train_loss=0.617, val_MAE=0.665, val_loss=0.686]Epoch 17:   2%|▏         | 17/1000 [2:51:54<165:23:36, 605.71s/it, lr=0.001, test_MAE=0.725, time=606, train_MAE=0.596, train_loss=0.617, val_MAE=0.665, val_loss=0.686]Epoch 17:   2%|▏         | 17/1000 [3:01:56<165:23:36, 605.71s/it, lr=0.001, test_MAE=0.739, time=603, train_MAE=0.581, train_loss=0.602, val_MAE=0.679, val_loss=0.7]  Epoch 17:   2%|▏         | 18/1000 [3:01:56<164:59:23, 604.85s/it, lr=0.001, test_MAE=0.739, time=603, train_MAE=0.581, train_loss=0.602, val_MAE=0.679, val_loss=0.7]Epoch 18:   2%|▏         | 18/1000 [3:01:56<164:59:23, 604.85s/it, lr=0.001, test_MAE=0.739, time=603, train_MAE=0.581, train_loss=0.602, val_MAE=0.679, val_loss=0.7]Epoch 18:   2%|▏         | 18/1000 [3:11:49<164:59:23, 604.85s/it, lr=0.001, test_MAE=0.644, time=593, train_MAE=0.586, train_loss=0.608, val_MAE=0.604, val_loss=0.626]Epoch 18:   2%|▏         | 19/1000 [3:11:49<163:50:22, 601.25s/it, lr=0.001, test_MAE=0.644, time=593, train_MAE=0.586, train_loss=0.608, val_MAE=0.604, val_loss=0.626]Epoch 19:   2%|▏         | 19/1000 [3:11:49<163:50:22, 601.25s/it, lr=0.001, test_MAE=0.644, time=593, train_MAE=0.586, train_loss=0.608, val_MAE=0.604, val_loss=0.626]Epoch 19:   2%|▏         | 19/1000 [3:21:41<163:50:22, 601.25s/it, lr=0.001, test_MAE=0.646, time=592, train_MAE=0.581, train_loss=0.603, val_MAE=0.598, val_loss=0.62] Epoch    20: reducing learning rate of group 0 to 5.0000e-04.
Epoch 19:   2%|▏         | 20/1000 [3:21:41<162:54:48, 598.46s/it, lr=0.001, test_MAE=0.646, time=592, train_MAE=0.581, train_loss=0.603, val_MAE=0.598, val_loss=0.62]Epoch 20:   2%|▏         | 20/1000 [3:21:41<162:54:48, 598.46s/it, lr=0.001, test_MAE=0.646, time=592, train_MAE=0.581, train_loss=0.603, val_MAE=0.598, val_loss=0.62]Epoch 20:   2%|▏         | 20/1000 [3:31:30<162:54:48, 598.46s/it, lr=0.0005, test_MAE=0.605, time=588, train_MAE=0.553, train_loss=0.575, val_MAE=0.571, val_loss=0.593]Epoch 20:   2%|▏         | 21/1000 [3:31:30<161:56:28, 595.49s/it, lr=0.0005, test_MAE=0.605, time=588, train_MAE=0.553, train_loss=0.575, val_MAE=0.571, val_loss=0.593]Epoch 21:   2%|▏         | 21/1000 [3:31:30<161:56:28, 595.49s/it, lr=0.0005, test_MAE=0.605, time=588, train_MAE=0.553, train_loss=0.575, val_MAE=0.571, val_loss=0.593]Epoch 21:   2%|▏         | 21/1000 [3:41:13<161:56:28, 595.49s/it, lr=0.0005, test_MAE=0.648, time=583, train_MAE=0.553, train_loss=0.574, val_MAE=0.629, val_loss=0.651]Epoch 21:   2%|▏         | 22/1000 [3:41:13<160:46:55, 591.84s/it, lr=0.0005, test_MAE=0.648, time=583, train_MAE=0.553, train_loss=0.574, val_MAE=0.629, val_loss=0.651]Epoch 22:   2%|▏         | 22/1000 [3:41:13<160:46:55, 591.84s/it, lr=0.0005, test_MAE=0.648, time=583, train_MAE=0.553, train_loss=0.574, val_MAE=0.629, val_loss=0.651]Epoch 22:   2%|▏         | 22/1000 [3:51:02<160:46:55, 591.84s/it, lr=0.0005, test_MAE=0.666, time=589, train_MAE=0.55, train_loss=0.571, val_MAE=0.61, val_loss=0.632]  Epoch 22:   2%|▏         | 23/1000 [3:51:02<160:24:51, 591.09s/it, lr=0.0005, test_MAE=0.666, time=589, train_MAE=0.55, train_loss=0.571, val_MAE=0.61, val_loss=0.632]Epoch 23:   2%|▏         | 23/1000 [3:51:02<160:24:51, 591.09s/it, lr=0.0005, test_MAE=0.666, time=589, train_MAE=0.55, train_loss=0.571, val_MAE=0.61, val_loss=0.632]Epoch 23:   2%|▏         | 23/1000 [4:00:45<160:24:51, 591.09s/it, lr=0.0005, test_MAE=0.627, time=582, train_MAE=0.552, train_loss=0.573, val_MAE=0.575, val_loss=0.597]Epoch 23:   2%|▏         | 24/1000 [4:00:45<159:32:41, 588.49s/it, lr=0.0005, test_MAE=0.627, time=582, train_MAE=0.552, train_loss=0.573, val_MAE=0.575, val_loss=0.597]Epoch 24:   2%|▏         | 24/1000 [4:00:45<159:32:41, 588.49s/it, lr=0.0005, test_MAE=0.627, time=582, train_MAE=0.552, train_loss=0.573, val_MAE=0.575, val_loss=0.597]Epoch 24:   2%|▏         | 24/1000 [4:10:21<159:32:41, 588.49s/it, lr=0.0005, test_MAE=0.637, time=576, train_MAE=0.549, train_loss=0.571, val_MAE=0.6, val_loss=0.621]  Epoch 24:   2%|▎         | 25/1000 [4:10:21<158:24:14, 584.88s/it, lr=0.0005, test_MAE=0.637, time=576, train_MAE=0.549, train_loss=0.571, val_MAE=0.6, val_loss=0.621]Epoch 25:   2%|▎         | 25/1000 [4:10:21<158:24:14, 584.88s/it, lr=0.0005, test_MAE=0.637, time=576, train_MAE=0.549, train_loss=0.571, val_MAE=0.6, val_loss=0.621]Epoch 25:   2%|▎         | 25/1000 [4:19:59<158:24:14, 584.88s/it, lr=0.0005, test_MAE=0.598, time=578, train_MAE=0.547, train_loss=0.569, val_MAE=0.555, val_loss=0.576]Epoch 25:   3%|▎         | 26/1000 [4:19:59<157:41:57, 582.87s/it, lr=0.0005, test_MAE=0.598, time=578, train_MAE=0.547, train_loss=0.569, val_MAE=0.555, val_loss=0.576]Epoch 26:   3%|▎         | 26/1000 [4:19:59<157:41:57, 582.87s/it, lr=0.0005, test_MAE=0.598, time=578, train_MAE=0.547, train_loss=0.569, val_MAE=0.555, val_loss=0.576]Epoch 26:   3%|▎         | 26/1000 [4:29:34<157:41:57, 582.87s/it, lr=0.0005, test_MAE=0.62, time=574, train_MAE=0.55, train_loss=0.572, val_MAE=0.578, val_loss=0.6]    Epoch 26:   3%|▎         | 27/1000 [4:29:34<156:50:42, 580.31s/it, lr=0.0005, test_MAE=0.62, time=574, train_MAE=0.55, train_loss=0.572, val_MAE=0.578, val_loss=0.6]Epoch 27:   3%|▎         | 27/1000 [4:29:34<156:50:42, 580.31s/it, lr=0.0005, test_MAE=0.62, time=574, train_MAE=0.55, train_loss=0.572, val_MAE=0.578, val_loss=0.6]Epoch 27:   3%|▎         | 27/1000 [4:39:08<156:50:42, 580.31s/it, lr=0.0005, test_MAE=0.593, time=574, train_MAE=0.548, train_loss=0.57, val_MAE=0.552, val_loss=0.573]Epoch 27:   3%|▎         | 28/1000 [4:39:08<156:10:11, 578.41s/it, lr=0.0005, test_MAE=0.593, time=574, train_MAE=0.548, train_loss=0.57, val_MAE=0.552, val_loss=0.573]Epoch 28:   3%|▎         | 28/1000 [4:39:08<156:10:11, 578.41s/it, lr=0.0005, test_MAE=0.593, time=574, train_MAE=0.548, train_loss=0.57, val_MAE=0.552, val_loss=0.573]Epoch 28:   3%|▎         | 28/1000 [4:48:41<156:10:11, 578.41s/it, lr=0.0005, test_MAE=0.653, time=573, train_MAE=0.541, train_loss=0.563, val_MAE=0.608, val_loss=0.63]Epoch 28:   3%|▎         | 29/1000 [4:48:41<155:36:39, 576.93s/it, lr=0.0005, test_MAE=0.653, time=573, train_MAE=0.541, train_loss=0.563, val_MAE=0.608, val_loss=0.63]Epoch 29:   3%|▎         | 29/1000 [4:48:41<155:36:39, 576.93s/it, lr=0.0005, test_MAE=0.653, time=573, train_MAE=0.541, train_loss=0.563, val_MAE=0.608, val_loss=0.63]Epoch 29:   3%|▎         | 29/1000 [4:58:25<155:36:39, 576.93s/it, lr=0.0005, test_MAE=0.682, time=584, train_MAE=0.542, train_loss=0.564, val_MAE=0.651, val_loss=0.673]Epoch 29:   3%|▎         | 30/1000 [4:58:25<156:02:08, 579.10s/it, lr=0.0005, test_MAE=0.682, time=584, train_MAE=0.542, train_loss=0.564, val_MAE=0.651, val_loss=0.673]Epoch 30:   3%|▎         | 30/1000 [4:58:25<156:02:08, 579.10s/it, lr=0.0005, test_MAE=0.682, time=584, train_MAE=0.542, train_loss=0.564, val_MAE=0.651, val_loss=0.673]Epoch 30:   3%|▎         | 30/1000 [5:08:03<156:02:08, 579.10s/it, lr=0.0005, test_MAE=0.598, time=578, train_MAE=0.536, train_loss=0.558, val_MAE=0.566, val_loss=0.587]Epoch 30:   3%|▎         | 31/1000 [5:08:03<155:46:59, 578.76s/it, lr=0.0005, test_MAE=0.598, time=578, train_MAE=0.536, train_loss=0.558, val_MAE=0.566, val_loss=0.587]Epoch 31:   3%|▎         | 31/1000 [5:08:03<155:46:59, 578.76s/it, lr=0.0005, test_MAE=0.598, time=578, train_MAE=0.536, train_loss=0.558, val_MAE=0.566, val_loss=0.587]Epoch 31:   3%|▎         | 31/1000 [5:17:48<155:46:59, 578.76s/it, lr=0.0005, test_MAE=0.652, time=585, train_MAE=0.538, train_loss=0.56, val_MAE=0.622, val_loss=0.644] Epoch 31:   3%|▎         | 32/1000 [5:17:48<156:07:44, 580.65s/it, lr=0.0005, test_MAE=0.652, time=585, train_MAE=0.538, train_loss=0.56, val_MAE=0.622, val_loss=0.644]Epoch 32:   3%|▎         | 32/1000 [5:17:48<156:07:44, 580.65s/it, lr=0.0005, test_MAE=0.652, time=585, train_MAE=0.538, train_loss=0.56, val_MAE=0.622, val_loss=0.644]Epoch 32:   3%|▎         | 32/1000 [5:27:35<156:07:44, 580.65s/it, lr=0.0005, test_MAE=0.712, time=587, train_MAE=0.531, train_loss=0.553, val_MAE=0.667, val_loss=0.689]Epoch 32:   3%|▎         | 33/1000 [5:27:35<156:27:52, 582.49s/it, lr=0.0005, test_MAE=0.712, time=587, train_MAE=0.531, train_loss=0.553, val_MAE=0.667, val_loss=0.689]Epoch 33:   3%|▎         | 33/1000 [5:27:35<156:27:52, 582.49s/it, lr=0.0005, test_MAE=0.712, time=587, train_MAE=0.531, train_loss=0.553, val_MAE=0.667, val_loss=0.689]Epoch 33:   3%|▎         | 33/1000 [5:37:24<156:27:52, 582.49s/it, lr=0.0005, test_MAE=0.671, time=588, train_MAE=0.536, train_loss=0.558, val_MAE=0.621, val_loss=0.643]Epoch    34: reducing learning rate of group 0 to 2.5000e-04.
Epoch 33:   3%|▎         | 34/1000 [5:37:24<156:46:10, 584.23s/it, lr=0.0005, test_MAE=0.671, time=588, train_MAE=0.536, train_loss=0.558, val_MAE=0.621, val_loss=0.643]Epoch 34:   3%|▎         | 34/1000 [5:37:24<156:46:10, 584.23s/it, lr=0.0005, test_MAE=0.671, time=588, train_MAE=0.536, train_loss=0.558, val_MAE=0.621, val_loss=0.643]Epoch 34:   3%|▎         | 34/1000 [5:46:56<156:46:10, 584.23s/it, lr=0.00025, test_MAE=0.619, time=573, train_MAE=0.53, train_loss=0.551, val_MAE=0.59, val_loss=0.612] Epoch 34:   3%|▎         | 34/1000 [5:46:56<164:17:26, 612.26s/it, lr=0.00025, test_MAE=0.619, time=573, train_MAE=0.53, train_loss=0.551, val_MAE=0.59, val_loss=0.612]
Traceback (most recent call last):
  File "main_molecules_graph_regression.py", line 506, in <module>
    main()
  File "main_molecules_graph_regression.py", line 503, in main
    train_val_pipeline(MODEL_NAME, dataset, params, net_params, dirs)
  File "main_molecules_graph_regression.py", line 214, in train_val_pipeline
    torch.save(model.state_dict(), '{}.pkl'.format(ckpt_dir + "/epoch_" + str(epoch)))
  File "/home/bsw38/.conda/envs/benchmark_gnn_2/lib/python3.7/site-packages/torch/serialization.py", line 361, in save
    with _open_file_like(f, 'wb') as opened_file:
  File "/home/bsw38/.conda/envs/benchmark_gnn_2/lib/python3.7/site-packages/torch/serialization.py", line 229, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/bsw38/.conda/envs/benchmark_gnn_2/lib/python3.7/site-packages/torch/serialization.py", line 210, in __init__
    super(_open_file, self).__init__(open(name, mode))
OSError: [Errno 122] Disk quota exceeded: 'out/molecules_graph_regression/checkpoints/EigvalFilters_ZINC_GPU0_41_10h34m37s_on_Aug_06_2025/RUN_/epoch_34.pkl'

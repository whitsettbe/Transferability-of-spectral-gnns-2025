I'm echoing to stdout
I'm echoing to stderr
My JobID is 56785140
I have 8 CPUs on node r108u25n01
Using backend: pytorch
cuda not available
[I] Loading dataset ZINC...
train, test, val sizes : 10000 1000 1000
[I] Finished loading.
[I] Data load time: 5.1355s
Dataset: ZINC,
Model: EigvalFilters

params={'seed': 41, 'epochs': 1000, 'batch_size': 128, 'init_lr': 0.001, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 5, 'min_lr': 1e-05, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 48}

net_params={'L': 4, 'hidden_dim': 106, 'out_dim': 106, 'residual': True, 'readout': 'mean', 'k': 2, 'in_feat_dropout': 0.0, 'dropout': 0.0, 'graph_norm': True, 'batch_norm': True, 'self_loop': False, 'subtype': 'parallel_dense_simp', 'normalized_laplacian': False, 'post_normalized': False, 'eigval_norm': 'scale(0,2)_all', 'num_eigs': 15, 'bias_mode': '', 'eigval_hidden_dim': 5, 'eigval_num_hidden_layer': 3, 'l1_reg': 0.0, 'l2_reg': 0.0, 'gen_reg': 5.0, 'eigmod': 'import_csv', 'eigInFiles': {'train': 'supp_data/molecules/zinc_train_rec_full.csv', 'test': 'supp_data/molecules/zinc_test_rec_full.csv', 'val': 'supp_data/molecules/zinc_val_rec_full.csv'}, 'fixMissingPhi1': False, 'extraOrtho': True, 'device': device(type='cpu'), 'gpu_id': 0, 'batch_size': 128, 'biases': False, 'num_atom_type': 28, 'num_bond_type': 4, 'total_param': -1}


Total Parameters: -1


Training Graphs:  10000
Validation Graphs:  1000
Test Graphs:  1000
  0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [03:00<?, ?it/s, lr=0.001, test_MAE=1.38, time=181, train_MAE=0.856, train_loss=0.957, val_MAE=1.29, val_loss=1.36]Epoch 0:   0%|          | 1/1000 [03:00<50:07:26, 180.63s/it, lr=0.001, test_MAE=1.38, time=181, train_MAE=0.856, train_loss=0.957, val_MAE=1.29, val_loss=1.36]Epoch 1:   0%|          | 1/1000 [03:00<50:07:26, 180.63s/it, lr=0.001, test_MAE=1.38, time=181, train_MAE=0.856, train_loss=0.957, val_MAE=1.29, val_loss=1.36]Epoch 1:   0%|          | 1/1000 [03:36<50:07:26, 180.63s/it, lr=0.001, test_MAE=0.726, time=36.2, train_MAE=0.673, train_loss=0.728, val_MAE=0.704, val_loss=0.752]Epoch 1:   0%|          | 2/1000 [03:36<38:03:55, 137.31s/it, lr=0.001, test_MAE=0.726, time=36.2, train_MAE=0.673, train_loss=0.728, val_MAE=0.704, val_loss=0.752]Epoch 2:   0%|          | 2/1000 [03:36<38:03:55, 137.31s/it, lr=0.001, test_MAE=0.726, time=36.2, train_MAE=0.673, train_loss=0.728, val_MAE=0.704, val_loss=0.752]Epoch 2:   0%|          | 2/1000 [04:13<38:03:55, 137.31s/it, lr=0.001, test_MAE=0.759, time=36.9, train_MAE=0.665, train_loss=0.711, val_MAE=0.688, val_loss=0.733]Epoch 2:   0%|          | 3/1000 [04:13<29:41:07, 107.19s/it, lr=0.001, test_MAE=0.759, time=36.9, train_MAE=0.665, train_loss=0.711, val_MAE=0.688, val_loss=0.733]Epoch 3:   0%|          | 3/1000 [04:13<29:41:07, 107.19s/it, lr=0.001, test_MAE=0.759, time=36.9, train_MAE=0.665, train_loss=0.711, val_MAE=0.688, val_loss=0.733]Epoch 3:   0%|          | 3/1000 [04:50<29:41:07, 107.19s/it, lr=0.001, test_MAE=0.716, time=36.9, train_MAE=0.657, train_loss=0.699, val_MAE=0.669, val_loss=0.71] Epoch 3:   0%|          | 4/1000 [04:50<23:49:26, 86.11s/it, lr=0.001, test_MAE=0.716, time=36.9, train_MAE=0.657, train_loss=0.699, val_MAE=0.669, val_loss=0.71] Epoch 4:   0%|          | 4/1000 [04:50<23:49:26, 86.11s/it, lr=0.001, test_MAE=0.716, time=36.9, train_MAE=0.657, train_loss=0.699, val_MAE=0.669, val_loss=0.71]Epoch 4:   0%|          | 4/1000 [05:27<23:49:26, 86.11s/it, lr=0.001, test_MAE=0.755, time=36.5, train_MAE=0.648, train_loss=0.69, val_MAE=0.705, val_loss=0.745]Epoch 4:   0%|          | 5/1000 [05:27<19:41:20, 71.24s/it, lr=0.001, test_MAE=0.755, time=36.5, train_MAE=0.648, train_loss=0.69, val_MAE=0.705, val_loss=0.745]Epoch 5:   0%|          | 5/1000 [05:27<19:41:20, 71.24s/it, lr=0.001, test_MAE=0.755, time=36.5, train_MAE=0.648, train_loss=0.69, val_MAE=0.705, val_loss=0.745]Epoch 5:   0%|          | 5/1000 [06:03<19:41:20, 71.24s/it, lr=0.001, test_MAE=0.723, time=36.3, train_MAE=0.651, train_loss=0.69, val_MAE=0.656, val_loss=0.693]Epoch 5:   1%|          | 6/1000 [06:03<16:46:20, 60.75s/it, lr=0.001, test_MAE=0.723, time=36.3, train_MAE=0.651, train_loss=0.69, val_MAE=0.656, val_loss=0.693]Epoch 6:   1%|          | 6/1000 [06:03<16:46:20, 60.75s/it, lr=0.001, test_MAE=0.723, time=36.3, train_MAE=0.651, train_loss=0.69, val_MAE=0.656, val_loss=0.693]Epoch 6:   1%|          | 6/1000 [06:40<16:46:20, 60.75s/it, lr=0.001, test_MAE=0.819, time=37.3, train_MAE=0.648, train_loss=0.687, val_MAE=0.775, val_loss=0.813]Epoch 6:   1%|          | 7/1000 [06:40<14:48:46, 53.70s/it, lr=0.001, test_MAE=0.819, time=37.3, train_MAE=0.648, train_loss=0.687, val_MAE=0.775, val_loss=0.813]Epoch 7:   1%|          | 7/1000 [06:40<14:48:46, 53.70s/it, lr=0.001, test_MAE=0.819, time=37.3, train_MAE=0.648, train_loss=0.687, val_MAE=0.775, val_loss=0.813]Epoch 7:   1%|          | 7/1000 [07:17<14:48:46, 53.70s/it, lr=0.001, test_MAE=0.835, time=36.5, train_MAE=0.637, train_loss=0.675, val_MAE=0.768, val_loss=0.806]Epoch 7:   1%|          | 8/1000 [07:17<13:22:46, 48.56s/it, lr=0.001, test_MAE=0.835, time=36.5, train_MAE=0.637, train_loss=0.675, val_MAE=0.768, val_loss=0.806]Epoch 8:   1%|          | 8/1000 [07:17<13:22:46, 48.56s/it, lr=0.001, test_MAE=0.835, time=36.5, train_MAE=0.637, train_loss=0.675, val_MAE=0.768, val_loss=0.806]Epoch 8:   1%|          | 8/1000 [07:53<13:22:46, 48.56s/it, lr=0.001, test_MAE=0.977, time=36.6, train_MAE=0.636, train_loss=0.675, val_MAE=0.924, val_loss=0.962]Epoch 8:   1%|          | 9/1000 [07:53<12:22:56, 44.98s/it, lr=0.001, test_MAE=0.977, time=36.6, train_MAE=0.636, train_loss=0.675, val_MAE=0.924, val_loss=0.962]Epoch 9:   1%|          | 9/1000 [07:53<12:22:56, 44.98s/it, lr=0.001, test_MAE=0.977, time=36.6, train_MAE=0.636, train_loss=0.675, val_MAE=0.924, val_loss=0.962]Epoch 9:   1%|          | 9/1000 [08:30<12:22:56, 44.98s/it, lr=0.001, test_MAE=0.733, time=36.5, train_MAE=0.642, train_loss=0.681, val_MAE=0.682, val_loss=0.72] Epoch 9:   1%|          | 10/1000 [08:30<11:40:24, 42.45s/it, lr=0.001, test_MAE=0.733, time=36.5, train_MAE=0.642, train_loss=0.681, val_MAE=0.682, val_loss=0.72]Epoch 10:   1%|          | 10/1000 [08:30<11:40:24, 42.45s/it, lr=0.001, test_MAE=0.733, time=36.5, train_MAE=0.642, train_loss=0.681, val_MAE=0.682, val_loss=0.72]Epoch 10:   1%|          | 10/1000 [09:07<11:40:24, 42.45s/it, lr=0.001, test_MAE=0.874, time=36.9, train_MAE=0.627, train_loss=0.664, val_MAE=0.844, val_loss=0.881]Epoch 10:   1%|          | 11/1000 [09:07<11:12:32, 40.80s/it, lr=0.001, test_MAE=0.874, time=36.9, train_MAE=0.627, train_loss=0.664, val_MAE=0.844, val_loss=0.881]Epoch 11:   1%|          | 11/1000 [09:07<11:12:32, 40.80s/it, lr=0.001, test_MAE=0.874, time=36.9, train_MAE=0.627, train_loss=0.664, val_MAE=0.844, val_loss=0.881]Epoch 11:   1%|          | 11/1000 [09:44<11:12:32, 40.80s/it, lr=0.001, test_MAE=0.877, time=36.6, train_MAE=0.633, train_loss=0.672, val_MAE=0.813, val_loss=0.852]Epoch    12: reducing learning rate of group 0 to 5.0000e-04.
Epoch 11:   1%|          | 12/1000 [09:44<10:51:04, 39.54s/it, lr=0.001, test_MAE=0.877, time=36.6, train_MAE=0.633, train_loss=0.672, val_MAE=0.813, val_loss=0.852]Epoch 12:   1%|          | 12/1000 [09:44<10:51:04, 39.54s/it, lr=0.001, test_MAE=0.877, time=36.6, train_MAE=0.633, train_loss=0.672, val_MAE=0.813, val_loss=0.852]Epoch 12:   1%|          | 12/1000 [10:20<10:51:04, 39.54s/it, lr=0.0005, test_MAE=0.676, time=36.3, train_MAE=0.619, train_loss=0.652, val_MAE=0.634, val_loss=0.665]Epoch 12:   1%|▏         | 13/1000 [10:20<10:34:19, 38.56s/it, lr=0.0005, test_MAE=0.676, time=36.3, train_MAE=0.619, train_loss=0.652, val_MAE=0.634, val_loss=0.665]Epoch 13:   1%|▏         | 13/1000 [10:20<10:34:19, 38.56s/it, lr=0.0005, test_MAE=0.676, time=36.3, train_MAE=0.619, train_loss=0.652, val_MAE=0.634, val_loss=0.665]Epoch 13:   1%|▏         | 13/1000 [10:57<10:34:19, 38.56s/it, lr=0.0005, test_MAE=0.676, time=37.3, train_MAE=0.611, train_loss=0.642, val_MAE=0.647, val_loss=0.678]Epoch 13:   1%|▏         | 14/1000 [10:57<10:27:20, 38.18s/it, lr=0.0005, test_MAE=0.676, time=37.3, train_MAE=0.611, train_loss=0.642, val_MAE=0.647, val_loss=0.678]Epoch 14:   1%|▏         | 14/1000 [10:57<10:27:20, 38.18s/it, lr=0.0005, test_MAE=0.676, time=37.3, train_MAE=0.611, train_loss=0.642, val_MAE=0.647, val_loss=0.678]Epoch 14:   1%|▏         | 14/1000 [11:34<10:27:20, 38.18s/it, lr=0.0005, test_MAE=0.865, time=36.5, train_MAE=0.619, train_loss=0.651, val_MAE=0.808, val_loss=0.84] Epoch 14:   2%|▏         | 15/1000 [11:34<10:18:42, 37.69s/it, lr=0.0005, test_MAE=0.865, time=36.5, train_MAE=0.619, train_loss=0.651, val_MAE=0.808, val_loss=0.84]Epoch 15:   2%|▏         | 15/1000 [11:34<10:18:42, 37.69s/it, lr=0.0005, test_MAE=0.865, time=36.5, train_MAE=0.619, train_loss=0.651, val_MAE=0.808, val_loss=0.84]Epoch 15:   2%|▏         | 15/1000 [12:10<10:18:42, 37.69s/it, lr=0.0005, test_MAE=0.696, time=36.6, train_MAE=0.608, train_loss=0.641, val_MAE=0.648, val_loss=0.679]Epoch 15:   2%|▏         | 16/1000 [12:10<10:12:44, 37.36s/it, lr=0.0005, test_MAE=0.696, time=36.6, train_MAE=0.608, train_loss=0.641, val_MAE=0.648, val_loss=0.679]Epoch 16:   2%|▏         | 16/1000 [12:10<10:12:44, 37.36s/it, lr=0.0005, test_MAE=0.696, time=36.6, train_MAE=0.608, train_loss=0.641, val_MAE=0.648, val_loss=0.679]Epoch 16:   2%|▏         | 16/1000 [12:47<10:12:44, 37.36s/it, lr=0.0005, test_MAE=0.69, time=36.6, train_MAE=0.606, train_loss=0.638, val_MAE=0.644, val_loss=0.675] Epoch 16:   2%|▏         | 17/1000 [12:47<10:08:19, 37.13s/it, lr=0.0005, test_MAE=0.69, time=36.6, train_MAE=0.606, train_loss=0.638, val_MAE=0.644, val_loss=0.675]Epoch 17:   2%|▏         | 17/1000 [12:47<10:08:19, 37.13s/it, lr=0.0005, test_MAE=0.69, time=36.6, train_MAE=0.606, train_loss=0.638, val_MAE=0.644, val_loss=0.675]Epoch 17:   2%|▏         | 17/1000 [13:24<10:08:19, 37.13s/it, lr=0.0005, test_MAE=0.75, time=37, train_MAE=0.601, train_loss=0.633, val_MAE=0.695, val_loss=0.728]  Epoch 17:   2%|▏         | 18/1000 [13:24<10:06:52, 37.08s/it, lr=0.0005, test_MAE=0.75, time=37, train_MAE=0.601, train_loss=0.633, val_MAE=0.695, val_loss=0.728]Epoch 18:   2%|▏         | 18/1000 [13:24<10:06:52, 37.08s/it, lr=0.0005, test_MAE=0.75, time=37, train_MAE=0.601, train_loss=0.633, val_MAE=0.695, val_loss=0.728]Epoch 18:   2%|▏         | 18/1000 [14:00<10:06:52, 37.08s/it, lr=0.0005, test_MAE=0.75, time=36.7, train_MAE=0.607, train_loss=0.64, val_MAE=0.689, val_loss=0.721]Epoch    19: reducing learning rate of group 0 to 2.5000e-04.
Epoch 18:   2%|▏         | 19/1000 [14:00<10:04:18, 36.96s/it, lr=0.0005, test_MAE=0.75, time=36.7, train_MAE=0.607, train_loss=0.64, val_MAE=0.689, val_loss=0.721]Epoch 19:   2%|▏         | 19/1000 [14:00<10:04:18, 36.96s/it, lr=0.0005, test_MAE=0.75, time=36.7, train_MAE=0.607, train_loss=0.64, val_MAE=0.689, val_loss=0.721]Epoch 19:   2%|▏         | 19/1000 [14:37<10:04:18, 36.96s/it, lr=0.00025, test_MAE=0.719, time=36.5, train_MAE=0.595, train_loss=0.624, val_MAE=0.666, val_loss=0.695]Epoch 19:   2%|▏         | 20/1000 [14:37<10:01:37, 36.83s/it, lr=0.00025, test_MAE=0.719, time=36.5, train_MAE=0.595, train_loss=0.624, val_MAE=0.666, val_loss=0.695]Epoch 20:   2%|▏         | 20/1000 [14:37<10:01:37, 36.83s/it, lr=0.00025, test_MAE=0.719, time=36.5, train_MAE=0.595, train_loss=0.624, val_MAE=0.666, val_loss=0.695]Epoch 20:   2%|▏         | 20/1000 [15:14<10:01:37, 36.83s/it, lr=0.00025, test_MAE=0.694, time=36.9, train_MAE=0.59, train_loss=0.618, val_MAE=0.653, val_loss=0.681] Epoch 20:   2%|▏         | 21/1000 [15:14<10:01:23, 36.86s/it, lr=0.00025, test_MAE=0.694, time=36.9, train_MAE=0.59, train_loss=0.618, val_MAE=0.653, val_loss=0.681]Epoch 21:   2%|▏         | 21/1000 [15:14<10:01:23, 36.86s/it, lr=0.00025, test_MAE=0.694, time=36.9, train_MAE=0.59, train_loss=0.618, val_MAE=0.653, val_loss=0.681]Epoch 21:   2%|▏         | 21/1000 [15:51<10:01:23, 36.86s/it, lr=0.00025, test_MAE=0.666, time=36.6, train_MAE=0.588, train_loss=0.616, val_MAE=0.623, val_loss=0.651]Epoch 21:   2%|▏         | 22/1000 [15:51<9:59:34, 36.78s/it, lr=0.00025, test_MAE=0.666, time=36.6, train_MAE=0.588, train_loss=0.616, val_MAE=0.623, val_loss=0.651] Epoch 22:   2%|▏         | 22/1000 [15:51<9:59:34, 36.78s/it, lr=0.00025, test_MAE=0.666, time=36.6, train_MAE=0.588, train_loss=0.616, val_MAE=0.623, val_loss=0.651]Epoch 22:   2%|▏         | 22/1000 [16:27<9:59:34, 36.78s/it, lr=0.00025, test_MAE=0.674, time=36.6, train_MAE=0.589, train_loss=0.618, val_MAE=0.636, val_loss=0.664]Epoch 22:   2%|▏         | 23/1000 [16:27<9:58:01, 36.73s/it, lr=0.00025, test_MAE=0.674, time=36.6, train_MAE=0.589, train_loss=0.618, val_MAE=0.636, val_loss=0.664]Epoch 23:   2%|▏         | 23/1000 [16:27<9:58:01, 36.73s/it, lr=0.00025, test_MAE=0.674, time=36.6, train_MAE=0.589, train_loss=0.618, val_MAE=0.636, val_loss=0.664]Epoch 23:   2%|▏         | 23/1000 [17:04<9:58:01, 36.73s/it, lr=0.00025, test_MAE=0.688, time=36.6, train_MAE=0.588, train_loss=0.617, val_MAE=0.647, val_loss=0.676]Epoch 23:   2%|▏         | 24/1000 [17:04<9:56:49, 36.69s/it, lr=0.00025, test_MAE=0.688, time=36.6, train_MAE=0.588, train_loss=0.617, val_MAE=0.647, val_loss=0.676]Epoch 24:   2%|▏         | 24/1000 [17:04<9:56:49, 36.69s/it, lr=0.00025, test_MAE=0.688, time=36.6, train_MAE=0.588, train_loss=0.617, val_MAE=0.647, val_loss=0.676]Epoch 24:   2%|▏         | 24/1000 [17:41<9:56:49, 36.69s/it, lr=0.00025, test_MAE=0.668, time=36.9, train_MAE=0.583, train_loss=0.612, val_MAE=0.627, val_loss=0.656]Epoch 24:   2%|▎         | 25/1000 [17:41<9:57:24, 36.76s/it, lr=0.00025, test_MAE=0.668, time=36.9, train_MAE=0.583, train_loss=0.612, val_MAE=0.627, val_loss=0.656]Epoch 25:   2%|▎         | 25/1000 [17:41<9:57:24, 36.76s/it, lr=0.00025, test_MAE=0.668, time=36.9, train_MAE=0.583, train_loss=0.612, val_MAE=0.627, val_loss=0.656]Epoch 25:   2%|▎         | 25/1000 [18:17<9:57:24, 36.76s/it, lr=0.00025, test_MAE=0.678, time=36.3, train_MAE=0.584, train_loss=0.613, val_MAE=0.64, val_loss=0.668] Epoch 25:   3%|▎         | 26/1000 [18:17<9:54:45, 36.64s/it, lr=0.00025, test_MAE=0.678, time=36.3, train_MAE=0.584, train_loss=0.613, val_MAE=0.64, val_loss=0.668]Epoch 26:   3%|▎         | 26/1000 [18:17<9:54:45, 36.64s/it, lr=0.00025, test_MAE=0.678, time=36.3, train_MAE=0.584, train_loss=0.613, val_MAE=0.64, val_loss=0.668]Epoch 26:   3%|▎         | 26/1000 [18:54<9:54:45, 36.64s/it, lr=0.00025, test_MAE=0.693, time=36.9, train_MAE=0.587, train_loss=0.615, val_MAE=0.668, val_loss=0.696]Epoch 26:   3%|▎         | 27/1000 [18:54<9:55:23, 36.71s/it, lr=0.00025, test_MAE=0.693, time=36.9, train_MAE=0.587, train_loss=0.615, val_MAE=0.668, val_loss=0.696]Epoch 27:   3%|▎         | 27/1000 [18:54<9:55:23, 36.71s/it, lr=0.00025, test_MAE=0.693, time=36.9, train_MAE=0.587, train_loss=0.615, val_MAE=0.668, val_loss=0.696]Epoch 27:   3%|▎         | 27/1000 [19:31<9:55:23, 36.71s/it, lr=0.00025, test_MAE=0.676, time=36.9, train_MAE=0.582, train_loss=0.611, val_MAE=0.64, val_loss=0.668] Epoch    28: reducing learning rate of group 0 to 1.2500e-04.
Epoch 27:   3%|▎         | 28/1000 [19:31<9:55:40, 36.77s/it, lr=0.00025, test_MAE=0.676, time=36.9, train_MAE=0.582, train_loss=0.611, val_MAE=0.64, val_loss=0.668]Epoch 28:   3%|▎         | 28/1000 [19:31<9:55:40, 36.77s/it, lr=0.00025, test_MAE=0.676, time=36.9, train_MAE=0.582, train_loss=0.611, val_MAE=0.64, val_loss=0.668]Epoch 28:   3%|▎         | 28/1000 [20:07<9:55:40, 36.77s/it, lr=0.000125, test_MAE=0.672, time=36.6, train_MAE=0.586, train_loss=0.614, val_MAE=0.637, val_loss=0.663]Epoch 28:   3%|▎         | 29/1000 [20:07<9:54:18, 36.72s/it, lr=0.000125, test_MAE=0.672, time=36.6, train_MAE=0.586, train_loss=0.614, val_MAE=0.637, val_loss=0.663]Epoch 29:   3%|▎         | 29/1000 [20:07<9:54:18, 36.72s/it, lr=0.000125, test_MAE=0.672, time=36.6, train_MAE=0.586, train_loss=0.614, val_MAE=0.637, val_loss=0.663]Epoch 29:   3%|▎         | 29/1000 [20:44<9:54:18, 36.72s/it, lr=0.000125, test_MAE=0.666, time=36.3, train_MAE=0.578, train_loss=0.604, val_MAE=0.634, val_loss=0.66] Epoch 29:   3%|▎         | 30/1000 [20:44<9:51:27, 36.59s/it, lr=0.000125, test_MAE=0.666, time=36.3, train_MAE=0.578, train_loss=0.604, val_MAE=0.634, val_loss=0.66]Epoch 30:   3%|▎         | 30/1000 [20:44<9:51:27, 36.59s/it, lr=0.000125, test_MAE=0.666, time=36.3, train_MAE=0.578, train_loss=0.604, val_MAE=0.634, val_loss=0.66]Epoch 30:   3%|▎         | 30/1000 [21:21<9:51:27, 36.59s/it, lr=0.000125, test_MAE=0.67, time=37.2, train_MAE=0.572, train_loss=0.599, val_MAE=0.637, val_loss=0.663]Epoch 30:   3%|▎         | 31/1000 [21:21<9:54:05, 36.79s/it, lr=0.000125, test_MAE=0.67, time=37.2, train_MAE=0.572, train_loss=0.599, val_MAE=0.637, val_loss=0.663]Epoch 31:   3%|▎         | 31/1000 [21:21<9:54:05, 36.79s/it, lr=0.000125, test_MAE=0.67, time=37.2, train_MAE=0.572, train_loss=0.599, val_MAE=0.637, val_loss=0.663]Epoch 31:   3%|▎         | 31/1000 [21:58<9:54:05, 36.79s/it, lr=0.000125, test_MAE=0.681, time=36.6, train_MAE=0.564, train_loss=0.591, val_MAE=0.642, val_loss=0.668]Epoch 31:   3%|▎         | 32/1000 [21:58<9:52:41, 36.74s/it, lr=0.000125, test_MAE=0.681, time=36.6, train_MAE=0.564, train_loss=0.591, val_MAE=0.642, val_loss=0.668]Epoch 32:   3%|▎         | 32/1000 [21:58<9:52:41, 36.74s/it, lr=0.000125, test_MAE=0.681, time=36.6, train_MAE=0.564, train_loss=0.591, val_MAE=0.642, val_loss=0.668]Epoch 32:   3%|▎         | 32/1000 [22:34<9:52:41, 36.74s/it, lr=0.000125, test_MAE=0.677, time=36.6, train_MAE=0.567, train_loss=0.593, val_MAE=0.643, val_loss=0.669]Epoch 32:   3%|▎         | 33/1000 [22:34<9:51:32, 36.70s/it, lr=0.000125, test_MAE=0.677, time=36.6, train_MAE=0.567, train_loss=0.593, val_MAE=0.643, val_loss=0.669]Epoch 33:   3%|▎         | 33/1000 [22:34<9:51:32, 36.70s/it, lr=0.000125, test_MAE=0.677, time=36.6, train_MAE=0.567, train_loss=0.593, val_MAE=0.643, val_loss=0.669]Epoch 33:   3%|▎         | 33/1000 [23:11<9:51:32, 36.70s/it, lr=0.000125, test_MAE=0.673, time=36.6, train_MAE=0.566, train_loss=0.593, val_MAE=0.638, val_loss=0.664]Epoch    34: reducing learning rate of group 0 to 6.2500e-05.
Epoch 33:   3%|▎         | 34/1000 [23:11<9:50:14, 36.66s/it, lr=0.000125, test_MAE=0.673, time=36.6, train_MAE=0.566, train_loss=0.593, val_MAE=0.638, val_loss=0.664]Epoch 34:   3%|▎         | 34/1000 [23:11<9:50:14, 36.66s/it, lr=0.000125, test_MAE=0.673, time=36.6, train_MAE=0.566, train_loss=0.593, val_MAE=0.638, val_loss=0.664]Epoch 34:   3%|▎         | 34/1000 [23:48<9:50:14, 36.66s/it, lr=6.25e-5, test_MAE=0.67, time=36.9, train_MAE=0.562, train_loss=0.588, val_MAE=0.635, val_loss=0.661]  Epoch 34:   4%|▎         | 35/1000 [23:48<9:50:47, 36.73s/it, lr=6.25e-5, test_MAE=0.67, time=36.9, train_MAE=0.562, train_loss=0.588, val_MAE=0.635, val_loss=0.661]Epoch 35:   4%|▎         | 35/1000 [23:48<9:50:47, 36.73s/it, lr=6.25e-5, test_MAE=0.67, time=36.9, train_MAE=0.562, train_loss=0.588, val_MAE=0.635, val_loss=0.661]Epoch 35:   4%|▎         | 35/1000 [24:24<9:50:47, 36.73s/it, lr=6.25e-5, test_MAE=0.668, time=36.6, train_MAE=0.561, train_loss=0.587, val_MAE=0.636, val_loss=0.661]Epoch 35:   4%|▎         | 36/1000 [24:24<9:49:27, 36.69s/it, lr=6.25e-5, test_MAE=0.668, time=36.6, train_MAE=0.561, train_loss=0.587, val_MAE=0.636, val_loss=0.661]Epoch 36:   4%|▎         | 36/1000 [24:24<9:49:27, 36.69s/it, lr=6.25e-5, test_MAE=0.668, time=36.6, train_MAE=0.561, train_loss=0.587, val_MAE=0.636, val_loss=0.661]Epoch 36:   4%|▎         | 36/1000 [25:01<9:49:27, 36.69s/it, lr=6.25e-5, test_MAE=0.676, time=36.6, train_MAE=0.556, train_loss=0.581, val_MAE=0.64, val_loss=0.665] Epoch 36:   4%|▎         | 37/1000 [25:01<9:48:35, 36.67s/it, lr=6.25e-5, test_MAE=0.676, time=36.6, train_MAE=0.556, train_loss=0.581, val_MAE=0.64, val_loss=0.665]Epoch 37:   4%|▎         | 37/1000 [25:01<9:48:35, 36.67s/it, lr=6.25e-5, test_MAE=0.676, time=36.6, train_MAE=0.556, train_loss=0.581, val_MAE=0.64, val_loss=0.665]Epoch 37:   4%|▎         | 37/1000 [25:38<9:48:35, 36.67s/it, lr=6.25e-5, test_MAE=0.67, time=36.9, train_MAE=0.558, train_loss=0.583, val_MAE=0.638, val_loss=0.663]Epoch 37:   4%|▍         | 38/1000 [25:38<9:49:10, 36.75s/it, lr=6.25e-5, test_MAE=0.67, time=36.9, train_MAE=0.558, train_loss=0.583, val_MAE=0.638, val_loss=0.663]Epoch 38:   4%|▍         | 38/1000 [25:38<9:49:10, 36.75s/it, lr=6.25e-5, test_MAE=0.67, time=36.9, train_MAE=0.558, train_loss=0.583, val_MAE=0.638, val_loss=0.663]Epoch 38:   4%|▍         | 38/1000 [26:14<9:49:10, 36.75s/it, lr=6.25e-5, test_MAE=0.671, time=36.6, train_MAE=0.558, train_loss=0.583, val_MAE=0.638, val_loss=0.664]Epoch 38:   4%|▍         | 39/1000 [26:14<9:47:42, 36.69s/it, lr=6.25e-5, test_MAE=0.671, time=36.6, train_MAE=0.558, train_loss=0.583, val_MAE=0.638, val_loss=0.664]Epoch 39:   4%|▍         | 39/1000 [26:14<9:47:42, 36.69s/it, lr=6.25e-5, test_MAE=0.671, time=36.6, train_MAE=0.558, train_loss=0.583, val_MAE=0.638, val_loss=0.664]Epoch 39:   4%|▍         | 39/1000 [26:51<9:47:42, 36.69s/it, lr=6.25e-5, test_MAE=0.671, time=36.6, train_MAE=0.554, train_loss=0.579, val_MAE=0.639, val_loss=0.664]Epoch    40: reducing learning rate of group 0 to 3.1250e-05.
Epoch 39:   4%|▍         | 40/1000 [26:51<9:46:39, 36.67s/it, lr=6.25e-5, test_MAE=0.671, time=36.6, train_MAE=0.554, train_loss=0.579, val_MAE=0.639, val_loss=0.664]Epoch 40:   4%|▍         | 40/1000 [26:51<9:46:39, 36.67s/it, lr=6.25e-5, test_MAE=0.671, time=36.6, train_MAE=0.554, train_loss=0.579, val_MAE=0.639, val_loss=0.664]Epoch 40:   4%|▍         | 40/1000 [27:28<9:46:39, 36.67s/it, lr=3.13e-5, test_MAE=0.686, time=36.6, train_MAE=0.579, train_loss=0.604, val_MAE=0.648, val_loss=0.673]Epoch 40:   4%|▍         | 41/1000 [27:28<9:45:33, 36.64s/it, lr=3.13e-5, test_MAE=0.686, time=36.6, train_MAE=0.579, train_loss=0.604, val_MAE=0.648, val_loss=0.673]Epoch 41:   4%|▍         | 41/1000 [27:28<9:45:33, 36.64s/it, lr=3.13e-5, test_MAE=0.686, time=36.6, train_MAE=0.579, train_loss=0.604, val_MAE=0.648, val_loss=0.673]Epoch 41:   4%|▍         | 41/1000 [28:05<9:45:33, 36.64s/it, lr=3.13e-5, test_MAE=0.668, time=37, train_MAE=0.557, train_loss=0.582, val_MAE=0.636, val_loss=0.661]  Epoch 41:   4%|▍         | 42/1000 [28:05<9:46:55, 36.76s/it, lr=3.13e-5, test_MAE=0.668, time=37, train_MAE=0.557, train_loss=0.582, val_MAE=0.636, val_loss=0.661]Epoch 42:   4%|▍         | 42/1000 [28:05<9:46:55, 36.76s/it, lr=3.13e-5, test_MAE=0.668, time=37, train_MAE=0.557, train_loss=0.582, val_MAE=0.636, val_loss=0.661]Epoch 42:   4%|▍         | 42/1000 [28:41<9:46:55, 36.76s/it, lr=3.13e-5, test_MAE=0.671, time=36.5, train_MAE=0.553, train_loss=0.577, val_MAE=0.635, val_loss=0.66]Epoch 42:   4%|▍         | 43/1000 [28:41<9:45:12, 36.69s/it, lr=3.13e-5, test_MAE=0.671, time=36.5, train_MAE=0.553, train_loss=0.577, val_MAE=0.635, val_loss=0.66]Epoch 43:   4%|▍         | 43/1000 [28:41<9:45:12, 36.69s/it, lr=3.13e-5, test_MAE=0.671, time=36.5, train_MAE=0.553, train_loss=0.577, val_MAE=0.635, val_loss=0.66]Epoch 43:   4%|▍         | 43/1000 [29:18<9:45:12, 36.69s/it, lr=3.13e-5, test_MAE=0.672, time=36.6, train_MAE=0.554, train_loss=0.578, val_MAE=0.641, val_loss=0.666]Epoch 43:   4%|▍         | 44/1000 [29:18<9:44:22, 36.68s/it, lr=3.13e-5, test_MAE=0.672, time=36.6, train_MAE=0.554, train_loss=0.578, val_MAE=0.641, val_loss=0.666]Epoch 44:   4%|▍         | 44/1000 [29:18<9:44:22, 36.68s/it, lr=3.13e-5, test_MAE=0.672, time=36.6, train_MAE=0.554, train_loss=0.578, val_MAE=0.641, val_loss=0.666]Epoch 44:   4%|▍         | 44/1000 [29:55<9:44:22, 36.68s/it, lr=3.13e-5, test_MAE=0.669, time=36.9, train_MAE=0.554, train_loss=0.578, val_MAE=0.638, val_loss=0.662]Epoch 44:   4%|▍         | 45/1000 [29:55<9:44:51, 36.75s/it, lr=3.13e-5, test_MAE=0.669, time=36.9, train_MAE=0.554, train_loss=0.578, val_MAE=0.638, val_loss=0.662]Epoch 45:   4%|▍         | 45/1000 [29:55<9:44:51, 36.75s/it, lr=3.13e-5, test_MAE=0.669, time=36.9, train_MAE=0.554, train_loss=0.578, val_MAE=0.638, val_loss=0.662]Epoch 45:   4%|▍         | 45/1000 [30:31<9:44:51, 36.75s/it, lr=3.13e-5, test_MAE=0.672, time=36.6, train_MAE=0.551, train_loss=0.575, val_MAE=0.639, val_loss=0.663]Epoch    46: reducing learning rate of group 0 to 1.5625e-05.
Epoch 45:   5%|▍         | 46/1000 [30:31<9:43:28, 36.70s/it, lr=3.13e-5, test_MAE=0.672, time=36.6, train_MAE=0.551, train_loss=0.575, val_MAE=0.639, val_loss=0.663]Epoch 46:   5%|▍         | 46/1000 [30:31<9:43:28, 36.70s/it, lr=3.13e-5, test_MAE=0.672, time=36.6, train_MAE=0.551, train_loss=0.575, val_MAE=0.639, val_loss=0.663]Epoch 46:   5%|▍         | 46/1000 [31:08<9:43:28, 36.70s/it, lr=1.56e-5, test_MAE=0.671, time=36.7, train_MAE=0.551, train_loss=0.576, val_MAE=0.64, val_loss=0.664] Epoch 46:   5%|▍         | 47/1000 [31:08<9:42:42, 36.69s/it, lr=1.56e-5, test_MAE=0.671, time=36.7, train_MAE=0.551, train_loss=0.576, val_MAE=0.64, val_loss=0.664]Epoch 47:   5%|▍         | 47/1000 [31:08<9:42:42, 36.69s/it, lr=1.56e-5, test_MAE=0.671, time=36.7, train_MAE=0.551, train_loss=0.576, val_MAE=0.64, val_loss=0.664]Epoch 47:   5%|▍         | 47/1000 [31:44<9:42:42, 36.69s/it, lr=1.56e-5, test_MAE=0.671, time=36.6, train_MAE=0.548, train_loss=0.573, val_MAE=0.639, val_loss=0.664]Epoch 47:   5%|▍         | 48/1000 [31:44<9:41:35, 36.66s/it, lr=1.56e-5, test_MAE=0.671, time=36.6, train_MAE=0.548, train_loss=0.573, val_MAE=0.639, val_loss=0.664]Epoch 48:   5%|▍         | 48/1000 [31:44<9:41:35, 36.66s/it, lr=1.56e-5, test_MAE=0.671, time=36.6, train_MAE=0.548, train_loss=0.573, val_MAE=0.639, val_loss=0.664]Epoch 48:   5%|▍         | 48/1000 [32:21<9:41:35, 36.66s/it, lr=1.56e-5, test_MAE=0.673, time=37, train_MAE=0.552, train_loss=0.576, val_MAE=0.641, val_loss=0.665]  Epoch 48:   5%|▍         | 49/1000 [32:22<9:42:42, 36.76s/it, lr=1.56e-5, test_MAE=0.673, time=37, train_MAE=0.552, train_loss=0.576, val_MAE=0.641, val_loss=0.665]Epoch 49:   5%|▍         | 49/1000 [32:22<9:42:42, 36.76s/it, lr=1.56e-5, test_MAE=0.673, time=37, train_MAE=0.552, train_loss=0.576, val_MAE=0.641, val_loss=0.665]Epoch 49:   5%|▍         | 49/1000 [32:58<9:42:42, 36.76s/it, lr=1.56e-5, test_MAE=0.673, time=36.6, train_MAE=0.546, train_loss=0.57, val_MAE=0.641, val_loss=0.665]Epoch 49:   5%|▌         | 50/1000 [32:58<9:41:20, 36.72s/it, lr=1.56e-5, test_MAE=0.673, time=36.6, train_MAE=0.546, train_loss=0.57, val_MAE=0.641, val_loss=0.665]Epoch 50:   5%|▌         | 50/1000 [32:58<9:41:20, 36.72s/it, lr=1.56e-5, test_MAE=0.673, time=36.6, train_MAE=0.546, train_loss=0.57, val_MAE=0.641, val_loss=0.665]Epoch 50:   5%|▌         | 50/1000 [33:35<9:41:20, 36.72s/it, lr=1.56e-5, test_MAE=0.673, time=36.6, train_MAE=0.547, train_loss=0.571, val_MAE=0.641, val_loss=0.665]Epoch 50:   5%|▌         | 51/1000 [33:35<9:39:59, 36.67s/it, lr=1.56e-5, test_MAE=0.673, time=36.6, train_MAE=0.547, train_loss=0.571, val_MAE=0.641, val_loss=0.665]Epoch 51:   5%|▌         | 51/1000 [33:35<9:39:59, 36.67s/it, lr=1.56e-5, test_MAE=0.673, time=36.6, train_MAE=0.547, train_loss=0.571, val_MAE=0.641, val_loss=0.665]Epoch 51:   5%|▌         | 51/1000 [34:12<9:39:59, 36.67s/it, lr=1.56e-5, test_MAE=0.675, time=37, train_MAE=0.545, train_loss=0.569, val_MAE=0.641, val_loss=0.665]  Epoch    52: reducing learning rate of group 0 to 7.8125e-06.

!! LR EQUAL TO MIN LR SET.
Epoch 51:   5%|▌         | 51/1000 [34:12<10:36:25, 40.24s/it, lr=1.56e-5, test_MAE=0.675, time=37, train_MAE=0.545, train_loss=0.569, val_MAE=0.641, val_loss=0.665]
Test MAE: 0.6747
Train MAE: 0.5343
Convergence Time (Epochs): 51.0000
TOTAL TIME TAKEN: 2075.1138s
AVG TIME PER EPOCH: 39.4541s

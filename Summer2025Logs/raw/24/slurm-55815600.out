I'm echoing to stdout
I'm echoing to stderr
My JobID is 55815600
I have 4 CPUs on node r108u25n01
Using backend: pytorch
cuda not available
[I] Loading dataset ZINC...
train, test, val sizes : 10000 1000 1000
[I] Finished loading.
[I] Data load time: 5.0852s
Training Graphs:  10000
Validation Graphs:  1000
Test Graphs:  1000
  0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [01:01<?, ?it/s, lr=0.001, test_MAE=1.16, time=61.2, train_MAE=1.37, train_loss=1.37, val_MAE=1.08, val_loss=1.08]Epoch 0:   0%|          | 1/1000 [01:01<16:59:10, 61.21s/it, lr=0.001, test_MAE=1.16, time=61.2, train_MAE=1.37, train_loss=1.37, val_MAE=1.08, val_loss=1.08]Epoch 1:   0%|          | 1/1000 [01:01<16:59:10, 61.21s/it, lr=0.001, test_MAE=1.16, time=61.2, train_MAE=1.37, train_loss=1.37, val_MAE=1.08, val_loss=1.08]Epoch 1:   0%|          | 1/1000 [01:56<16:59:10, 61.21s/it, lr=0.001, test_MAE=0.932, time=55.4, train_MAE=0.955, train_loss=0.955, val_MAE=0.862, val_loss=0.862]Epoch 1:   0%|          | 2/1000 [01:56<16:29:09, 59.47s/it, lr=0.001, test_MAE=0.932, time=55.4, train_MAE=0.955, train_loss=0.955, val_MAE=0.862, val_loss=0.862]Epoch 2:   0%|          | 2/1000 [01:56<16:29:09, 59.47s/it, lr=0.001, test_MAE=0.932, time=55.4, train_MAE=0.955, train_loss=0.955, val_MAE=0.862, val_loss=0.862]Epoch 2:   0%|          | 2/1000 [02:51<16:29:09, 59.47s/it, lr=0.001, test_MAE=0.892, time=55.1, train_MAE=0.812, train_loss=0.812, val_MAE=0.827, val_loss=0.827]Epoch 2:   0%|          | 3/1000 [02:51<16:06:56, 58.19s/it, lr=0.001, test_MAE=0.892, time=55.1, train_MAE=0.812, train_loss=0.812, val_MAE=0.827, val_loss=0.827]Epoch 3:   0%|          | 3/1000 [02:51<16:06:56, 58.19s/it, lr=0.001, test_MAE=0.892, time=55.1, train_MAE=0.812, train_loss=0.812, val_MAE=0.827, val_loss=0.827]Epoch 3:   0%|          | 3/1000 [03:47<16:06:56, 58.19s/it, lr=0.001, test_MAE=0.835, time=55.2, train_MAE=0.729, train_loss=0.729, val_MAE=0.783, val_loss=0.783]Epoch 3:   0%|          | 4/1000 [03:47<15:51:16, 57.31s/it, lr=0.001, test_MAE=0.835, time=55.2, train_MAE=0.729, train_loss=0.729, val_MAE=0.783, val_loss=0.783]Epoch 4:   0%|          | 4/1000 [03:47<15:51:16, 57.31s/it, lr=0.001, test_MAE=0.835, time=55.2, train_MAE=0.729, train_loss=0.729, val_MAE=0.783, val_loss=0.783]Epoch 4:   0%|          | 4/1000 [04:42<15:51:16, 57.31s/it, lr=0.001, test_MAE=0.8, time=55.6, train_MAE=0.675, train_loss=0.675, val_MAE=0.752, val_loss=0.752]  Epoch 4:   0%|          | 5/1000 [04:42<15:42:11, 56.82s/it, lr=0.001, test_MAE=0.8, time=55.6, train_MAE=0.675, train_loss=0.675, val_MAE=0.752, val_loss=0.752]Epoch 5:   0%|          | 5/1000 [04:42<15:42:11, 56.82s/it, lr=0.001, test_MAE=0.8, time=55.6, train_MAE=0.675, train_loss=0.675, val_MAE=0.752, val_loss=0.752]Epoch 5:   0%|          | 5/1000 [05:37<15:42:11, 56.82s/it, lr=0.001, test_MAE=0.778, time=54.9, train_MAE=0.635, train_loss=0.635, val_MAE=0.725, val_loss=0.725]Epoch 5:   1%|          | 6/1000 [05:37<15:31:39, 56.24s/it, lr=0.001, test_MAE=0.778, time=54.9, train_MAE=0.635, train_loss=0.635, val_MAE=0.725, val_loss=0.725]Epoch 6:   1%|          | 6/1000 [05:37<15:31:39, 56.24s/it, lr=0.001, test_MAE=0.778, time=54.9, train_MAE=0.635, train_loss=0.635, val_MAE=0.725, val_loss=0.725]Epoch 6:   1%|          | 6/1000 [06:32<15:31:39, 56.24s/it, lr=0.001, test_MAE=0.768, time=55.2, train_MAE=0.6, train_loss=0.6, val_MAE=0.719, val_loss=0.719]    Epoch 6:   1%|          | 7/1000 [06:32<15:25:42, 55.93s/it, lr=0.001, test_MAE=0.768, time=55.2, train_MAE=0.6, train_loss=0.6, val_MAE=0.719, val_loss=0.719]Epoch 7:   1%|          | 7/1000 [06:32<15:25:42, 55.93s/it, lr=0.001, test_MAE=0.768, time=55.2, train_MAE=0.6, train_loss=0.6, val_MAE=0.719, val_loss=0.719]Epoch 7:   1%|          | 7/1000 [07:27<15:25:42, 55.93s/it, lr=0.001, test_MAE=0.826, time=54.8, train_MAE=0.558, train_loss=0.558, val_MAE=0.771, val_loss=0.771]Epoch 7:   1%|          | 8/1000 [07:27<15:19:20, 55.61s/it, lr=0.001, test_MAE=0.826, time=54.8, train_MAE=0.558, train_loss=0.558, val_MAE=0.771, val_loss=0.771]Epoch 8:   1%|          | 8/1000 [07:27<15:19:20, 55.61s/it, lr=0.001, test_MAE=0.826, time=54.8, train_MAE=0.558, train_loss=0.558, val_MAE=0.771, val_loss=0.771]Epoch 8:   1%|          | 8/1000 [08:23<15:19:20, 55.61s/it, lr=0.001, test_MAE=0.831, time=55.8, train_MAE=0.565, train_loss=0.565, val_MAE=0.786, val_loss=0.786]Epoch 8:   1%|          | 9/1000 [08:23<15:19:27, 55.67s/it, lr=0.001, test_MAE=0.831, time=55.8, train_MAE=0.565, train_loss=0.565, val_MAE=0.786, val_loss=0.786]Epoch 9:   1%|          | 9/1000 [08:23<15:19:27, 55.67s/it, lr=0.001, test_MAE=0.831, time=55.8, train_MAE=0.565, train_loss=0.565, val_MAE=0.786, val_loss=0.786]Epoch 9:   1%|          | 9/1000 [09:19<15:19:27, 55.67s/it, lr=0.001, test_MAE=0.764, time=55.6, train_MAE=0.542, train_loss=0.542, val_MAE=0.723, val_loss=0.723]Epoch 9:   1%|          | 10/1000 [09:19<15:18:10, 55.65s/it, lr=0.001, test_MAE=0.764, time=55.6, train_MAE=0.542, train_loss=0.542, val_MAE=0.723, val_loss=0.723]Epoch 10:   1%|          | 10/1000 [09:19<15:18:10, 55.65s/it, lr=0.001, test_MAE=0.764, time=55.6, train_MAE=0.542, train_loss=0.542, val_MAE=0.723, val_loss=0.723]Epoch 10:   1%|          | 10/1000 [10:14<15:18:10, 55.65s/it, lr=0.001, test_MAE=0.755, time=55.5, train_MAE=0.504, train_loss=0.504, val_MAE=0.709, val_loss=0.709]Epoch 10:   1%|          | 11/1000 [10:14<15:16:47, 55.62s/it, lr=0.001, test_MAE=0.755, time=55.5, train_MAE=0.504, train_loss=0.504, val_MAE=0.709, val_loss=0.709]Epoch 11:   1%|          | 11/1000 [10:14<15:16:47, 55.62s/it, lr=0.001, test_MAE=0.755, time=55.5, train_MAE=0.504, train_loss=0.504, val_MAE=0.709, val_loss=0.709]Epoch 11:   1%|          | 11/1000 [11:10<15:16:47, 55.62s/it, lr=0.001, test_MAE=0.783, time=55.5, train_MAE=0.484, train_loss=0.484, val_MAE=0.738, val_loss=0.738]Epoch 11:   1%|          | 12/1000 [11:10<15:15:24, 55.59s/it, lr=0.001, test_MAE=0.783, time=55.5, train_MAE=0.484, train_loss=0.484, val_MAE=0.738, val_loss=0.738]Epoch 12:   1%|          | 12/1000 [11:10<15:15:24, 55.59s/it, lr=0.001, test_MAE=0.783, time=55.5, train_MAE=0.484, train_loss=0.484, val_MAE=0.738, val_loss=0.738]Epoch 12:   1%|          | 12/1000 [12:06<15:15:24, 55.59s/it, lr=0.001, test_MAE=0.761, time=55.8, train_MAE=0.459, train_loss=0.459, val_MAE=0.715, val_loss=0.715]Epoch 12:   1%|▏         | 13/1000 [12:06<15:15:50, 55.67s/it, lr=0.001, test_MAE=0.761, time=55.8, train_MAE=0.459, train_loss=0.459, val_MAE=0.715, val_loss=0.715]Epoch 13:   1%|▏         | 13/1000 [12:06<15:15:50, 55.67s/it, lr=0.001, test_MAE=0.761, time=55.8, train_MAE=0.459, train_loss=0.459, val_MAE=0.715, val_loss=0.715]Epoch 13:   1%|▏         | 13/1000 [13:01<15:15:50, 55.67s/it, lr=0.001, test_MAE=0.772, time=55.4, train_MAE=0.478, train_loss=0.478, val_MAE=0.731, val_loss=0.731]Epoch 13:   1%|▏         | 14/1000 [13:01<15:13:55, 55.61s/it, lr=0.001, test_MAE=0.772, time=55.4, train_MAE=0.478, train_loss=0.478, val_MAE=0.731, val_loss=0.731]Epoch 14:   1%|▏         | 14/1000 [13:01<15:13:55, 55.61s/it, lr=0.001, test_MAE=0.772, time=55.4, train_MAE=0.478, train_loss=0.478, val_MAE=0.731, val_loss=0.731]Epoch 14:   1%|▏         | 14/1000 [13:57<15:13:55, 55.61s/it, lr=0.001, test_MAE=0.764, time=55.6, train_MAE=0.444, train_loss=0.444, val_MAE=0.718, val_loss=0.718]Epoch 14:   2%|▏         | 15/1000 [13:57<15:12:55, 55.61s/it, lr=0.001, test_MAE=0.764, time=55.6, train_MAE=0.444, train_loss=0.444, val_MAE=0.718, val_loss=0.718]Epoch 15:   2%|▏         | 15/1000 [13:57<15:12:55, 55.61s/it, lr=0.001, test_MAE=0.764, time=55.6, train_MAE=0.444, train_loss=0.444, val_MAE=0.718, val_loss=0.718]Epoch 15:   2%|▏         | 15/1000 [14:52<15:12:55, 55.61s/it, lr=0.001, test_MAE=0.801, time=55.9, train_MAE=0.42, train_loss=0.42, val_MAE=0.759, val_loss=0.759]  Epoch 15:   2%|▏         | 16/1000 [14:53<15:13:26, 55.70s/it, lr=0.001, test_MAE=0.801, time=55.9, train_MAE=0.42, train_loss=0.42, val_MAE=0.759, val_loss=0.759]Epoch 16:   2%|▏         | 16/1000 [14:53<15:13:26, 55.70s/it, lr=0.001, test_MAE=0.801, time=55.9, train_MAE=0.42, train_loss=0.42, val_MAE=0.759, val_loss=0.759]Epoch 16:   2%|▏         | 16/1000 [15:49<15:13:26, 55.70s/it, lr=0.001, test_MAE=0.779, time=56.7, train_MAE=0.474, train_loss=0.474, val_MAE=0.733, val_loss=0.733]Epoch    17: reducing learning rate of group 0 to 5.0000e-04.
Epoch 16:   2%|▏         | 17/1000 [15:49<15:17:36, 56.01s/it, lr=0.001, test_MAE=0.779, time=56.7, train_MAE=0.474, train_loss=0.474, val_MAE=0.733, val_loss=0.733]Epoch 17:   2%|▏         | 17/1000 [15:49<15:17:36, 56.01s/it, lr=0.001, test_MAE=0.779, time=56.7, train_MAE=0.474, train_loss=0.474, val_MAE=0.733, val_loss=0.733]Epoch 17:   2%|▏         | 17/1000 [16:47<15:17:36, 56.01s/it, lr=0.0005, test_MAE=0.769, time=57.5, train_MAE=0.375, train_loss=0.375, val_MAE=0.727, val_loss=0.727]Epoch 17:   2%|▏         | 18/1000 [16:47<15:24:07, 56.46s/it, lr=0.0005, test_MAE=0.769, time=57.5, train_MAE=0.375, train_loss=0.375, val_MAE=0.727, val_loss=0.727]Epoch 18:   2%|▏         | 18/1000 [16:47<15:24:07, 56.46s/it, lr=0.0005, test_MAE=0.769, time=57.5, train_MAE=0.375, train_loss=0.375, val_MAE=0.727, val_loss=0.727]Epoch 18:   2%|▏         | 18/1000 [17:44<15:24:07, 56.46s/it, lr=0.0005, test_MAE=0.767, time=57.5, train_MAE=0.377, train_loss=0.377, val_MAE=0.723, val_loss=0.723]Epoch 18:   2%|▏         | 19/1000 [17:44<15:28:34, 56.79s/it, lr=0.0005, test_MAE=0.767, time=57.5, train_MAE=0.377, train_loss=0.377, val_MAE=0.723, val_loss=0.723]Epoch 19:   2%|▏         | 19/1000 [17:44<15:28:34, 56.79s/it, lr=0.0005, test_MAE=0.767, time=57.5, train_MAE=0.377, train_loss=0.377, val_MAE=0.723, val_loss=0.723]Epoch 19:   2%|▏         | 19/1000 [18:41<15:28:34, 56.79s/it, lr=0.0005, test_MAE=0.767, time=56.3, train_MAE=0.366, train_loss=0.366, val_MAE=0.725, val_loss=0.725]Epoch 19:   2%|▏         | 20/1000 [18:41<15:25:10, 56.64s/it, lr=0.0005, test_MAE=0.767, time=56.3, train_MAE=0.366, train_loss=0.366, val_MAE=0.725, val_loss=0.725]Epoch 20:   2%|▏         | 20/1000 [18:41<15:25:10, 56.64s/it, lr=0.0005, test_MAE=0.767, time=56.3, train_MAE=0.366, train_loss=0.366, val_MAE=0.725, val_loss=0.725]Epoch 20:   2%|▏         | 20/1000 [19:35<15:25:10, 56.64s/it, lr=0.0005, test_MAE=0.771, time=54.3, train_MAE=0.351, train_loss=0.351, val_MAE=0.729, val_loss=0.729]Epoch 20:   2%|▏         | 21/1000 [19:35<15:13:00, 55.96s/it, lr=0.0005, test_MAE=0.771, time=54.3, train_MAE=0.351, train_loss=0.351, val_MAE=0.729, val_loss=0.729]Epoch 21:   2%|▏         | 21/1000 [19:35<15:13:00, 55.96s/it, lr=0.0005, test_MAE=0.771, time=54.3, train_MAE=0.351, train_loss=0.351, val_MAE=0.729, val_loss=0.729]Epoch 21:   2%|▏         | 21/1000 [20:29<15:13:00, 55.96s/it, lr=0.0005, test_MAE=0.773, time=54.3, train_MAE=0.356, train_loss=0.356, val_MAE=0.731, val_loss=0.731]Epoch 21:   2%|▏         | 22/1000 [20:29<15:04:14, 55.47s/it, lr=0.0005, test_MAE=0.773, time=54.3, train_MAE=0.356, train_loss=0.356, val_MAE=0.731, val_loss=0.731]Epoch 22:   2%|▏         | 22/1000 [20:29<15:04:14, 55.47s/it, lr=0.0005, test_MAE=0.773, time=54.3, train_MAE=0.356, train_loss=0.356, val_MAE=0.731, val_loss=0.731]Epoch 22:   2%|▏         | 22/1000 [21:25<15:04:14, 55.47s/it, lr=0.0005, test_MAE=0.778, time=55.3, train_MAE=0.342, train_loss=0.342, val_MAE=0.73, val_loss=0.73]  Epoch    23: reducing learning rate of group 0 to 2.5000e-04.
Epoch 22:   2%|▏         | 23/1000 [21:25<15:02:40, 55.44s/it, lr=0.0005, test_MAE=0.778, time=55.3, train_MAE=0.342, train_loss=0.342, val_MAE=0.73, val_loss=0.73]Epoch 23:   2%|▏         | 23/1000 [21:25<15:02:40, 55.44s/it, lr=0.0005, test_MAE=0.778, time=55.3, train_MAE=0.342, train_loss=0.342, val_MAE=0.73, val_loss=0.73]Epoch 23:   2%|▏         | 23/1000 [22:20<15:02:40, 55.44s/it, lr=0.00025, test_MAE=0.772, time=55.7, train_MAE=0.318, train_loss=0.318, val_MAE=0.731, val_loss=0.731]Epoch 23:   2%|▏         | 24/1000 [22:20<15:02:55, 55.51s/it, lr=0.00025, test_MAE=0.772, time=55.7, train_MAE=0.318, train_loss=0.318, val_MAE=0.731, val_loss=0.731]Epoch 24:   2%|▏         | 24/1000 [22:20<15:02:55, 55.51s/it, lr=0.00025, test_MAE=0.772, time=55.7, train_MAE=0.318, train_loss=0.318, val_MAE=0.731, val_loss=0.731]Epoch 24:   2%|▏         | 24/1000 [23:16<15:02:55, 55.51s/it, lr=0.00025, test_MAE=0.772, time=55.8, train_MAE=0.307, train_loss=0.307, val_MAE=0.734, val_loss=0.734]Epoch 24:   2%|▎         | 25/1000 [23:16<15:03:42, 55.61s/it, lr=0.00025, test_MAE=0.772, time=55.8, train_MAE=0.307, train_loss=0.307, val_MAE=0.734, val_loss=0.734]Epoch 25:   2%|▎         | 25/1000 [23:16<15:03:42, 55.61s/it, lr=0.00025, test_MAE=0.772, time=55.8, train_MAE=0.307, train_loss=0.307, val_MAE=0.734, val_loss=0.734]Epoch 25:   2%|▎         | 25/1000 [24:11<15:03:42, 55.61s/it, lr=0.00025, test_MAE=0.768, time=55.2, train_MAE=0.317, train_loss=0.317, val_MAE=0.732, val_loss=0.732]Epoch 25:   3%|▎         | 26/1000 [24:11<15:00:46, 55.49s/it, lr=0.00025, test_MAE=0.768, time=55.2, train_MAE=0.317, train_loss=0.317, val_MAE=0.732, val_loss=0.732]Epoch 26:   3%|▎         | 26/1000 [24:11<15:00:46, 55.49s/it, lr=0.00025, test_MAE=0.768, time=55.2, train_MAE=0.317, train_loss=0.317, val_MAE=0.732, val_loss=0.732]Epoch 26:   3%|▎         | 26/1000 [25:07<15:00:46, 55.49s/it, lr=0.00025, test_MAE=0.773, time=55.1, train_MAE=0.308, train_loss=0.308, val_MAE=0.732, val_loss=0.732]Epoch 26:   3%|▎         | 27/1000 [25:07<14:58:19, 55.40s/it, lr=0.00025, test_MAE=0.773, time=55.1, train_MAE=0.308, train_loss=0.308, val_MAE=0.732, val_loss=0.732]Epoch 27:   3%|▎         | 27/1000 [25:07<14:58:19, 55.40s/it, lr=0.00025, test_MAE=0.773, time=55.1, train_MAE=0.308, train_loss=0.308, val_MAE=0.732, val_loss=0.732]Epoch 27:   3%|▎         | 27/1000 [26:02<14:58:19, 55.40s/it, lr=0.00025, test_MAE=0.771, time=55.7, train_MAE=0.312, train_loss=0.312, val_MAE=0.733, val_loss=0.733]Epoch 27:   3%|▎         | 28/1000 [26:02<14:58:55, 55.49s/it, lr=0.00025, test_MAE=0.771, time=55.7, train_MAE=0.312, train_loss=0.312, val_MAE=0.733, val_loss=0.733]Epoch 28:   3%|▎         | 28/1000 [26:02<14:58:55, 55.49s/it, lr=0.00025, test_MAE=0.771, time=55.7, train_MAE=0.312, train_loss=0.312, val_MAE=0.733, val_loss=0.733]Epoch 28:   3%|▎         | 28/1000 [26:58<14:58:55, 55.49s/it, lr=0.00025, test_MAE=0.772, time=56, train_MAE=0.32, train_loss=0.32, val_MAE=0.735, val_loss=0.735]    Epoch    29: reducing learning rate of group 0 to 1.2500e-04.
Epoch 28:   3%|▎         | 29/1000 [26:58<15:00:30, 55.64s/it, lr=0.00025, test_MAE=0.772, time=56, train_MAE=0.32, train_loss=0.32, val_MAE=0.735, val_loss=0.735]Epoch 29:   3%|▎         | 29/1000 [26:58<15:00:30, 55.64s/it, lr=0.00025, test_MAE=0.772, time=56, train_MAE=0.32, train_loss=0.32, val_MAE=0.735, val_loss=0.735]Epoch 29:   3%|▎         | 29/1000 [27:54<15:00:30, 55.64s/it, lr=0.000125, test_MAE=0.776, time=55.7, train_MAE=0.302, train_loss=0.302, val_MAE=0.737, val_loss=0.737]Epoch 29:   3%|▎         | 30/1000 [27:54<14:59:50, 55.66s/it, lr=0.000125, test_MAE=0.776, time=55.7, train_MAE=0.302, train_loss=0.302, val_MAE=0.737, val_loss=0.737]Epoch 30:   3%|▎         | 30/1000 [27:54<14:59:50, 55.66s/it, lr=0.000125, test_MAE=0.776, time=55.7, train_MAE=0.302, train_loss=0.302, val_MAE=0.737, val_loss=0.737]Epoch 30:   3%|▎         | 30/1000 [28:50<14:59:50, 55.66s/it, lr=0.000125, test_MAE=0.773, time=56.1, train_MAE=0.285, train_loss=0.285, val_MAE=0.734, val_loss=0.734]Epoch 30:   3%|▎         | 31/1000 [28:50<15:00:54, 55.78s/it, lr=0.000125, test_MAE=0.773, time=56.1, train_MAE=0.285, train_loss=0.285, val_MAE=0.734, val_loss=0.734]Epoch 31:   3%|▎         | 31/1000 [28:50<15:00:54, 55.78s/it, lr=0.000125, test_MAE=0.773, time=56.1, train_MAE=0.285, train_loss=0.285, val_MAE=0.734, val_loss=0.734]Epoch 31:   3%|▎         | 31/1000 [29:48<15:00:54, 55.78s/it, lr=0.000125, test_MAE=0.773, time=57.8, train_MAE=0.294, train_loss=0.294, val_MAE=0.733, val_loss=0.733]Epoch 31:   3%|▎         | 32/1000 [29:48<15:09:59, 56.40s/it, lr=0.000125, test_MAE=0.773, time=57.8, train_MAE=0.294, train_loss=0.294, val_MAE=0.733, val_loss=0.733]Epoch 32:   3%|▎         | 32/1000 [29:48<15:09:59, 56.40s/it, lr=0.000125, test_MAE=0.773, time=57.8, train_MAE=0.294, train_loss=0.294, val_MAE=0.733, val_loss=0.733]Epoch 32:   3%|▎         | 32/1000 [30:46<15:09:59, 56.40s/it, lr=0.000125, test_MAE=0.775, time=57.9, train_MAE=0.283, train_loss=0.283, val_MAE=0.734, val_loss=0.734]Epoch 32:   3%|▎         | 33/1000 [30:46<15:16:24, 56.86s/it, lr=0.000125, test_MAE=0.775, time=57.9, train_MAE=0.283, train_loss=0.283, val_MAE=0.734, val_loss=0.734]Epoch 33:   3%|▎         | 33/1000 [30:46<15:16:24, 56.86s/it, lr=0.000125, test_MAE=0.775, time=57.9, train_MAE=0.283, train_loss=0.283, val_MAE=0.734, val_loss=0.734]Epoch 33:   3%|▎         | 33/1000 [31:42<15:16:24, 56.86s/it, lr=0.000125, test_MAE=0.775, time=56.4, train_MAE=0.282, train_loss=0.282, val_MAE=0.737, val_loss=0.737]Epoch 33:   3%|▎         | 34/1000 [31:42<15:13:26, 56.74s/it, lr=0.000125, test_MAE=0.775, time=56.4, train_MAE=0.282, train_loss=0.282, val_MAE=0.737, val_loss=0.737]Epoch 34:   3%|▎         | 34/1000 [31:42<15:13:26, 56.74s/it, lr=0.000125, test_MAE=0.775, time=56.4, train_MAE=0.282, train_loss=0.282, val_MAE=0.737, val_loss=0.737]Epoch 34:   3%|▎         | 34/1000 [32:37<15:13:26, 56.74s/it, lr=0.000125, test_MAE=0.781, time=54.5, train_MAE=0.28, train_loss=0.28, val_MAE=0.745, val_loss=0.745]  Epoch    35: reducing learning rate of group 0 to 6.2500e-05.
Epoch 34:   4%|▎         | 35/1000 [32:37<15:01:45, 56.07s/it, lr=0.000125, test_MAE=0.781, time=54.5, train_MAE=0.28, train_loss=0.28, val_MAE=0.745, val_loss=0.745]Epoch 35:   4%|▎         | 35/1000 [32:37<15:01:45, 56.07s/it, lr=0.000125, test_MAE=0.781, time=54.5, train_MAE=0.28, train_loss=0.28, val_MAE=0.745, val_loss=0.745]Epoch 35:   4%|▎         | 35/1000 [33:32<15:01:45, 56.07s/it, lr=6.25e-5, test_MAE=0.775, time=55.5, train_MAE=0.265, train_loss=0.265, val_MAE=0.734, val_loss=0.734]Epoch 35:   4%|▎         | 36/1000 [33:32<14:58:10, 55.90s/it, lr=6.25e-5, test_MAE=0.775, time=55.5, train_MAE=0.265, train_loss=0.265, val_MAE=0.734, val_loss=0.734]Epoch 36:   4%|▎         | 36/1000 [33:32<14:58:10, 55.90s/it, lr=6.25e-5, test_MAE=0.775, time=55.5, train_MAE=0.265, train_loss=0.265, val_MAE=0.734, val_loss=0.734]Epoch 36:   4%|▎         | 36/1000 [34:28<14:58:10, 55.90s/it, lr=6.25e-5, test_MAE=0.775, time=55.6, train_MAE=0.278, train_loss=0.278, val_MAE=0.735, val_loss=0.735]Epoch 36:   4%|▎         | 37/1000 [34:28<14:56:02, 55.83s/it, lr=6.25e-5, test_MAE=0.775, time=55.6, train_MAE=0.278, train_loss=0.278, val_MAE=0.735, val_loss=0.735]Epoch 37:   4%|▎         | 37/1000 [34:28<14:56:02, 55.83s/it, lr=6.25e-5, test_MAE=0.775, time=55.6, train_MAE=0.278, train_loss=0.278, val_MAE=0.735, val_loss=0.735]Epoch 37:   4%|▎         | 37/1000 [35:24<14:56:02, 55.83s/it, lr=6.25e-5, test_MAE=0.777, time=55.5, train_MAE=0.275, train_loss=0.275, val_MAE=0.736, val_loss=0.736]Epoch 37:   4%|▍         | 38/1000 [35:24<14:53:47, 55.75s/it, lr=6.25e-5, test_MAE=0.777, time=55.5, train_MAE=0.275, train_loss=0.275, val_MAE=0.736, val_loss=0.736]Epoch 38:   4%|▍         | 38/1000 [35:24<14:53:47, 55.75s/it, lr=6.25e-5, test_MAE=0.777, time=55.5, train_MAE=0.275, train_loss=0.275, val_MAE=0.736, val_loss=0.736]Epoch 38:   4%|▍         | 38/1000 [36:19<14:53:47, 55.75s/it, lr=6.25e-5, test_MAE=0.776, time=55.2, train_MAE=0.273, train_loss=0.273, val_MAE=0.735, val_loss=0.735]Epoch 38:   4%|▍         | 39/1000 [36:19<14:50:32, 55.60s/it, lr=6.25e-5, test_MAE=0.776, time=55.2, train_MAE=0.273, train_loss=0.273, val_MAE=0.735, val_loss=0.735]Epoch 39:   4%|▍         | 39/1000 [36:19<14:50:32, 55.60s/it, lr=6.25e-5, test_MAE=0.776, time=55.2, train_MAE=0.273, train_loss=0.273, val_MAE=0.735, val_loss=0.735]Epoch 39:   4%|▍         | 39/1000 [37:14<14:50:32, 55.60s/it, lr=6.25e-5, test_MAE=0.776, time=55.7, train_MAE=0.274, train_loss=0.274, val_MAE=0.735, val_loss=0.735]Epoch 39:   4%|▍         | 40/1000 [37:14<14:49:59, 55.62s/it, lr=6.25e-5, test_MAE=0.776, time=55.7, train_MAE=0.274, train_loss=0.274, val_MAE=0.735, val_loss=0.735]Epoch 40:   4%|▍         | 40/1000 [37:14<14:49:59, 55.62s/it, lr=6.25e-5, test_MAE=0.776, time=55.7, train_MAE=0.274, train_loss=0.274, val_MAE=0.735, val_loss=0.735]Epoch 40:   4%|▍         | 40/1000 [38:10<14:49:59, 55.62s/it, lr=6.25e-5, test_MAE=0.778, time=55.6, train_MAE=0.279, train_loss=0.279, val_MAE=0.738, val_loss=0.738]Epoch    41: reducing learning rate of group 0 to 3.1250e-05.
Epoch 40:   4%|▍         | 41/1000 [38:10<14:49:21, 55.64s/it, lr=6.25e-5, test_MAE=0.778, time=55.6, train_MAE=0.279, train_loss=0.279, val_MAE=0.738, val_loss=0.738]Epoch 41:   4%|▍         | 41/1000 [38:10<14:49:21, 55.64s/it, lr=6.25e-5, test_MAE=0.778, time=55.6, train_MAE=0.279, train_loss=0.279, val_MAE=0.738, val_loss=0.738]Epoch 41:   4%|▍         | 41/1000 [39:06<14:49:21, 55.64s/it, lr=3.13e-5, test_MAE=0.775, time=55.7, train_MAE=0.277, train_loss=0.277, val_MAE=0.737, val_loss=0.737]Epoch 41:   4%|▍         | 42/1000 [39:06<14:48:50, 55.67s/it, lr=3.13e-5, test_MAE=0.775, time=55.7, train_MAE=0.277, train_loss=0.277, val_MAE=0.737, val_loss=0.737]Epoch 42:   4%|▍         | 42/1000 [39:06<14:48:50, 55.67s/it, lr=3.13e-5, test_MAE=0.775, time=55.7, train_MAE=0.277, train_loss=0.277, val_MAE=0.737, val_loss=0.737]Epoch 42:   4%|▍         | 42/1000 [40:01<14:48:50, 55.67s/it, lr=3.13e-5, test_MAE=0.778, time=55.5, train_MAE=0.267, train_loss=0.267, val_MAE=0.739, val_loss=0.739]Epoch 42:   4%|▍         | 43/1000 [40:01<14:47:34, 55.65s/it, lr=3.13e-5, test_MAE=0.778, time=55.5, train_MAE=0.267, train_loss=0.267, val_MAE=0.739, val_loss=0.739]Epoch 43:   4%|▍         | 43/1000 [40:01<14:47:34, 55.65s/it, lr=3.13e-5, test_MAE=0.778, time=55.5, train_MAE=0.267, train_loss=0.267, val_MAE=0.739, val_loss=0.739]Epoch 43:   4%|▍         | 43/1000 [40:58<14:47:34, 55.65s/it, lr=3.13e-5, test_MAE=0.782, time=56.1, train_MAE=0.275, train_loss=0.275, val_MAE=0.745, val_loss=0.745]Epoch 43:   4%|▍         | 44/1000 [40:58<14:49:26, 55.82s/it, lr=3.13e-5, test_MAE=0.782, time=56.1, train_MAE=0.275, train_loss=0.275, val_MAE=0.745, val_loss=0.745]Epoch 44:   4%|▍         | 44/1000 [40:58<14:49:26, 55.82s/it, lr=3.13e-5, test_MAE=0.782, time=56.1, train_MAE=0.275, train_loss=0.275, val_MAE=0.745, val_loss=0.745]Epoch 44:   4%|▍         | 44/1000 [41:55<14:49:26, 55.82s/it, lr=3.13e-5, test_MAE=0.777, time=57.8, train_MAE=0.27, train_loss=0.27, val_MAE=0.738, val_loss=0.738]  Epoch 44:   4%|▍         | 45/1000 [41:56<14:57:55, 56.41s/it, lr=3.13e-5, test_MAE=0.777, time=57.8, train_MAE=0.27, train_loss=0.27, val_MAE=0.738, val_loss=0.738]Epoch 45:   4%|▍         | 45/1000 [41:56<14:57:55, 56.41s/it, lr=3.13e-5, test_MAE=0.777, time=57.8, train_MAE=0.27, train_loss=0.27, val_MAE=0.738, val_loss=0.738]Epoch 45:   4%|▍         | 45/1000 [42:53<14:57:55, 56.41s/it, lr=3.13e-5, test_MAE=0.777, time=57.5, train_MAE=0.268, train_loss=0.268, val_MAE=0.737, val_loss=0.737]Epoch 45:   5%|▍         | 46/1000 [42:53<15:02:46, 56.78s/it, lr=3.13e-5, test_MAE=0.777, time=57.5, train_MAE=0.268, train_loss=0.268, val_MAE=0.737, val_loss=0.737]Epoch 46:   5%|▍         | 46/1000 [42:53<15:02:46, 56.78s/it, lr=3.13e-5, test_MAE=0.777, time=57.5, train_MAE=0.268, train_loss=0.268, val_MAE=0.737, val_loss=0.737]Epoch 46:   5%|▍         | 46/1000 [43:50<15:02:46, 56.78s/it, lr=3.13e-5, test_MAE=0.779, time=57.2, train_MAE=0.275, train_loss=0.275, val_MAE=0.742, val_loss=0.742]Epoch    47: reducing learning rate of group 0 to 1.5625e-05.
Epoch 46:   5%|▍         | 47/1000 [43:50<15:04:00, 56.92s/it, lr=3.13e-5, test_MAE=0.779, time=57.2, train_MAE=0.275, train_loss=0.275, val_MAE=0.742, val_loss=0.742]Epoch 47:   5%|▍         | 47/1000 [43:50<15:04:00, 56.92s/it, lr=3.13e-5, test_MAE=0.779, time=57.2, train_MAE=0.275, train_loss=0.275, val_MAE=0.742, val_loss=0.742]Epoch 47:   5%|▍         | 47/1000 [44:46<15:04:00, 56.92s/it, lr=1.56e-5, test_MAE=0.778, time=55.3, train_MAE=0.267, train_loss=0.267, val_MAE=0.737, val_loss=0.737]Epoch 47:   5%|▍         | 48/1000 [44:46<14:55:25, 56.43s/it, lr=1.56e-5, test_MAE=0.778, time=55.3, train_MAE=0.267, train_loss=0.267, val_MAE=0.737, val_loss=0.737]Epoch 48:   5%|▍         | 48/1000 [44:46<14:55:25, 56.43s/it, lr=1.56e-5, test_MAE=0.778, time=55.3, train_MAE=0.267, train_loss=0.267, val_MAE=0.737, val_loss=0.737]Epoch 48:   5%|▍         | 48/1000 [45:41<14:55:25, 56.43s/it, lr=1.56e-5, test_MAE=0.776, time=55.6, train_MAE=0.263, train_loss=0.263, val_MAE=0.736, val_loss=0.736]Epoch 48:   5%|▍         | 49/1000 [45:41<14:50:41, 56.20s/it, lr=1.56e-5, test_MAE=0.776, time=55.6, train_MAE=0.263, train_loss=0.263, val_MAE=0.736, val_loss=0.736]Epoch 49:   5%|▍         | 49/1000 [45:41<14:50:41, 56.20s/it, lr=1.56e-5, test_MAE=0.776, time=55.6, train_MAE=0.263, train_loss=0.263, val_MAE=0.736, val_loss=0.736]Epoch 49:   5%|▍         | 49/1000 [46:37<14:50:41, 56.20s/it, lr=1.56e-5, test_MAE=0.778, time=55.4, train_MAE=0.273, train_loss=0.273, val_MAE=0.739, val_loss=0.739]Epoch 49:   5%|▌         | 50/1000 [46:37<14:46:19, 55.98s/it, lr=1.56e-5, test_MAE=0.778, time=55.4, train_MAE=0.273, train_loss=0.273, val_MAE=0.739, val_loss=0.739]Epoch 50:   5%|▌         | 50/1000 [46:37<14:46:19, 55.98s/it, lr=1.56e-5, test_MAE=0.778, time=55.4, train_MAE=0.273, train_loss=0.273, val_MAE=0.739, val_loss=0.739]Epoch 50:   5%|▌         | 50/1000 [47:32<14:46:19, 55.98s/it, lr=1.56e-5, test_MAE=0.778, time=55.1, train_MAE=0.264, train_loss=0.264, val_MAE=0.738, val_loss=0.738]Epoch 50:   5%|▌         | 51/1000 [47:32<14:41:20, 55.72s/it, lr=1.56e-5, test_MAE=0.778, time=55.1, train_MAE=0.264, train_loss=0.264, val_MAE=0.738, val_loss=0.738]Epoch 51:   5%|▌         | 51/1000 [47:32<14:41:20, 55.72s/it, lr=1.56e-5, test_MAE=0.778, time=55.1, train_MAE=0.264, train_loss=0.264, val_MAE=0.738, val_loss=0.738]Epoch 51:   5%|▌         | 51/1000 [48:27<14:41:20, 55.72s/it, lr=1.56e-5, test_MAE=0.783, time=55.4, train_MAE=0.267, train_loss=0.267, val_MAE=0.742, val_loss=0.742]Epoch 51:   5%|▌         | 52/1000 [48:27<14:38:59, 55.63s/it, lr=1.56e-5, test_MAE=0.783, time=55.4, train_MAE=0.267, train_loss=0.267, val_MAE=0.742, val_loss=0.742]Epoch 52:   5%|▌         | 52/1000 [48:27<14:38:59, 55.63s/it, lr=1.56e-5, test_MAE=0.783, time=55.4, train_MAE=0.267, train_loss=0.267, val_MAE=0.742, val_loss=0.742]Epoch 52:   5%|▌         | 52/1000 [49:23<14:38:59, 55.63s/it, lr=1.56e-5, test_MAE=0.779, time=55.3, train_MAE=0.261, train_loss=0.261, val_MAE=0.741, val_loss=0.741]Epoch    53: reducing learning rate of group 0 to 7.8125e-06.

!! LR EQUAL TO MIN LR SET.
Epoch 52:   5%|▌         | 52/1000 [49:23<15:00:21, 56.98s/it, lr=1.56e-5, test_MAE=0.779, time=55.3, train_MAE=0.261, train_loss=0.261, val_MAE=0.741, val_loss=0.741]
Test MAE: 0.7789
Train MAE: 0.2090
Convergence Time (Epochs): 52.0000
TOTAL TIME TAKEN: 2992.7155s
AVG TIME PER EPOCH: 55.8688s
Traceback (most recent call last):
  File "main_molecules_graph_regression.py", line 398, in <module>
    main()
  File "main_molecules_graph_regression.py", line 395, in main
    train_val_pipeline(MODEL_NAME, dataset, params, net_params, dirs)
  File "main_molecules_graph_regression.py", line 217, in train_val_pipeline
    test_mae, train_mae, epoch, (time.time() - t0) / 3600, np.mean(per_epoch_time)))
  File "/home/bsw38/.conda/envs/benchmark_gnn_2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1347, in __repr__
    mod_str = repr(module)
  File "/home/bsw38/.conda/envs/benchmark_gnn_2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1347, in __repr__
    mod_str = repr(module)
  File "/vast/palmer/pi/krishnaswamy_smita/bsw38/GraphML/ChebNetGNNs/Benchmark-gnn/layers/Spec_layer.py", line 228, in __repr__
  File "/home/bsw38/.conda/envs/benchmark_gnn_2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'SpecLayer' object has no attribute '_k'

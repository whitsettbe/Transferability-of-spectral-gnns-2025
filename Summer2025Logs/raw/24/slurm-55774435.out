I'm echoing to stdout
I'm echoing to stderr
My JobID is 55774435
I have 4 CPUs on node r108u25n01
Using backend: pytorch
cuda not available
[I] Loading dataset ZINC...
train, test, val sizes : 10000 1000 1000
[I] Finished loading.
[I] Data load time: 5.0684s
Training Graphs:  10000
Validation Graphs:  1000
Test Graphs:  1000
  0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [01:01<?, ?it/s, lr=0.001, test_MAE=1.16, time=61.4, train_MAE=1.37, train_loss=1.37, val_MAE=1.08, val_loss=1.08]Epoch 0:   0%|          | 1/1000 [01:01<17:02:29, 61.41s/it, lr=0.001, test_MAE=1.16, time=61.4, train_MAE=1.37, train_loss=1.37, val_MAE=1.08, val_loss=1.08]Epoch 1:   0%|          | 1/1000 [01:01<17:02:29, 61.41s/it, lr=0.001, test_MAE=1.16, time=61.4, train_MAE=1.37, train_loss=1.37, val_MAE=1.08, val_loss=1.08]Epoch 1:   0%|          | 1/1000 [01:56<17:02:29, 61.41s/it, lr=0.001, test_MAE=0.932, time=55.1, train_MAE=0.955, train_loss=0.955, val_MAE=0.862, val_loss=0.862]Epoch 1:   0%|          | 2/1000 [01:56<16:29:54, 59.51s/it, lr=0.001, test_MAE=0.932, time=55.1, train_MAE=0.955, train_loss=0.955, val_MAE=0.862, val_loss=0.862]Epoch 2:   0%|          | 2/1000 [01:56<16:29:54, 59.51s/it, lr=0.001, test_MAE=0.932, time=55.1, train_MAE=0.955, train_loss=0.955, val_MAE=0.862, val_loss=0.862]Epoch 2:   0%|          | 2/1000 [02:50<16:29:54, 59.51s/it, lr=0.001, test_MAE=0.892, time=54.4, train_MAE=0.812, train_loss=0.812, val_MAE=0.827, val_loss=0.827]Epoch 2:   0%|          | 3/1000 [02:50<16:03:24, 57.98s/it, lr=0.001, test_MAE=0.892, time=54.4, train_MAE=0.812, train_loss=0.812, val_MAE=0.827, val_loss=0.827]Epoch 3:   0%|          | 3/1000 [02:50<16:03:24, 57.98s/it, lr=0.001, test_MAE=0.892, time=54.4, train_MAE=0.812, train_loss=0.812, val_MAE=0.827, val_loss=0.827]Epoch 3:   0%|          | 3/1000 [03:45<16:03:24, 57.98s/it, lr=0.001, test_MAE=0.835, time=54.6, train_MAE=0.729, train_loss=0.729, val_MAE=0.783, val_loss=0.783]Epoch 3:   0%|          | 4/1000 [03:45<15:45:42, 56.97s/it, lr=0.001, test_MAE=0.835, time=54.6, train_MAE=0.729, train_loss=0.729, val_MAE=0.783, val_loss=0.783]Epoch 4:   0%|          | 4/1000 [03:45<15:45:42, 56.97s/it, lr=0.001, test_MAE=0.835, time=54.6, train_MAE=0.729, train_loss=0.729, val_MAE=0.783, val_loss=0.783]Epoch 4:   0%|          | 4/1000 [04:40<15:45:42, 56.97s/it, lr=0.001, test_MAE=0.8, time=55.5, train_MAE=0.675, train_loss=0.675, val_MAE=0.752, val_loss=0.752]  Epoch 4:   0%|          | 5/1000 [04:41<15:37:29, 56.53s/it, lr=0.001, test_MAE=0.8, time=55.5, train_MAE=0.675, train_loss=0.675, val_MAE=0.752, val_loss=0.752]Epoch 5:   0%|          | 5/1000 [04:41<15:37:29, 56.53s/it, lr=0.001, test_MAE=0.8, time=55.5, train_MAE=0.675, train_loss=0.675, val_MAE=0.752, val_loss=0.752]Epoch 5:   0%|          | 5/1000 [05:36<15:37:29, 56.53s/it, lr=0.001, test_MAE=0.778, time=55.8, train_MAE=0.635, train_loss=0.635, val_MAE=0.725, val_loss=0.725]Epoch 5:   1%|          | 6/1000 [05:36<15:33:10, 56.33s/it, lr=0.001, test_MAE=0.778, time=55.8, train_MAE=0.635, train_loss=0.635, val_MAE=0.725, val_loss=0.725]Epoch 6:   1%|          | 6/1000 [05:36<15:33:10, 56.33s/it, lr=0.001, test_MAE=0.778, time=55.8, train_MAE=0.635, train_loss=0.635, val_MAE=0.725, val_loss=0.725]Epoch 6:   1%|          | 6/1000 [06:32<15:33:10, 56.33s/it, lr=0.001, test_MAE=0.768, time=56.1, train_MAE=0.6, train_loss=0.6, val_MAE=0.719, val_loss=0.719]    Epoch 6:   1%|          | 7/1000 [06:32<15:30:59, 56.25s/it, lr=0.001, test_MAE=0.768, time=56.1, train_MAE=0.6, train_loss=0.6, val_MAE=0.719, val_loss=0.719]Epoch 7:   1%|          | 7/1000 [06:32<15:30:59, 56.25s/it, lr=0.001, test_MAE=0.768, time=56.1, train_MAE=0.6, train_loss=0.6, val_MAE=0.719, val_loss=0.719]Epoch 7:   1%|          | 7/1000 [07:28<15:30:59, 56.25s/it, lr=0.001, test_MAE=0.826, time=56, train_MAE=0.558, train_loss=0.558, val_MAE=0.771, val_loss=0.771]Epoch 7:   1%|          | 8/1000 [07:28<15:28:50, 56.18s/it, lr=0.001, test_MAE=0.826, time=56, train_MAE=0.558, train_loss=0.558, val_MAE=0.771, val_loss=0.771]Epoch 8:   1%|          | 8/1000 [07:28<15:28:50, 56.18s/it, lr=0.001, test_MAE=0.826, time=56, train_MAE=0.558, train_loss=0.558, val_MAE=0.771, val_loss=0.771]Epoch 8:   1%|          | 8/1000 [08:25<15:28:50, 56.18s/it, lr=0.001, test_MAE=0.831, time=56.3, train_MAE=0.565, train_loss=0.565, val_MAE=0.786, val_loss=0.786]Epoch 8:   1%|          | 9/1000 [08:25<15:28:34, 56.22s/it, lr=0.001, test_MAE=0.831, time=56.3, train_MAE=0.565, train_loss=0.565, val_MAE=0.786, val_loss=0.786]Epoch 9:   1%|          | 9/1000 [08:25<15:28:34, 56.22s/it, lr=0.001, test_MAE=0.831, time=56.3, train_MAE=0.565, train_loss=0.565, val_MAE=0.786, val_loss=0.786]Epoch 9:   1%|          | 9/1000 [09:20<15:28:34, 56.22s/it, lr=0.001, test_MAE=0.764, time=55.1, train_MAE=0.542, train_loss=0.542, val_MAE=0.723, val_loss=0.723]Epoch 9:   1%|          | 10/1000 [09:20<15:22:18, 55.90s/it, lr=0.001, test_MAE=0.764, time=55.1, train_MAE=0.542, train_loss=0.542, val_MAE=0.723, val_loss=0.723]Epoch 10:   1%|          | 10/1000 [09:20<15:22:18, 55.90s/it, lr=0.001, test_MAE=0.764, time=55.1, train_MAE=0.542, train_loss=0.542, val_MAE=0.723, val_loss=0.723]Epoch 10:   1%|          | 10/1000 [10:15<15:22:18, 55.90s/it, lr=0.001, test_MAE=0.755, time=54.8, train_MAE=0.504, train_loss=0.504, val_MAE=0.709, val_loss=0.709]Epoch 10:   1%|          | 11/1000 [10:15<15:16:10, 55.58s/it, lr=0.001, test_MAE=0.755, time=54.8, train_MAE=0.504, train_loss=0.504, val_MAE=0.709, val_loss=0.709]Epoch 11:   1%|          | 11/1000 [10:15<15:16:10, 55.58s/it, lr=0.001, test_MAE=0.755, time=54.8, train_MAE=0.504, train_loss=0.504, val_MAE=0.709, val_loss=0.709]Epoch 11:   1%|          | 11/1000 [11:10<15:16:10, 55.58s/it, lr=0.001, test_MAE=0.783, time=54.8, train_MAE=0.484, train_loss=0.484, val_MAE=0.738, val_loss=0.738]Epoch 11:   1%|          | 12/1000 [11:10<15:11:35, 55.36s/it, lr=0.001, test_MAE=0.783, time=54.8, train_MAE=0.484, train_loss=0.484, val_MAE=0.738, val_loss=0.738]Epoch 12:   1%|          | 12/1000 [11:10<15:11:35, 55.36s/it, lr=0.001, test_MAE=0.783, time=54.8, train_MAE=0.484, train_loss=0.484, val_MAE=0.738, val_loss=0.738]Epoch 12:   1%|          | 12/1000 [12:05<15:11:35, 55.36s/it, lr=0.001, test_MAE=0.761, time=55, train_MAE=0.459, train_loss=0.459, val_MAE=0.715, val_loss=0.715]  Epoch 12:   1%|▏         | 13/1000 [12:05<15:09:04, 55.26s/it, lr=0.001, test_MAE=0.761, time=55, train_MAE=0.459, train_loss=0.459, val_MAE=0.715, val_loss=0.715]Epoch 13:   1%|▏         | 13/1000 [12:05<15:09:04, 55.26s/it, lr=0.001, test_MAE=0.761, time=55, train_MAE=0.459, train_loss=0.459, val_MAE=0.715, val_loss=0.715]Epoch 13:   1%|▏         | 13/1000 [12:59<15:09:04, 55.26s/it, lr=0.001, test_MAE=0.772, time=54.6, train_MAE=0.478, train_loss=0.478, val_MAE=0.731, val_loss=0.731]Epoch 13:   1%|▏         | 14/1000 [12:59<15:04:45, 55.06s/it, lr=0.001, test_MAE=0.772, time=54.6, train_MAE=0.478, train_loss=0.478, val_MAE=0.731, val_loss=0.731]Epoch 14:   1%|▏         | 14/1000 [12:59<15:04:45, 55.06s/it, lr=0.001, test_MAE=0.772, time=54.6, train_MAE=0.478, train_loss=0.478, val_MAE=0.731, val_loss=0.731]Epoch 14:   1%|▏         | 14/1000 [13:54<15:04:45, 55.06s/it, lr=0.001, test_MAE=0.764, time=54.6, train_MAE=0.444, train_loss=0.444, val_MAE=0.718, val_loss=0.718]Epoch 14:   2%|▏         | 15/1000 [13:54<15:01:41, 54.93s/it, lr=0.001, test_MAE=0.764, time=54.6, train_MAE=0.444, train_loss=0.444, val_MAE=0.718, val_loss=0.718]Epoch 15:   2%|▏         | 15/1000 [13:54<15:01:41, 54.93s/it, lr=0.001, test_MAE=0.764, time=54.6, train_MAE=0.444, train_loss=0.444, val_MAE=0.718, val_loss=0.718]Epoch 15:   2%|▏         | 15/1000 [14:49<15:01:41, 54.93s/it, lr=0.001, test_MAE=0.801, time=55.1, train_MAE=0.42, train_loss=0.42, val_MAE=0.759, val_loss=0.759]  Epoch 15:   2%|▏         | 16/1000 [14:49<15:02:24, 55.02s/it, lr=0.001, test_MAE=0.801, time=55.1, train_MAE=0.42, train_loss=0.42, val_MAE=0.759, val_loss=0.759]Epoch 16:   2%|▏         | 16/1000 [14:49<15:02:24, 55.02s/it, lr=0.001, test_MAE=0.801, time=55.1, train_MAE=0.42, train_loss=0.42, val_MAE=0.759, val_loss=0.759]Epoch 16:   2%|▏         | 16/1000 [15:44<15:02:24, 55.02s/it, lr=0.001, test_MAE=0.779, time=55.1, train_MAE=0.474, train_loss=0.474, val_MAE=0.733, val_loss=0.733]Epoch    17: reducing learning rate of group 0 to 5.0000e-04.
Epoch 16:   2%|▏         | 17/1000 [15:44<15:01:46, 55.04s/it, lr=0.001, test_MAE=0.779, time=55.1, train_MAE=0.474, train_loss=0.474, val_MAE=0.733, val_loss=0.733]Epoch 17:   2%|▏         | 17/1000 [15:44<15:01:46, 55.04s/it, lr=0.001, test_MAE=0.779, time=55.1, train_MAE=0.474, train_loss=0.474, val_MAE=0.733, val_loss=0.733]Epoch 17:   2%|▏         | 17/1000 [16:39<15:01:46, 55.04s/it, lr=0.0005, test_MAE=0.769, time=54.7, train_MAE=0.375, train_loss=0.375, val_MAE=0.727, val_loss=0.727]Epoch 17:   2%|▏         | 18/1000 [16:39<14:59:09, 54.94s/it, lr=0.0005, test_MAE=0.769, time=54.7, train_MAE=0.375, train_loss=0.375, val_MAE=0.727, val_loss=0.727]Epoch 18:   2%|▏         | 18/1000 [16:39<14:59:09, 54.94s/it, lr=0.0005, test_MAE=0.769, time=54.7, train_MAE=0.375, train_loss=0.375, val_MAE=0.727, val_loss=0.727]Epoch 18:   2%|▏         | 18/1000 [17:34<14:59:09, 54.94s/it, lr=0.0005, test_MAE=0.767, time=54.7, train_MAE=0.377, train_loss=0.377, val_MAE=0.723, val_loss=0.723]Epoch 18:   2%|▏         | 19/1000 [17:34<14:57:04, 54.87s/it, lr=0.0005, test_MAE=0.767, time=54.7, train_MAE=0.377, train_loss=0.377, val_MAE=0.723, val_loss=0.723]Epoch 19:   2%|▏         | 19/1000 [17:34<14:57:04, 54.87s/it, lr=0.0005, test_MAE=0.767, time=54.7, train_MAE=0.377, train_loss=0.377, val_MAE=0.723, val_loss=0.723]Epoch 19:   2%|▏         | 19/1000 [18:28<14:57:04, 54.87s/it, lr=0.0005, test_MAE=0.767, time=54.9, train_MAE=0.366, train_loss=0.366, val_MAE=0.725, val_loss=0.725]Epoch 19:   2%|▏         | 20/1000 [18:28<14:56:25, 54.88s/it, lr=0.0005, test_MAE=0.767, time=54.9, train_MAE=0.366, train_loss=0.366, val_MAE=0.725, val_loss=0.725]Epoch 20:   2%|▏         | 20/1000 [18:28<14:56:25, 54.88s/it, lr=0.0005, test_MAE=0.767, time=54.9, train_MAE=0.366, train_loss=0.366, val_MAE=0.725, val_loss=0.725]Epoch 20:   2%|▏         | 20/1000 [19:23<14:56:25, 54.88s/it, lr=0.0005, test_MAE=0.771, time=55, train_MAE=0.351, train_loss=0.351, val_MAE=0.729, val_loss=0.729]  Epoch 20:   2%|▏         | 21/1000 [19:24<14:56:12, 54.93s/it, lr=0.0005, test_MAE=0.771, time=55, train_MAE=0.351, train_loss=0.351, val_MAE=0.729, val_loss=0.729]Epoch 21:   2%|▏         | 21/1000 [19:24<14:56:12, 54.93s/it, lr=0.0005, test_MAE=0.771, time=55, train_MAE=0.351, train_loss=0.351, val_MAE=0.729, val_loss=0.729]Epoch 21:   2%|▏         | 21/1000 [20:18<14:56:12, 54.93s/it, lr=0.0005, test_MAE=0.773, time=54.5, train_MAE=0.356, train_loss=0.356, val_MAE=0.731, val_loss=0.731]Epoch 21:   2%|▏         | 22/1000 [20:18<14:53:25, 54.81s/it, lr=0.0005, test_MAE=0.773, time=54.5, train_MAE=0.356, train_loss=0.356, val_MAE=0.731, val_loss=0.731]Epoch 22:   2%|▏         | 22/1000 [20:18<14:53:25, 54.81s/it, lr=0.0005, test_MAE=0.773, time=54.5, train_MAE=0.356, train_loss=0.356, val_MAE=0.731, val_loss=0.731]Epoch 22:   2%|▏         | 22/1000 [21:13<14:53:25, 54.81s/it, lr=0.0005, test_MAE=0.778, time=54.6, train_MAE=0.342, train_loss=0.342, val_MAE=0.73, val_loss=0.73]  Epoch    23: reducing learning rate of group 0 to 2.5000e-04.
Epoch 22:   2%|▏         | 23/1000 [21:13<14:51:28, 54.75s/it, lr=0.0005, test_MAE=0.778, time=54.6, train_MAE=0.342, train_loss=0.342, val_MAE=0.73, val_loss=0.73]Epoch 23:   2%|▏         | 23/1000 [21:13<14:51:28, 54.75s/it, lr=0.0005, test_MAE=0.778, time=54.6, train_MAE=0.342, train_loss=0.342, val_MAE=0.73, val_loss=0.73]Epoch 23:   2%|▏         | 23/1000 [22:08<14:51:28, 54.75s/it, lr=0.00025, test_MAE=0.772, time=55, train_MAE=0.318, train_loss=0.318, val_MAE=0.731, val_loss=0.731]Epoch 23:   2%|▏         | 24/1000 [22:08<14:51:43, 54.82s/it, lr=0.00025, test_MAE=0.772, time=55, train_MAE=0.318, train_loss=0.318, val_MAE=0.731, val_loss=0.731]Epoch 24:   2%|▏         | 24/1000 [22:08<14:51:43, 54.82s/it, lr=0.00025, test_MAE=0.772, time=55, train_MAE=0.318, train_loss=0.318, val_MAE=0.731, val_loss=0.731]Epoch 24:   2%|▏         | 24/1000 [23:03<14:51:43, 54.82s/it, lr=0.00025, test_MAE=0.772, time=54.9, train_MAE=0.307, train_loss=0.307, val_MAE=0.734, val_loss=0.734]Epoch 24:   2%|▎         | 25/1000 [23:03<14:51:26, 54.86s/it, lr=0.00025, test_MAE=0.772, time=54.9, train_MAE=0.307, train_loss=0.307, val_MAE=0.734, val_loss=0.734]Epoch 25:   2%|▎         | 25/1000 [23:03<14:51:26, 54.86s/it, lr=0.00025, test_MAE=0.772, time=54.9, train_MAE=0.307, train_loss=0.307, val_MAE=0.734, val_loss=0.734]Epoch 25:   2%|▎         | 25/1000 [23:57<14:51:26, 54.86s/it, lr=0.00025, test_MAE=0.768, time=54.6, train_MAE=0.317, train_loss=0.317, val_MAE=0.732, val_loss=0.732]Epoch 25:   3%|▎         | 26/1000 [23:57<14:49:19, 54.78s/it, lr=0.00025, test_MAE=0.768, time=54.6, train_MAE=0.317, train_loss=0.317, val_MAE=0.732, val_loss=0.732]Epoch 26:   3%|▎         | 26/1000 [23:57<14:49:19, 54.78s/it, lr=0.00025, test_MAE=0.768, time=54.6, train_MAE=0.317, train_loss=0.317, val_MAE=0.732, val_loss=0.732]Epoch 26:   3%|▎         | 26/1000 [24:52<14:49:19, 54.78s/it, lr=0.00025, test_MAE=0.773, time=54.8, train_MAE=0.308, train_loss=0.308, val_MAE=0.732, val_loss=0.732]Epoch 26:   3%|▎         | 27/1000 [24:52<14:48:28, 54.79s/it, lr=0.00025, test_MAE=0.773, time=54.8, train_MAE=0.308, train_loss=0.308, val_MAE=0.732, val_loss=0.732]Epoch 27:   3%|▎         | 27/1000 [24:52<14:48:28, 54.79s/it, lr=0.00025, test_MAE=0.773, time=54.8, train_MAE=0.308, train_loss=0.308, val_MAE=0.732, val_loss=0.732]Epoch 27:   3%|▎         | 27/1000 [25:47<14:48:28, 54.79s/it, lr=0.00025, test_MAE=0.771, time=55, train_MAE=0.312, train_loss=0.312, val_MAE=0.733, val_loss=0.733]  Epoch 27:   3%|▎         | 28/1000 [25:47<14:48:31, 54.85s/it, lr=0.00025, test_MAE=0.771, time=55, train_MAE=0.312, train_loss=0.312, val_MAE=0.733, val_loss=0.733]Epoch 28:   3%|▎         | 28/1000 [25:47<14:48:31, 54.85s/it, lr=0.00025, test_MAE=0.771, time=55, train_MAE=0.312, train_loss=0.312, val_MAE=0.733, val_loss=0.733]Epoch 28:   3%|▎         | 28/1000 [26:42<14:48:31, 54.85s/it, lr=0.00025, test_MAE=0.772, time=55, train_MAE=0.32, train_loss=0.32, val_MAE=0.735, val_loss=0.735]  Epoch    29: reducing learning rate of group 0 to 1.2500e-04.
Epoch 28:   3%|▎         | 29/1000 [26:42<14:48:28, 54.90s/it, lr=0.00025, test_MAE=0.772, time=55, train_MAE=0.32, train_loss=0.32, val_MAE=0.735, val_loss=0.735]Epoch 29:   3%|▎         | 29/1000 [26:42<14:48:28, 54.90s/it, lr=0.00025, test_MAE=0.772, time=55, train_MAE=0.32, train_loss=0.32, val_MAE=0.735, val_loss=0.735]Epoch 29:   3%|▎         | 29/1000 [27:37<14:48:28, 54.90s/it, lr=0.000125, test_MAE=0.776, time=54.7, train_MAE=0.302, train_loss=0.302, val_MAE=0.737, val_loss=0.737]Epoch 29:   3%|▎         | 30/1000 [27:37<14:46:47, 54.85s/it, lr=0.000125, test_MAE=0.776, time=54.7, train_MAE=0.302, train_loss=0.302, val_MAE=0.737, val_loss=0.737]Epoch 30:   3%|▎         | 30/1000 [27:37<14:46:47, 54.85s/it, lr=0.000125, test_MAE=0.776, time=54.7, train_MAE=0.302, train_loss=0.302, val_MAE=0.737, val_loss=0.737]Epoch 30:   3%|▎         | 30/1000 [28:31<14:46:47, 54.85s/it, lr=0.000125, test_MAE=0.773, time=54.6, train_MAE=0.285, train_loss=0.285, val_MAE=0.734, val_loss=0.734]Epoch 30:   3%|▎         | 31/1000 [28:31<14:44:37, 54.78s/it, lr=0.000125, test_MAE=0.773, time=54.6, train_MAE=0.285, train_loss=0.285, val_MAE=0.734, val_loss=0.734]Epoch 31:   3%|▎         | 31/1000 [28:31<14:44:37, 54.78s/it, lr=0.000125, test_MAE=0.773, time=54.6, train_MAE=0.285, train_loss=0.285, val_MAE=0.734, val_loss=0.734]Epoch 31:   3%|▎         | 31/1000 [29:26<14:44:37, 54.78s/it, lr=0.000125, test_MAE=0.773, time=55.1, train_MAE=0.294, train_loss=0.294, val_MAE=0.733, val_loss=0.733]Epoch 31:   3%|▎         | 32/1000 [29:26<14:45:10, 54.87s/it, lr=0.000125, test_MAE=0.773, time=55.1, train_MAE=0.294, train_loss=0.294, val_MAE=0.733, val_loss=0.733]Epoch 32:   3%|▎         | 32/1000 [29:26<14:45:10, 54.87s/it, lr=0.000125, test_MAE=0.773, time=55.1, train_MAE=0.294, train_loss=0.294, val_MAE=0.733, val_loss=0.733]Epoch 32:   3%|▎         | 32/1000 [30:21<14:45:10, 54.87s/it, lr=0.000125, test_MAE=0.775, time=55, train_MAE=0.283, train_loss=0.283, val_MAE=0.734, val_loss=0.734]  Epoch 32:   3%|▎         | 33/1000 [30:21<14:44:50, 54.90s/it, lr=0.000125, test_MAE=0.775, time=55, train_MAE=0.283, train_loss=0.283, val_MAE=0.734, val_loss=0.734]Epoch 33:   3%|▎         | 33/1000 [30:21<14:44:50, 54.90s/it, lr=0.000125, test_MAE=0.775, time=55, train_MAE=0.283, train_loss=0.283, val_MAE=0.734, val_loss=0.734]Epoch 33:   3%|▎         | 33/1000 [31:16<14:44:50, 54.90s/it, lr=0.000125, test_MAE=0.775, time=54.6, train_MAE=0.282, train_loss=0.282, val_MAE=0.737, val_loss=0.737]Epoch 33:   3%|▎         | 34/1000 [31:16<14:42:41, 54.83s/it, lr=0.000125, test_MAE=0.775, time=54.6, train_MAE=0.282, train_loss=0.282, val_MAE=0.737, val_loss=0.737]Epoch 34:   3%|▎         | 34/1000 [31:16<14:42:41, 54.83s/it, lr=0.000125, test_MAE=0.775, time=54.6, train_MAE=0.282, train_loss=0.282, val_MAE=0.737, val_loss=0.737]Epoch 34:   3%|▎         | 34/1000 [32:11<14:42:41, 54.83s/it, lr=0.000125, test_MAE=0.781, time=54.7, train_MAE=0.28, train_loss=0.28, val_MAE=0.745, val_loss=0.745]  Epoch    35: reducing learning rate of group 0 to 6.2500e-05.
Epoch 34:   4%|▎         | 35/1000 [32:11<14:41:31, 54.81s/it, lr=0.000125, test_MAE=0.781, time=54.7, train_MAE=0.28, train_loss=0.28, val_MAE=0.745, val_loss=0.745]Epoch 35:   4%|▎         | 35/1000 [32:11<14:41:31, 54.81s/it, lr=0.000125, test_MAE=0.781, time=54.7, train_MAE=0.28, train_loss=0.28, val_MAE=0.745, val_loss=0.745]Epoch 35:   4%|▎         | 35/1000 [33:06<14:41:31, 54.81s/it, lr=6.25e-5, test_MAE=0.775, time=54.9, train_MAE=0.265, train_loss=0.265, val_MAE=0.734, val_loss=0.734]Epoch 35:   4%|▎         | 36/1000 [33:06<14:41:16, 54.85s/it, lr=6.25e-5, test_MAE=0.775, time=54.9, train_MAE=0.265, train_loss=0.265, val_MAE=0.734, val_loss=0.734]Epoch 36:   4%|▎         | 36/1000 [33:06<14:41:16, 54.85s/it, lr=6.25e-5, test_MAE=0.775, time=54.9, train_MAE=0.265, train_loss=0.265, val_MAE=0.734, val_loss=0.734]Epoch 36:   4%|▎         | 36/1000 [34:01<14:41:16, 54.85s/it, lr=6.25e-5, test_MAE=0.775, time=55, train_MAE=0.278, train_loss=0.278, val_MAE=0.735, val_loss=0.735]  Epoch 36:   4%|▎         | 37/1000 [34:01<14:41:16, 54.91s/it, lr=6.25e-5, test_MAE=0.775, time=55, train_MAE=0.278, train_loss=0.278, val_MAE=0.735, val_loss=0.735]Epoch 37:   4%|▎         | 37/1000 [34:01<14:41:16, 54.91s/it, lr=6.25e-5, test_MAE=0.775, time=55, train_MAE=0.278, train_loss=0.278, val_MAE=0.735, val_loss=0.735]Epoch 37:   4%|▎         | 37/1000 [34:55<14:41:16, 54.91s/it, lr=6.25e-5, test_MAE=0.777, time=54.7, train_MAE=0.275, train_loss=0.275, val_MAE=0.736, val_loss=0.736]Epoch 37:   4%|▍         | 38/1000 [34:56<14:39:16, 54.84s/it, lr=6.25e-5, test_MAE=0.777, time=54.7, train_MAE=0.275, train_loss=0.275, val_MAE=0.736, val_loss=0.736]Epoch 38:   4%|▍         | 38/1000 [34:56<14:39:16, 54.84s/it, lr=6.25e-5, test_MAE=0.777, time=54.7, train_MAE=0.275, train_loss=0.275, val_MAE=0.736, val_loss=0.736]Epoch 38:   4%|▍         | 38/1000 [35:50<14:39:16, 54.84s/it, lr=6.25e-5, test_MAE=0.776, time=54.8, train_MAE=0.273, train_loss=0.273, val_MAE=0.735, val_loss=0.735]Epoch 38:   4%|▍         | 39/1000 [35:50<14:38:04, 54.82s/it, lr=6.25e-5, test_MAE=0.776, time=54.8, train_MAE=0.273, train_loss=0.273, val_MAE=0.735, val_loss=0.735]Epoch 39:   4%|▍         | 39/1000 [35:50<14:38:04, 54.82s/it, lr=6.25e-5, test_MAE=0.776, time=54.8, train_MAE=0.273, train_loss=0.273, val_MAE=0.735, val_loss=0.735]Epoch 39:   4%|▍         | 39/1000 [36:45<14:38:04, 54.82s/it, lr=6.25e-5, test_MAE=0.776, time=55, train_MAE=0.274, train_loss=0.274, val_MAE=0.735, val_loss=0.735]  Epoch 39:   4%|▍         | 40/1000 [36:45<14:37:56, 54.87s/it, lr=6.25e-5, test_MAE=0.776, time=55, train_MAE=0.274, train_loss=0.274, val_MAE=0.735, val_loss=0.735]Epoch 40:   4%|▍         | 40/1000 [36:45<14:37:56, 54.87s/it, lr=6.25e-5, test_MAE=0.776, time=55, train_MAE=0.274, train_loss=0.274, val_MAE=0.735, val_loss=0.735]Epoch 40:   4%|▍         | 40/1000 [37:40<14:37:56, 54.87s/it, lr=6.25e-5, test_MAE=0.778, time=55, train_MAE=0.279, train_loss=0.279, val_MAE=0.738, val_loss=0.738]Epoch    41: reducing learning rate of group 0 to 3.1250e-05.
Epoch 40:   4%|▍         | 41/1000 [37:40<14:37:50, 54.92s/it, lr=6.25e-5, test_MAE=0.778, time=55, train_MAE=0.279, train_loss=0.279, val_MAE=0.738, val_loss=0.738]Epoch 41:   4%|▍         | 41/1000 [37:40<14:37:50, 54.92s/it, lr=6.25e-5, test_MAE=0.778, time=55, train_MAE=0.279, train_loss=0.279, val_MAE=0.738, val_loss=0.738]Epoch 41:   4%|▍         | 41/1000 [38:35<14:37:50, 54.92s/it, lr=3.13e-5, test_MAE=0.775, time=54.7, train_MAE=0.277, train_loss=0.277, val_MAE=0.737, val_loss=0.737]Epoch 41:   4%|▍         | 42/1000 [38:35<14:36:03, 54.87s/it, lr=3.13e-5, test_MAE=0.775, time=54.7, train_MAE=0.277, train_loss=0.277, val_MAE=0.737, val_loss=0.737]Epoch 42:   4%|▍         | 42/1000 [38:35<14:36:03, 54.87s/it, lr=3.13e-5, test_MAE=0.775, time=54.7, train_MAE=0.277, train_loss=0.277, val_MAE=0.737, val_loss=0.737]Epoch 42:   4%|▍         | 42/1000 [39:30<14:36:03, 54.87s/it, lr=3.13e-5, test_MAE=0.778, time=54.8, train_MAE=0.267, train_loss=0.267, val_MAE=0.739, val_loss=0.739]Epoch 42:   4%|▍         | 43/1000 [39:30<14:35:05, 54.86s/it, lr=3.13e-5, test_MAE=0.778, time=54.8, train_MAE=0.267, train_loss=0.267, val_MAE=0.739, val_loss=0.739]Epoch 43:   4%|▍         | 43/1000 [39:30<14:35:05, 54.86s/it, lr=3.13e-5, test_MAE=0.778, time=54.8, train_MAE=0.267, train_loss=0.267, val_MAE=0.739, val_loss=0.739]Epoch 43:   4%|▍         | 43/1000 [40:25<14:35:05, 54.86s/it, lr=3.13e-5, test_MAE=0.782, time=54.9, train_MAE=0.275, train_loss=0.275, val_MAE=0.745, val_loss=0.745]Epoch 43:   4%|▍         | 44/1000 [40:25<14:34:28, 54.88s/it, lr=3.13e-5, test_MAE=0.782, time=54.9, train_MAE=0.275, train_loss=0.275, val_MAE=0.745, val_loss=0.745]Epoch 44:   4%|▍         | 44/1000 [40:25<14:34:28, 54.88s/it, lr=3.13e-5, test_MAE=0.782, time=54.9, train_MAE=0.275, train_loss=0.275, val_MAE=0.745, val_loss=0.745]Epoch 44:   4%|▍         | 44/1000 [41:20<14:34:28, 54.88s/it, lr=3.13e-5, test_MAE=0.777, time=54.9, train_MAE=0.27, train_loss=0.27, val_MAE=0.738, val_loss=0.738]  Epoch 44:   4%|▍         | 45/1000 [41:20<14:33:48, 54.90s/it, lr=3.13e-5, test_MAE=0.777, time=54.9, train_MAE=0.27, train_loss=0.27, val_MAE=0.738, val_loss=0.738]Epoch 45:   4%|▍         | 45/1000 [41:20<14:33:48, 54.90s/it, lr=3.13e-5, test_MAE=0.777, time=54.9, train_MAE=0.27, train_loss=0.27, val_MAE=0.738, val_loss=0.738]Epoch 45:   4%|▍         | 45/1000 [42:14<14:33:48, 54.90s/it, lr=3.13e-5, test_MAE=0.777, time=54.6, train_MAE=0.268, train_loss=0.268, val_MAE=0.737, val_loss=0.737]Epoch 45:   5%|▍         | 46/1000 [42:14<14:31:39, 54.82s/it, lr=3.13e-5, test_MAE=0.777, time=54.6, train_MAE=0.268, train_loss=0.268, val_MAE=0.737, val_loss=0.737]Epoch 46:   5%|▍         | 46/1000 [42:14<14:31:39, 54.82s/it, lr=3.13e-5, test_MAE=0.777, time=54.6, train_MAE=0.268, train_loss=0.268, val_MAE=0.737, val_loss=0.737]Epoch 46:   5%|▍         | 46/1000 [43:09<14:31:39, 54.82s/it, lr=3.13e-5, test_MAE=0.779, time=54.5, train_MAE=0.275, train_loss=0.275, val_MAE=0.742, val_loss=0.742]Epoch    47: reducing learning rate of group 0 to 1.5625e-05.
Epoch 46:   5%|▍         | 47/1000 [43:09<14:29:26, 54.74s/it, lr=3.13e-5, test_MAE=0.779, time=54.5, train_MAE=0.275, train_loss=0.275, val_MAE=0.742, val_loss=0.742]Epoch 47:   5%|▍         | 47/1000 [43:09<14:29:26, 54.74s/it, lr=3.13e-5, test_MAE=0.779, time=54.5, train_MAE=0.275, train_loss=0.275, val_MAE=0.742, val_loss=0.742]Epoch 47:   5%|▍         | 47/1000 [44:04<14:29:26, 54.74s/it, lr=1.56e-5, test_MAE=0.778, time=55.1, train_MAE=0.267, train_loss=0.267, val_MAE=0.737, val_loss=0.737]Epoch 47:   5%|▍         | 48/1000 [44:04<14:30:29, 54.86s/it, lr=1.56e-5, test_MAE=0.778, time=55.1, train_MAE=0.267, train_loss=0.267, val_MAE=0.737, val_loss=0.737]Epoch 48:   5%|▍         | 48/1000 [44:04<14:30:29, 54.86s/it, lr=1.56e-5, test_MAE=0.778, time=55.1, train_MAE=0.267, train_loss=0.267, val_MAE=0.737, val_loss=0.737]Epoch 48:   5%|▍         | 48/1000 [45:00<14:30:29, 54.86s/it, lr=1.56e-5, test_MAE=0.776, time=55.4, train_MAE=0.263, train_loss=0.263, val_MAE=0.736, val_loss=0.736]Epoch 48:   5%|▍         | 49/1000 [45:00<14:32:16, 55.03s/it, lr=1.56e-5, test_MAE=0.776, time=55.4, train_MAE=0.263, train_loss=0.263, val_MAE=0.736, val_loss=0.736]Epoch 49:   5%|▍         | 49/1000 [45:00<14:32:16, 55.03s/it, lr=1.56e-5, test_MAE=0.776, time=55.4, train_MAE=0.263, train_loss=0.263, val_MAE=0.736, val_loss=0.736]Epoch 49:   5%|▍         | 49/1000 [45:55<14:32:16, 55.03s/it, lr=1.56e-5, test_MAE=0.778, time=55.9, train_MAE=0.273, train_loss=0.273, val_MAE=0.739, val_loss=0.739]Epoch 49:   5%|▌         | 50/1000 [45:55<14:35:36, 55.30s/it, lr=1.56e-5, test_MAE=0.778, time=55.9, train_MAE=0.273, train_loss=0.273, val_MAE=0.739, val_loss=0.739]Epoch 50:   5%|▌         | 50/1000 [45:55<14:35:36, 55.30s/it, lr=1.56e-5, test_MAE=0.778, time=55.9, train_MAE=0.273, train_loss=0.273, val_MAE=0.739, val_loss=0.739]Epoch 50:   5%|▌         | 50/1000 [46:52<14:35:36, 55.30s/it, lr=1.56e-5, test_MAE=0.778, time=56.1, train_MAE=0.264, train_loss=0.264, val_MAE=0.738, val_loss=0.738]Epoch 50:   5%|▌         | 51/1000 [46:52<14:38:27, 55.54s/it, lr=1.56e-5, test_MAE=0.778, time=56.1, train_MAE=0.264, train_loss=0.264, val_MAE=0.738, val_loss=0.738]Epoch 51:   5%|▌         | 51/1000 [46:52<14:38:27, 55.54s/it, lr=1.56e-5, test_MAE=0.778, time=56.1, train_MAE=0.264, train_loss=0.264, val_MAE=0.738, val_loss=0.738]Epoch 51:   5%|▌         | 51/1000 [47:48<14:38:27, 55.54s/it, lr=1.56e-5, test_MAE=0.783, time=56.4, train_MAE=0.267, train_loss=0.267, val_MAE=0.742, val_loss=0.742]Epoch 51:   5%|▌         | 52/1000 [47:48<14:41:39, 55.80s/it, lr=1.56e-5, test_MAE=0.783, time=56.4, train_MAE=0.267, train_loss=0.267, val_MAE=0.742, val_loss=0.742]Epoch 52:   5%|▌         | 52/1000 [47:48<14:41:39, 55.80s/it, lr=1.56e-5, test_MAE=0.783, time=56.4, train_MAE=0.267, train_loss=0.267, val_MAE=0.742, val_loss=0.742]Epoch 52:   5%|▌         | 52/1000 [48:44<14:41:39, 55.80s/it, lr=1.56e-5, test_MAE=0.779, time=55.7, train_MAE=0.261, train_loss=0.261, val_MAE=0.741, val_loss=0.741]Epoch    53: reducing learning rate of group 0 to 7.8125e-06.

!! LR EQUAL TO MIN LR SET.
Epoch 52:   5%|▌         | 52/1000 [48:44<14:48:29, 56.23s/it, lr=1.56e-5, test_MAE=0.779, time=55.7, train_MAE=0.261, train_loss=0.261, val_MAE=0.741, val_loss=0.741]
Test MAE: 0.7789
Train MAE: 0.2090
Convergence Time (Epochs): 52.0000
TOTAL TIME TAKEN: 2952.4815s
AVG TIME PER EPOCH: 55.1474s
Traceback (most recent call last):
  File "main_molecules_graph_regression.py", line 398, in <module>
    main()
  File "main_molecules_graph_regression.py", line 395, in main
    train_val_pipeline(MODEL_NAME, dataset, params, net_params, dirs)
  File "main_molecules_graph_regression.py", line 217, in train_val_pipeline
    test_mae, train_mae, epoch, (time.time() - t0) / 3600, np.mean(per_epoch_time)))
  File "/home/bsw38/.conda/envs/benchmark_gnn_2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1347, in __repr__
    mod_str = repr(module)
  File "/home/bsw38/.conda/envs/benchmark_gnn_2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1347, in __repr__
    mod_str = repr(module)
  File "/vast/palmer/pi/krishnaswamy_smita/bsw38/GraphML/ChebNetGNNs/Benchmark-gnn/layers/Spec_layer.py", line 228, in __repr__
    self.__class__.__name__, self.in_channels, self.out_channels, self._k)
  File "/home/bsw38/.conda/envs/benchmark_gnn_2/lib/python3.7/site-packages/torch/nn/modules/module.py", line 772, in __getattr__
    type(self).__name__, name))
torch.nn.modules.module.ModuleAttributeError: 'SpecLayer' object has no attribute '_k'

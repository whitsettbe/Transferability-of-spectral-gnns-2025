I'm echoing to stdout
I'm echoing to stderr
My JobID is 56775419
I have 8 CPUs on node r108u25n01
Using backend: pytorch
cuda not available
[I] Loading dataset ZINC...
train, test, val sizes : 10000 1000 1000
[I] Finished loading.
[I] Data load time: 5.6030s
Dataset: ZINC,
Model: EigvalFilters

params={'seed': 41, 'epochs': 1000, 'batch_size': 128, 'init_lr': 0.001, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 5, 'min_lr': 1e-05, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 48}

net_params={'L': 4, 'hidden_dim': 106, 'out_dim': 106, 'residual': True, 'readout': 'mean', 'k': 2, 'in_feat_dropout': 0.0, 'dropout': 0.0, 'graph_norm': True, 'batch_norm': True, 'self_loop': False, 'subtype': 'parallel_dense_simp', 'normalized_laplacian': False, 'post_normalized': True, 'eigval_norm': 'scale(0,2)_all', 'num_eigs': 64, 'bias_mode': '', 'eigval_hidden_dim': 5, 'eigval_num_hidden_layer': 3, 'l1_reg': 0.0, 'l2_reg': 0.0, 'gen_reg': 5.0, 'eigmod': 'import_csv', 'eigInFiles': {'train': 'supp_data/molecules/zinc_train_rec_full.csv', 'test': 'supp_data/molecules/zinc_test_rec_full.csv', 'val': 'supp_data/molecules/zinc_val_rec_full.csv'}, 'fixMissingPhi1': False, 'extraOrtho': True, 'device': device(type='cpu'), 'gpu_id': 0, 'batch_size': 128, 'biases': False, 'num_atom_type': 28, 'num_bond_type': 4, 'total_param': -1}


Total Parameters: -1


Training Graphs:  10000
Validation Graphs:  1000
Test Graphs:  1000
  0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [04:33<?, ?it/s, lr=0.001, test_MAE=1.49, time=274, train_MAE=1.01, train_loss=1.07, val_MAE=1.42, val_loss=1.48]Epoch 0:   0%|          | 1/1000 [04:33<75:54:19, 273.53s/it, lr=0.001, test_MAE=1.49, time=274, train_MAE=1.01, train_loss=1.07, val_MAE=1.42, val_loss=1.48]Epoch 1:   0%|          | 1/1000 [04:33<75:54:19, 273.53s/it, lr=0.001, test_MAE=1.49, time=274, train_MAE=1.01, train_loss=1.07, val_MAE=1.42, val_loss=1.48]Epoch 1:   0%|          | 1/1000 [05:26<75:54:19, 273.53s/it, lr=0.001, test_MAE=1.08, time=52.8, train_MAE=0.691, train_loss=0.751, val_MAE=1.04, val_loss=1.1]Epoch 1:   0%|          | 2/1000 [05:26<57:28:33, 207.33s/it, lr=0.001, test_MAE=1.08, time=52.8, train_MAE=0.691, train_loss=0.751, val_MAE=1.04, val_loss=1.1]Epoch 2:   0%|          | 2/1000 [05:26<57:28:33, 207.33s/it, lr=0.001, test_MAE=1.08, time=52.8, train_MAE=0.691, train_loss=0.751, val_MAE=1.04, val_loss=1.1]Epoch 2:   0%|          | 2/1000 [06:18<57:28:33, 207.33s/it, lr=0.001, test_MAE=0.867, time=52.4, train_MAE=0.658, train_loss=0.71, val_MAE=0.822, val_loss=0.872]Epoch 2:   0%|          | 3/1000 [06:18<44:32:39, 160.84s/it, lr=0.001, test_MAE=0.867, time=52.4, train_MAE=0.658, train_loss=0.71, val_MAE=0.822, val_loss=0.872]Epoch 3:   0%|          | 3/1000 [06:18<44:32:39, 160.84s/it, lr=0.001, test_MAE=0.867, time=52.4, train_MAE=0.658, train_loss=0.71, val_MAE=0.822, val_loss=0.872]Epoch 3:   0%|          | 3/1000 [07:10<44:32:39, 160.84s/it, lr=0.001, test_MAE=0.706, time=52, train_MAE=0.634, train_loss=0.685, val_MAE=0.639, val_loss=0.687] Epoch 3:   0%|          | 4/1000 [07:10<35:27:59, 128.19s/it, lr=0.001, test_MAE=0.706, time=52, train_MAE=0.634, train_loss=0.685, val_MAE=0.639, val_loss=0.687]Epoch 4:   0%|          | 4/1000 [07:10<35:27:59, 128.19s/it, lr=0.001, test_MAE=0.706, time=52, train_MAE=0.634, train_loss=0.685, val_MAE=0.639, val_loss=0.687]Epoch 4:   0%|          | 4/1000 [08:03<35:27:59, 128.19s/it, lr=0.001, test_MAE=0.776, time=52.3, train_MAE=0.621, train_loss=0.671, val_MAE=0.719, val_loss=0.769]Epoch 4:   0%|          | 5/1000 [08:03<29:08:20, 105.43s/it, lr=0.001, test_MAE=0.776, time=52.3, train_MAE=0.621, train_loss=0.671, val_MAE=0.719, val_loss=0.769]Epoch 5:   0%|          | 5/1000 [08:03<29:08:20, 105.43s/it, lr=0.001, test_MAE=0.776, time=52.3, train_MAE=0.621, train_loss=0.671, val_MAE=0.719, val_loss=0.769]Epoch 5:   0%|          | 5/1000 [08:55<29:08:20, 105.43s/it, lr=0.001, test_MAE=0.76, time=52.2, train_MAE=0.622, train_loss=0.673, val_MAE=0.72, val_loss=0.773]  Epoch 5:   1%|          | 6/1000 [08:55<24:41:52, 89.45s/it, lr=0.001, test_MAE=0.76, time=52.2, train_MAE=0.622, train_loss=0.673, val_MAE=0.72, val_loss=0.773] Epoch 6:   1%|          | 6/1000 [08:55<24:41:52, 89.45s/it, lr=0.001, test_MAE=0.76, time=52.2, train_MAE=0.622, train_loss=0.673, val_MAE=0.72, val_loss=0.773]Epoch 6:   1%|          | 6/1000 [09:47<24:41:52, 89.45s/it, lr=0.001, test_MAE=0.699, time=52.5, train_MAE=0.611, train_loss=0.664, val_MAE=0.635, val_loss=0.688]Epoch 6:   1%|          | 7/1000 [09:47<21:37:02, 78.37s/it, lr=0.001, test_MAE=0.699, time=52.5, train_MAE=0.611, train_loss=0.664, val_MAE=0.635, val_loss=0.688]Epoch 7:   1%|          | 7/1000 [09:47<21:37:02, 78.37s/it, lr=0.001, test_MAE=0.699, time=52.5, train_MAE=0.611, train_loss=0.664, val_MAE=0.635, val_loss=0.688]Epoch 7:   1%|          | 7/1000 [10:39<21:37:02, 78.37s/it, lr=0.001, test_MAE=0.702, time=52.1, train_MAE=0.607, train_loss=0.662, val_MAE=0.639, val_loss=0.692]Epoch 7:   1%|          | 8/1000 [10:39<19:25:42, 70.51s/it, lr=0.001, test_MAE=0.702, time=52.1, train_MAE=0.607, train_loss=0.662, val_MAE=0.639, val_loss=0.692]Epoch 8:   1%|          | 8/1000 [10:39<19:25:42, 70.51s/it, lr=0.001, test_MAE=0.702, time=52.1, train_MAE=0.607, train_loss=0.662, val_MAE=0.639, val_loss=0.692]Epoch 8:   1%|          | 8/1000 [11:31<19:25:42, 70.51s/it, lr=0.001, test_MAE=0.7, time=52.1, train_MAE=0.603, train_loss=0.658, val_MAE=0.659, val_loss=0.712]  Epoch 8:   1%|          | 9/1000 [11:31<17:53:10, 64.98s/it, lr=0.001, test_MAE=0.7, time=52.1, train_MAE=0.603, train_loss=0.658, val_MAE=0.659, val_loss=0.712]Epoch 9:   1%|          | 9/1000 [11:31<17:53:10, 64.98s/it, lr=0.001, test_MAE=0.7, time=52.1, train_MAE=0.603, train_loss=0.658, val_MAE=0.659, val_loss=0.712]Epoch 9:   1%|          | 9/1000 [12:24<17:53:10, 64.98s/it, lr=0.001, test_MAE=0.701, time=52.6, train_MAE=0.579, train_loss=0.634, val_MAE=0.635, val_loss=0.691]Epoch    10: reducing learning rate of group 0 to 5.0000e-04.
Epoch 9:   1%|          | 10/1000 [12:24<16:50:51, 61.26s/it, lr=0.001, test_MAE=0.701, time=52.6, train_MAE=0.579, train_loss=0.634, val_MAE=0.635, val_loss=0.691]Epoch 10:   1%|          | 10/1000 [12:24<16:50:51, 61.26s/it, lr=0.001, test_MAE=0.701, time=52.6, train_MAE=0.579, train_loss=0.634, val_MAE=0.635, val_loss=0.691]Epoch 10:   1%|          | 10/1000 [13:16<16:50:51, 61.26s/it, lr=0.0005, test_MAE=0.663, time=52.2, train_MAE=0.57, train_loss=0.621, val_MAE=0.617, val_loss=0.666]Epoch 10:   1%|          | 11/1000 [13:16<16:05:17, 58.56s/it, lr=0.0005, test_MAE=0.663, time=52.2, train_MAE=0.57, train_loss=0.621, val_MAE=0.617, val_loss=0.666]Epoch 11:   1%|          | 11/1000 [13:16<16:05:17, 58.56s/it, lr=0.0005, test_MAE=0.663, time=52.2, train_MAE=0.57, train_loss=0.621, val_MAE=0.617, val_loss=0.666]Epoch 11:   1%|          | 11/1000 [14:08<16:05:17, 58.56s/it, lr=0.0005, test_MAE=0.682, time=52, train_MAE=0.556, train_loss=0.607, val_MAE=0.63, val_loss=0.681]  Epoch 11:   1%|          | 12/1000 [14:08<15:31:54, 56.59s/it, lr=0.0005, test_MAE=0.682, time=52, train_MAE=0.556, train_loss=0.607, val_MAE=0.63, val_loss=0.681]Epoch 12:   1%|          | 12/1000 [14:08<15:31:54, 56.59s/it, lr=0.0005, test_MAE=0.682, time=52, train_MAE=0.556, train_loss=0.607, val_MAE=0.63, val_loss=0.681]Epoch 12:   1%|          | 12/1000 [15:01<15:31:54, 56.59s/it, lr=0.0005, test_MAE=0.663, time=52.2, train_MAE=0.551, train_loss=0.602, val_MAE=0.62, val_loss=0.672]Epoch 12:   1%|▏         | 13/1000 [15:01<15:09:20, 55.28s/it, lr=0.0005, test_MAE=0.663, time=52.2, train_MAE=0.551, train_loss=0.602, val_MAE=0.62, val_loss=0.672]Epoch 13:   1%|▏         | 13/1000 [15:01<15:09:20, 55.28s/it, lr=0.0005, test_MAE=0.663, time=52.2, train_MAE=0.551, train_loss=0.602, val_MAE=0.62, val_loss=0.672]Epoch 13:   1%|▏         | 13/1000 [15:53<15:09:20, 55.28s/it, lr=0.0005, test_MAE=0.764, time=52.9, train_MAE=0.544, train_loss=0.597, val_MAE=0.709, val_loss=0.762]Epoch 13:   1%|▏         | 14/1000 [15:53<14:56:48, 54.57s/it, lr=0.0005, test_MAE=0.764, time=52.9, train_MAE=0.544, train_loss=0.597, val_MAE=0.709, val_loss=0.762]Epoch 14:   1%|▏         | 14/1000 [15:53<14:56:48, 54.57s/it, lr=0.0005, test_MAE=0.764, time=52.9, train_MAE=0.544, train_loss=0.597, val_MAE=0.709, val_loss=0.762]Epoch 14:   1%|▏         | 14/1000 [16:56<14:56:48, 54.57s/it, lr=0.0005, test_MAE=0.67, time=62, train_MAE=0.538, train_loss=0.592, val_MAE=0.618, val_loss=0.673]   Epoch 14:   2%|▏         | 15/1000 [16:56<15:32:37, 56.81s/it, lr=0.0005, test_MAE=0.67, time=62, train_MAE=0.538, train_loss=0.592, val_MAE=0.618, val_loss=0.673]Epoch 15:   2%|▏         | 15/1000 [16:56<15:32:37, 56.81s/it, lr=0.0005, test_MAE=0.67, time=62, train_MAE=0.538, train_loss=0.592, val_MAE=0.618, val_loss=0.673]Epoch 15:   2%|▏         | 15/1000 [17:57<15:32:37, 56.81s/it, lr=0.0005, test_MAE=0.7, time=61.4, train_MAE=0.538, train_loss=0.594, val_MAE=0.635, val_loss=0.692]Epoch 15:   2%|▏         | 16/1000 [17:57<15:54:28, 58.20s/it, lr=0.0005, test_MAE=0.7, time=61.4, train_MAE=0.538, train_loss=0.594, val_MAE=0.635, val_loss=0.692]Epoch 16:   2%|▏         | 16/1000 [17:57<15:54:28, 58.20s/it, lr=0.0005, test_MAE=0.7, time=61.4, train_MAE=0.538, train_loss=0.594, val_MAE=0.635, val_loss=0.692]Epoch 16:   2%|▏         | 16/1000 [18:58<15:54:28, 58.20s/it, lr=0.0005, test_MAE=0.668, time=60.8, train_MAE=0.525, train_loss=0.582, val_MAE=0.631, val_loss=0.689]Epoch    17: reducing learning rate of group 0 to 2.5000e-04.
Epoch 16:   2%|▏         | 17/1000 [18:58<16:06:06, 58.97s/it, lr=0.0005, test_MAE=0.668, time=60.8, train_MAE=0.525, train_loss=0.582, val_MAE=0.631, val_loss=0.689]Epoch 17:   2%|▏         | 17/1000 [18:58<16:06:06, 58.97s/it, lr=0.0005, test_MAE=0.668, time=60.8, train_MAE=0.525, train_loss=0.582, val_MAE=0.631, val_loss=0.689]Epoch 17:   2%|▏         | 17/1000 [19:58<16:06:06, 58.97s/it, lr=0.00025, test_MAE=0.686, time=60.5, train_MAE=0.507, train_loss=0.563, val_MAE=0.643, val_loss=0.697]Epoch 17:   2%|▏         | 18/1000 [19:58<16:12:45, 59.44s/it, lr=0.00025, test_MAE=0.686, time=60.5, train_MAE=0.507, train_loss=0.563, val_MAE=0.643, val_loss=0.697]Epoch 18:   2%|▏         | 18/1000 [19:58<16:12:45, 59.44s/it, lr=0.00025, test_MAE=0.686, time=60.5, train_MAE=0.507, train_loss=0.563, val_MAE=0.643, val_loss=0.697]Epoch 18:   2%|▏         | 18/1000 [20:59<16:12:45, 59.44s/it, lr=0.00025, test_MAE=0.668, time=61.1, train_MAE=0.495, train_loss=0.548, val_MAE=0.625, val_loss=0.678]Epoch 18:   2%|▏         | 19/1000 [20:59<16:19:54, 59.93s/it, lr=0.00025, test_MAE=0.668, time=61.1, train_MAE=0.495, train_loss=0.548, val_MAE=0.625, val_loss=0.678]Epoch 19:   2%|▏         | 19/1000 [20:59<16:19:54, 59.93s/it, lr=0.00025, test_MAE=0.668, time=61.1, train_MAE=0.495, train_loss=0.548, val_MAE=0.625, val_loss=0.678]Epoch 19:   2%|▏         | 19/1000 [22:00<16:19:54, 59.93s/it, lr=0.00025, test_MAE=0.67, time=60.8, train_MAE=0.487, train_loss=0.542, val_MAE=0.621, val_loss=0.675] Epoch 19:   2%|▏         | 20/1000 [22:00<16:23:25, 60.21s/it, lr=0.00025, test_MAE=0.67, time=60.8, train_MAE=0.487, train_loss=0.542, val_MAE=0.621, val_loss=0.675]Epoch 20:   2%|▏         | 20/1000 [22:00<16:23:25, 60.21s/it, lr=0.00025, test_MAE=0.67, time=60.8, train_MAE=0.487, train_loss=0.542, val_MAE=0.621, val_loss=0.675]Epoch 20:   2%|▏         | 20/1000 [23:01<16:23:25, 60.21s/it, lr=0.00025, test_MAE=0.683, time=60.8, train_MAE=0.486, train_loss=0.541, val_MAE=0.624, val_loss=0.679]Epoch 20:   2%|▏         | 21/1000 [23:01<16:25:34, 60.40s/it, lr=0.00025, test_MAE=0.683, time=60.8, train_MAE=0.486, train_loss=0.541, val_MAE=0.624, val_loss=0.679]Epoch 21:   2%|▏         | 21/1000 [23:01<16:25:34, 60.40s/it, lr=0.00025, test_MAE=0.683, time=60.8, train_MAE=0.486, train_loss=0.541, val_MAE=0.624, val_loss=0.679]Epoch 21:   2%|▏         | 21/1000 [24:02<16:25:34, 60.40s/it, lr=0.00025, test_MAE=0.7, time=61.1, train_MAE=0.48, train_loss=0.536, val_MAE=0.651, val_loss=0.706]   Epoch 21:   2%|▏         | 22/1000 [24:02<16:28:15, 60.63s/it, lr=0.00025, test_MAE=0.7, time=61.1, train_MAE=0.48, train_loss=0.536, val_MAE=0.651, val_loss=0.706]Epoch 22:   2%|▏         | 22/1000 [24:02<16:28:15, 60.63s/it, lr=0.00025, test_MAE=0.7, time=61.1, train_MAE=0.48, train_loss=0.536, val_MAE=0.651, val_loss=0.706]Epoch 22:   2%|▏         | 22/1000 [25:03<16:28:15, 60.63s/it, lr=0.00025, test_MAE=0.698, time=60.3, train_MAE=0.477, train_loss=0.532, val_MAE=0.639, val_loss=0.695]Epoch    23: reducing learning rate of group 0 to 1.2500e-04.
Epoch 22:   2%|▏         | 23/1000 [25:03<16:25:56, 60.55s/it, lr=0.00025, test_MAE=0.698, time=60.3, train_MAE=0.477, train_loss=0.532, val_MAE=0.639, val_loss=0.695]Epoch 23:   2%|▏         | 23/1000 [25:03<16:25:56, 60.55s/it, lr=0.00025, test_MAE=0.698, time=60.3, train_MAE=0.477, train_loss=0.532, val_MAE=0.639, val_loss=0.695]Epoch 23:   2%|▏         | 23/1000 [26:04<16:25:56, 60.55s/it, lr=0.000125, test_MAE=0.681, time=61.1, train_MAE=0.458, train_loss=0.512, val_MAE=0.632, val_loss=0.685]Epoch 23:   2%|▏         | 24/1000 [26:04<16:27:39, 60.72s/it, lr=0.000125, test_MAE=0.681, time=61.1, train_MAE=0.458, train_loss=0.512, val_MAE=0.632, val_loss=0.685]Epoch 24:   2%|▏         | 24/1000 [26:04<16:27:39, 60.72s/it, lr=0.000125, test_MAE=0.681, time=61.1, train_MAE=0.458, train_loss=0.512, val_MAE=0.632, val_loss=0.685]Epoch 24:   2%|▏         | 24/1000 [27:04<16:27:39, 60.72s/it, lr=0.000125, test_MAE=0.698, time=60.8, train_MAE=0.451, train_loss=0.504, val_MAE=0.645, val_loss=0.698]Epoch 24:   2%|▎         | 25/1000 [27:04<16:26:55, 60.73s/it, lr=0.000125, test_MAE=0.698, time=60.8, train_MAE=0.451, train_loss=0.504, val_MAE=0.645, val_loss=0.698]Epoch 25:   2%|▎         | 25/1000 [27:04<16:26:55, 60.73s/it, lr=0.000125, test_MAE=0.698, time=60.8, train_MAE=0.451, train_loss=0.504, val_MAE=0.645, val_loss=0.698]Epoch 25:   2%|▎         | 25/1000 [28:05<16:26:55, 60.73s/it, lr=0.000125, test_MAE=0.671, time=60.6, train_MAE=0.437, train_loss=0.49, val_MAE=0.623, val_loss=0.676] Epoch 25:   3%|▎         | 26/1000 [28:05<16:25:10, 60.69s/it, lr=0.000125, test_MAE=0.671, time=60.6, train_MAE=0.437, train_loss=0.49, val_MAE=0.623, val_loss=0.676]Epoch 26:   3%|▎         | 26/1000 [28:05<16:25:10, 60.69s/it, lr=0.000125, test_MAE=0.671, time=60.6, train_MAE=0.437, train_loss=0.49, val_MAE=0.623, val_loss=0.676]Epoch 26:   3%|▎         | 26/1000 [29:06<16:25:10, 60.69s/it, lr=0.000125, test_MAE=0.676, time=61, train_MAE=0.433, train_loss=0.486, val_MAE=0.627, val_loss=0.68]  Epoch 26:   3%|▎         | 27/1000 [29:06<16:25:56, 60.80s/it, lr=0.000125, test_MAE=0.676, time=61, train_MAE=0.433, train_loss=0.486, val_MAE=0.627, val_loss=0.68]Epoch 27:   3%|▎         | 27/1000 [29:06<16:25:56, 60.80s/it, lr=0.000125, test_MAE=0.676, time=61, train_MAE=0.433, train_loss=0.486, val_MAE=0.627, val_loss=0.68]Epoch 27:   3%|▎         | 27/1000 [30:07<16:25:56, 60.80s/it, lr=0.000125, test_MAE=0.696, time=60.8, train_MAE=0.433, train_loss=0.487, val_MAE=0.644, val_loss=0.697]Epoch 27:   3%|▎         | 28/1000 [30:07<16:25:10, 60.81s/it, lr=0.000125, test_MAE=0.696, time=60.8, train_MAE=0.433, train_loss=0.487, val_MAE=0.644, val_loss=0.697]Epoch 28:   3%|▎         | 28/1000 [30:07<16:25:10, 60.81s/it, lr=0.000125, test_MAE=0.696, time=60.8, train_MAE=0.433, train_loss=0.487, val_MAE=0.644, val_loss=0.697]Epoch 28:   3%|▎         | 28/1000 [31:07<16:25:10, 60.81s/it, lr=0.000125, test_MAE=0.701, time=60.4, train_MAE=0.435, train_loss=0.489, val_MAE=0.647, val_loss=0.701]Epoch    29: reducing learning rate of group 0 to 6.2500e-05.
Epoch 28:   3%|▎         | 29/1000 [31:07<16:22:23, 60.70s/it, lr=0.000125, test_MAE=0.701, time=60.4, train_MAE=0.435, train_loss=0.489, val_MAE=0.647, val_loss=0.701]Epoch 29:   3%|▎         | 29/1000 [31:07<16:22:23, 60.70s/it, lr=0.000125, test_MAE=0.701, time=60.4, train_MAE=0.435, train_loss=0.489, val_MAE=0.647, val_loss=0.701]Epoch 29:   3%|▎         | 29/1000 [32:09<16:22:23, 60.70s/it, lr=6.25e-5, test_MAE=0.697, time=61.2, train_MAE=0.422, train_loss=0.475, val_MAE=0.645, val_loss=0.697] Epoch 29:   3%|▎         | 30/1000 [32:09<16:24:04, 60.87s/it, lr=6.25e-5, test_MAE=0.697, time=61.2, train_MAE=0.422, train_loss=0.475, val_MAE=0.645, val_loss=0.697]Epoch 30:   3%|▎         | 30/1000 [32:09<16:24:04, 60.87s/it, lr=6.25e-5, test_MAE=0.697, time=61.2, train_MAE=0.422, train_loss=0.475, val_MAE=0.645, val_loss=0.697]Epoch 30:   3%|▎         | 30/1000 [33:07<16:24:04, 60.87s/it, lr=6.25e-5, test_MAE=0.681, time=58.3, train_MAE=0.411, train_loss=0.463, val_MAE=0.63, val_loss=0.682] Epoch 30:   3%|▎         | 31/1000 [33:07<16:10:35, 60.10s/it, lr=6.25e-5, test_MAE=0.681, time=58.3, train_MAE=0.411, train_loss=0.463, val_MAE=0.63, val_loss=0.682]Epoch 31:   3%|▎         | 31/1000 [33:07<16:10:35, 60.10s/it, lr=6.25e-5, test_MAE=0.681, time=58.3, train_MAE=0.411, train_loss=0.463, val_MAE=0.63, val_loss=0.682]Epoch 31:   3%|▎         | 31/1000 [33:59<16:10:35, 60.10s/it, lr=6.25e-5, test_MAE=0.699, time=51.8, train_MAE=0.419, train_loss=0.471, val_MAE=0.647, val_loss=0.699]Epoch 31:   3%|▎         | 32/1000 [33:59<15:29:22, 57.61s/it, lr=6.25e-5, test_MAE=0.699, time=51.8, train_MAE=0.419, train_loss=0.471, val_MAE=0.647, val_loss=0.699]Epoch 32:   3%|▎         | 32/1000 [33:59<15:29:22, 57.61s/it, lr=6.25e-5, test_MAE=0.699, time=51.8, train_MAE=0.419, train_loss=0.471, val_MAE=0.647, val_loss=0.699]Epoch 32:   3%|▎         | 32/1000 [34:51<15:29:22, 57.61s/it, lr=6.25e-5, test_MAE=0.683, time=52, train_MAE=0.418, train_loss=0.47, val_MAE=0.631, val_loss=0.683]   Epoch 32:   3%|▎         | 33/1000 [34:51<15:01:24, 55.93s/it, lr=6.25e-5, test_MAE=0.683, time=52, train_MAE=0.418, train_loss=0.47, val_MAE=0.631, val_loss=0.683]Epoch 33:   3%|▎         | 33/1000 [34:51<15:01:24, 55.93s/it, lr=6.25e-5, test_MAE=0.683, time=52, train_MAE=0.418, train_loss=0.47, val_MAE=0.631, val_loss=0.683]Epoch 33:   3%|▎         | 33/1000 [35:43<15:01:24, 55.93s/it, lr=6.25e-5, test_MAE=0.684, time=52.5, train_MAE=0.405, train_loss=0.457, val_MAE=0.635, val_loss=0.687]Epoch 33:   3%|▎         | 34/1000 [35:43<14:43:54, 54.90s/it, lr=6.25e-5, test_MAE=0.684, time=52.5, train_MAE=0.405, train_loss=0.457, val_MAE=0.635, val_loss=0.687]Epoch 34:   3%|▎         | 34/1000 [35:43<14:43:54, 54.90s/it, lr=6.25e-5, test_MAE=0.684, time=52.5, train_MAE=0.405, train_loss=0.457, val_MAE=0.635, val_loss=0.687]Epoch 34:   3%|▎         | 34/1000 [36:35<14:43:54, 54.90s/it, lr=6.25e-5, test_MAE=0.715, time=51.8, train_MAE=0.407, train_loss=0.459, val_MAE=0.662, val_loss=0.714]Epoch    35: reducing learning rate of group 0 to 3.1250e-05.
Epoch 34:   4%|▎         | 35/1000 [36:35<14:28:17, 53.99s/it, lr=6.25e-5, test_MAE=0.715, time=51.8, train_MAE=0.407, train_loss=0.459, val_MAE=0.662, val_loss=0.714]Epoch 35:   4%|▎         | 35/1000 [36:35<14:28:17, 53.99s/it, lr=6.25e-5, test_MAE=0.715, time=51.8, train_MAE=0.407, train_loss=0.459, val_MAE=0.662, val_loss=0.714]Epoch 35:   4%|▎         | 35/1000 [37:27<14:28:17, 53.99s/it, lr=3.13e-5, test_MAE=0.695, time=52.1, train_MAE=0.397, train_loss=0.448, val_MAE=0.641, val_loss=0.692]Epoch 35:   4%|▎         | 36/1000 [37:27<14:18:11, 53.41s/it, lr=3.13e-5, test_MAE=0.695, time=52.1, train_MAE=0.397, train_loss=0.448, val_MAE=0.641, val_loss=0.692]Epoch 36:   4%|▎         | 36/1000 [37:27<14:18:11, 53.41s/it, lr=3.13e-5, test_MAE=0.695, time=52.1, train_MAE=0.397, train_loss=0.448, val_MAE=0.641, val_loss=0.692]Epoch 36:   4%|▎         | 36/1000 [38:19<14:18:11, 53.41s/it, lr=3.13e-5, test_MAE=0.699, time=52.1, train_MAE=0.397, train_loss=0.448, val_MAE=0.649, val_loss=0.701]Epoch 36:   4%|▎         | 37/1000 [38:19<14:10:51, 53.01s/it, lr=3.13e-5, test_MAE=0.699, time=52.1, train_MAE=0.397, train_loss=0.448, val_MAE=0.649, val_loss=0.701]Epoch 37:   4%|▎         | 37/1000 [38:19<14:10:51, 53.01s/it, lr=3.13e-5, test_MAE=0.699, time=52.1, train_MAE=0.397, train_loss=0.448, val_MAE=0.649, val_loss=0.701]Epoch 37:   4%|▎         | 37/1000 [39:11<14:10:51, 53.01s/it, lr=3.13e-5, test_MAE=0.688, time=51.8, train_MAE=0.395, train_loss=0.446, val_MAE=0.637, val_loss=0.688]Epoch 37:   4%|▍         | 38/1000 [39:11<14:04:03, 52.64s/it, lr=3.13e-5, test_MAE=0.688, time=51.8, train_MAE=0.395, train_loss=0.446, val_MAE=0.637, val_loss=0.688]Epoch 38:   4%|▍         | 38/1000 [39:11<14:04:03, 52.64s/it, lr=3.13e-5, test_MAE=0.688, time=51.8, train_MAE=0.395, train_loss=0.446, val_MAE=0.637, val_loss=0.688]Epoch 38:   4%|▍         | 38/1000 [40:04<14:04:03, 52.64s/it, lr=3.13e-5, test_MAE=0.692, time=52.5, train_MAE=0.395, train_loss=0.446, val_MAE=0.642, val_loss=0.693]Epoch 38:   4%|▍         | 39/1000 [40:04<14:02:30, 52.60s/it, lr=3.13e-5, test_MAE=0.692, time=52.5, train_MAE=0.395, train_loss=0.446, val_MAE=0.642, val_loss=0.693]Epoch 39:   4%|▍         | 39/1000 [40:04<14:02:30, 52.60s/it, lr=3.13e-5, test_MAE=0.692, time=52.5, train_MAE=0.395, train_loss=0.446, val_MAE=0.642, val_loss=0.693]Epoch 39:   4%|▍         | 39/1000 [40:56<14:02:30, 52.60s/it, lr=3.13e-5, test_MAE=0.69, time=52.2, train_MAE=0.393, train_loss=0.444, val_MAE=0.639, val_loss=0.69]  Epoch 39:   4%|▍         | 40/1000 [40:56<13:59:37, 52.48s/it, lr=3.13e-5, test_MAE=0.69, time=52.2, train_MAE=0.393, train_loss=0.444, val_MAE=0.639, val_loss=0.69]Epoch 40:   4%|▍         | 40/1000 [40:56<13:59:37, 52.48s/it, lr=3.13e-5, test_MAE=0.69, time=52.2, train_MAE=0.393, train_loss=0.444, val_MAE=0.639, val_loss=0.69]Epoch 40:   4%|▍         | 40/1000 [41:48<13:59:37, 52.48s/it, lr=3.13e-5, test_MAE=0.69, time=52.1, train_MAE=0.394, train_loss=0.445, val_MAE=0.64, val_loss=0.691]Epoch    41: reducing learning rate of group 0 to 1.5625e-05.
Epoch 40:   4%|▍         | 41/1000 [41:48<13:56:46, 52.35s/it, lr=3.13e-5, test_MAE=0.69, time=52.1, train_MAE=0.394, train_loss=0.445, val_MAE=0.64, val_loss=0.691]Epoch 41:   4%|▍         | 41/1000 [41:48<13:56:46, 52.35s/it, lr=3.13e-5, test_MAE=0.69, time=52.1, train_MAE=0.394, train_loss=0.445, val_MAE=0.64, val_loss=0.691]Epoch 41:   4%|▍         | 41/1000 [42:40<13:56:46, 52.35s/it, lr=1.56e-5, test_MAE=0.692, time=52.3, train_MAE=0.384, train_loss=0.435, val_MAE=0.642, val_loss=0.692]Epoch 41:   4%|▍         | 42/1000 [42:40<13:55:31, 52.33s/it, lr=1.56e-5, test_MAE=0.692, time=52.3, train_MAE=0.384, train_loss=0.435, val_MAE=0.642, val_loss=0.692]Epoch 42:   4%|▍         | 42/1000 [42:40<13:55:31, 52.33s/it, lr=1.56e-5, test_MAE=0.692, time=52.3, train_MAE=0.384, train_loss=0.435, val_MAE=0.642, val_loss=0.692]Epoch 42:   4%|▍         | 42/1000 [43:32<13:55:31, 52.33s/it, lr=1.56e-5, test_MAE=0.696, time=51.7, train_MAE=0.395, train_loss=0.446, val_MAE=0.645, val_loss=0.696]Epoch 42:   4%|▍         | 43/1000 [43:32<13:51:55, 52.16s/it, lr=1.56e-5, test_MAE=0.696, time=51.7, train_MAE=0.395, train_loss=0.446, val_MAE=0.645, val_loss=0.696]Epoch 43:   4%|▍         | 43/1000 [43:32<13:51:55, 52.16s/it, lr=1.56e-5, test_MAE=0.696, time=51.7, train_MAE=0.395, train_loss=0.446, val_MAE=0.645, val_loss=0.696]Epoch 43:   4%|▍         | 43/1000 [44:24<13:51:55, 52.16s/it, lr=1.56e-5, test_MAE=0.691, time=52.4, train_MAE=0.385, train_loss=0.435, val_MAE=0.64, val_loss=0.691] Epoch 43:   4%|▍         | 44/1000 [44:24<13:52:20, 52.24s/it, lr=1.56e-5, test_MAE=0.691, time=52.4, train_MAE=0.385, train_loss=0.435, val_MAE=0.64, val_loss=0.691]Epoch 44:   4%|▍         | 44/1000 [44:24<13:52:20, 52.24s/it, lr=1.56e-5, test_MAE=0.691, time=52.4, train_MAE=0.385, train_loss=0.435, val_MAE=0.64, val_loss=0.691]Epoch 44:   4%|▍         | 44/1000 [45:16<13:52:20, 52.24s/it, lr=1.56e-5, test_MAE=0.695, time=52.2, train_MAE=0.382, train_loss=0.433, val_MAE=0.647, val_loss=0.698]Epoch 44:   4%|▍         | 45/1000 [45:16<13:51:14, 52.22s/it, lr=1.56e-5, test_MAE=0.695, time=52.2, train_MAE=0.382, train_loss=0.433, val_MAE=0.647, val_loss=0.698]Epoch 45:   4%|▍         | 45/1000 [45:16<13:51:14, 52.22s/it, lr=1.56e-5, test_MAE=0.695, time=52.2, train_MAE=0.382, train_loss=0.433, val_MAE=0.647, val_loss=0.698]Epoch 45:   4%|▍         | 45/1000 [46:08<13:51:14, 52.22s/it, lr=1.56e-5, test_MAE=0.692, time=52, train_MAE=0.378, train_loss=0.428, val_MAE=0.642, val_loss=0.692]  Epoch 45:   5%|▍         | 46/1000 [46:08<13:49:32, 52.17s/it, lr=1.56e-5, test_MAE=0.692, time=52, train_MAE=0.378, train_loss=0.428, val_MAE=0.642, val_loss=0.692]Epoch 46:   5%|▍         | 46/1000 [46:09<13:49:32, 52.17s/it, lr=1.56e-5, test_MAE=0.692, time=52, train_MAE=0.378, train_loss=0.428, val_MAE=0.642, val_loss=0.692]Epoch 46:   5%|▍         | 46/1000 [47:01<13:49:32, 52.17s/it, lr=1.56e-5, test_MAE=0.694, time=52.6, train_MAE=0.379, train_loss=0.43, val_MAE=0.643, val_loss=0.694]Epoch    47: reducing learning rate of group 0 to 7.8125e-06.

!! LR EQUAL TO MIN LR SET.
Epoch 46:   5%|▍         | 46/1000 [47:01<16:15:18, 61.34s/it, lr=1.56e-5, test_MAE=0.694, time=52.6, train_MAE=0.379, train_loss=0.43, val_MAE=0.643, val_loss=0.694]
Test MAE: 0.6943
Train MAE: 0.3535
Convergence Time (Epochs): 46.0000
TOTAL TIME TAKEN: 2856.5043s
AVG TIME PER EPOCH: 60.0213s

I'm echoing to stdout
I'm echoing to stderr
My JobID is 56785786
I have 8 CPUs on node r108u25n01
Using backend: pytorch
cuda not available
[I] Loading dataset ZINC...
train, test, val sizes : 10000 1000 1000
[I] Finished loading.
[I] Data load time: 5.1235s
Dataset: ZINC,
Model: EigvalFilters

params={'seed': 41, 'epochs': 1000, 'batch_size': 128, 'init_lr': 0.001, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 5, 'min_lr': 1e-05, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 48}

net_params={'L': 4, 'hidden_dim': 53, 'out_dim': 53, 'residual': True, 'readout': 'mean', 'k': 2, 'in_feat_dropout': 0.0, 'dropout': 0.0, 'graph_norm': True, 'batch_norm': True, 'self_loop': False, 'subtype': 'parallel_dense_simp', 'normalized_laplacian': False, 'post_normalized': True, 'eigval_norm': 'scale(0,2)_all', 'num_eigs': 64, 'bias_mode': '', 'eigval_hidden_dim': 5, 'eigval_num_hidden_layer': 3, 'l1_reg': 0.0, 'l2_reg': 0.0, 'gen_reg': 5.0, 'eigmod': 'import_csv', 'eigInFiles': {'train': 'supp_data/molecules/zinc_train_rec_full.csv', 'test': 'supp_data/molecules/zinc_test_rec_full.csv', 'val': 'supp_data/molecules/zinc_val_rec_full.csv'}, 'fixMissingPhi1': False, 'extraOrtho': True, 'device': device(type='cpu'), 'gpu_id': 0, 'batch_size': 128, 'biases': False, 'num_atom_type': 28, 'num_bond_type': 4, 'total_param': -1}


Total Parameters: -1


Training Graphs:  10000
Validation Graphs:  1000
Test Graphs:  1000
  0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [03:13<?, ?it/s, lr=0.001, test_MAE=1.4, time=193, train_MAE=1.21, train_loss=1.27, val_MAE=1.32, val_loss=1.38]Epoch 0:   0%|          | 1/1000 [03:13<53:37:17, 193.23s/it, lr=0.001, test_MAE=1.4, time=193, train_MAE=1.21, train_loss=1.27, val_MAE=1.32, val_loss=1.38]Epoch 1:   0%|          | 1/1000 [03:13<53:37:17, 193.23s/it, lr=0.001, test_MAE=1.4, time=193, train_MAE=1.21, train_loss=1.27, val_MAE=1.32, val_loss=1.38]Epoch 1:   0%|          | 1/1000 [04:00<53:37:17, 193.23s/it, lr=0.001, test_MAE=0.882, time=47.7, train_MAE=0.751, train_loss=0.816, val_MAE=0.853, val_loss=0.91]Epoch 1:   0%|          | 2/1000 [04:00<41:27:55, 149.57s/it, lr=0.001, test_MAE=0.882, time=47.7, train_MAE=0.751, train_loss=0.816, val_MAE=0.853, val_loss=0.91]Epoch 2:   0%|          | 2/1000 [04:00<41:27:55, 149.57s/it, lr=0.001, test_MAE=0.882, time=47.7, train_MAE=0.751, train_loss=0.816, val_MAE=0.853, val_loss=0.91]Epoch 2:   0%|          | 2/1000 [04:48<41:27:55, 149.57s/it, lr=0.001, test_MAE=0.731, time=47.6, train_MAE=0.677, train_loss=0.734, val_MAE=0.673, val_loss=0.727]Epoch 2:   0%|          | 3/1000 [04:48<32:57:12, 118.99s/it, lr=0.001, test_MAE=0.731, time=47.6, train_MAE=0.677, train_loss=0.734, val_MAE=0.673, val_loss=0.727]Epoch 3:   0%|          | 3/1000 [04:48<32:57:12, 118.99s/it, lr=0.001, test_MAE=0.731, time=47.6, train_MAE=0.677, train_loss=0.734, val_MAE=0.673, val_loss=0.727]Epoch 3:   0%|          | 3/1000 [05:36<32:57:12, 118.99s/it, lr=0.001, test_MAE=0.749, time=47.6, train_MAE=0.655, train_loss=0.71, val_MAE=0.681, val_loss=0.734] Epoch 3:   0%|          | 4/1000 [05:36<26:59:55, 97.59s/it, lr=0.001, test_MAE=0.749, time=47.6, train_MAE=0.655, train_loss=0.71, val_MAE=0.681, val_loss=0.734] Epoch 4:   0%|          | 4/1000 [05:36<26:59:55, 97.59s/it, lr=0.001, test_MAE=0.749, time=47.6, train_MAE=0.655, train_loss=0.71, val_MAE=0.681, val_loss=0.734]Epoch 4:   0%|          | 4/1000 [06:23<26:59:55, 97.59s/it, lr=0.001, test_MAE=0.73, time=47.6, train_MAE=0.647, train_loss=0.7, val_MAE=0.678, val_loss=0.73]   Epoch 4:   0%|          | 5/1000 [06:23<22:49:47, 82.60s/it, lr=0.001, test_MAE=0.73, time=47.6, train_MAE=0.647, train_loss=0.7, val_MAE=0.678, val_loss=0.73]Epoch 5:   0%|          | 5/1000 [06:23<22:49:47, 82.60s/it, lr=0.001, test_MAE=0.73, time=47.6, train_MAE=0.647, train_loss=0.7, val_MAE=0.678, val_loss=0.73]Epoch 5:   0%|          | 5/1000 [07:11<22:49:47, 82.60s/it, lr=0.001, test_MAE=0.77, time=47.6, train_MAE=0.637, train_loss=0.691, val_MAE=0.725, val_loss=0.779]Epoch 5:   1%|          | 6/1000 [07:11<19:54:45, 72.12s/it, lr=0.001, test_MAE=0.77, time=47.6, train_MAE=0.637, train_loss=0.691, val_MAE=0.725, val_loss=0.779]Epoch 6:   1%|          | 6/1000 [07:11<19:54:45, 72.12s/it, lr=0.001, test_MAE=0.77, time=47.6, train_MAE=0.637, train_loss=0.691, val_MAE=0.725, val_loss=0.779]Epoch 6:   1%|          | 6/1000 [07:59<19:54:45, 72.12s/it, lr=0.001, test_MAE=0.759, time=47.9, train_MAE=0.624, train_loss=0.68, val_MAE=0.707, val_loss=0.762]Epoch 6:   1%|          | 7/1000 [07:59<17:53:32, 64.87s/it, lr=0.001, test_MAE=0.759, time=47.9, train_MAE=0.624, train_loss=0.68, val_MAE=0.707, val_loss=0.762]Epoch 7:   1%|          | 7/1000 [07:59<17:53:32, 64.87s/it, lr=0.001, test_MAE=0.759, time=47.9, train_MAE=0.624, train_loss=0.68, val_MAE=0.707, val_loss=0.762]Epoch 7:   1%|          | 7/1000 [08:47<17:53:32, 64.87s/it, lr=0.001, test_MAE=0.842, time=47.7, train_MAE=0.63, train_loss=0.685, val_MAE=0.78, val_loss=0.833] Epoch 7:   1%|          | 8/1000 [08:47<16:27:18, 59.72s/it, lr=0.001, test_MAE=0.842, time=47.7, train_MAE=0.63, train_loss=0.685, val_MAE=0.78, val_loss=0.833]Epoch 8:   1%|          | 8/1000 [08:47<16:27:18, 59.72s/it, lr=0.001, test_MAE=0.842, time=47.7, train_MAE=0.63, train_loss=0.685, val_MAE=0.78, val_loss=0.833]Epoch 8:   1%|          | 8/1000 [09:34<16:27:18, 59.72s/it, lr=0.001, test_MAE=0.732, time=47.7, train_MAE=0.616, train_loss=0.672, val_MAE=0.694, val_loss=0.75]Epoch     9: reducing learning rate of group 0 to 5.0000e-04.
Epoch 8:   1%|          | 9/1000 [09:34<15:26:46, 56.11s/it, lr=0.001, test_MAE=0.732, time=47.7, train_MAE=0.616, train_loss=0.672, val_MAE=0.694, val_loss=0.75]Epoch 9:   1%|          | 9/1000 [09:34<15:26:46, 56.11s/it, lr=0.001, test_MAE=0.732, time=47.7, train_MAE=0.616, train_loss=0.672, val_MAE=0.694, val_loss=0.75]Epoch 9:   1%|          | 9/1000 [10:22<15:26:46, 56.11s/it, lr=0.0005, test_MAE=0.677, time=47.9, train_MAE=0.601, train_loss=0.653, val_MAE=0.624, val_loss=0.673]Epoch 9:   1%|          | 10/1000 [10:22<14:45:28, 53.67s/it, lr=0.0005, test_MAE=0.677, time=47.9, train_MAE=0.601, train_loss=0.653, val_MAE=0.624, val_loss=0.673]Epoch 10:   1%|          | 10/1000 [10:22<14:45:28, 53.67s/it, lr=0.0005, test_MAE=0.677, time=47.9, train_MAE=0.601, train_loss=0.653, val_MAE=0.624, val_loss=0.673]Epoch 10:   1%|          | 10/1000 [11:10<14:45:28, 53.67s/it, lr=0.0005, test_MAE=0.689, time=47.6, train_MAE=0.592, train_loss=0.641, val_MAE=0.633, val_loss=0.68] Epoch 10:   1%|          | 11/1000 [11:10<14:14:25, 51.84s/it, lr=0.0005, test_MAE=0.689, time=47.6, train_MAE=0.592, train_loss=0.641, val_MAE=0.633, val_loss=0.68]Epoch 11:   1%|          | 11/1000 [11:10<14:14:25, 51.84s/it, lr=0.0005, test_MAE=0.689, time=47.6, train_MAE=0.592, train_loss=0.641, val_MAE=0.633, val_loss=0.68]Epoch 11:   1%|          | 11/1000 [11:57<14:14:25, 51.84s/it, lr=0.0005, test_MAE=0.693, time=47.4, train_MAE=0.587, train_loss=0.636, val_MAE=0.643, val_loss=0.692]Epoch 11:   1%|          | 12/1000 [11:57<13:51:28, 50.49s/it, lr=0.0005, test_MAE=0.693, time=47.4, train_MAE=0.587, train_loss=0.636, val_MAE=0.643, val_loss=0.692]Epoch 12:   1%|          | 12/1000 [11:57<13:51:28, 50.49s/it, lr=0.0005, test_MAE=0.693, time=47.4, train_MAE=0.587, train_loss=0.636, val_MAE=0.643, val_loss=0.692]Epoch 12:   1%|          | 12/1000 [12:45<13:51:28, 50.49s/it, lr=0.0005, test_MAE=0.751, time=48, train_MAE=0.578, train_loss=0.629, val_MAE=0.723, val_loss=0.773]  Epoch 12:   1%|▏         | 13/1000 [12:45<13:38:16, 49.74s/it, lr=0.0005, test_MAE=0.751, time=48, train_MAE=0.578, train_loss=0.629, val_MAE=0.723, val_loss=0.773]Epoch 13:   1%|▏         | 13/1000 [12:45<13:38:16, 49.74s/it, lr=0.0005, test_MAE=0.751, time=48, train_MAE=0.578, train_loss=0.629, val_MAE=0.723, val_loss=0.773]Epoch 13:   1%|▏         | 13/1000 [13:33<13:38:16, 49.74s/it, lr=0.0005, test_MAE=0.728, time=47.7, train_MAE=0.574, train_loss=0.626, val_MAE=0.659, val_loss=0.71]Epoch 13:   1%|▏         | 14/1000 [13:33<13:27:29, 49.14s/it, lr=0.0005, test_MAE=0.728, time=47.7, train_MAE=0.574, train_loss=0.626, val_MAE=0.659, val_loss=0.71]Epoch 14:   1%|▏         | 14/1000 [13:33<13:27:29, 49.14s/it, lr=0.0005, test_MAE=0.728, time=47.7, train_MAE=0.574, train_loss=0.626, val_MAE=0.659, val_loss=0.71]Epoch 14:   1%|▏         | 14/1000 [14:20<13:27:29, 49.14s/it, lr=0.0005, test_MAE=0.701, time=47.2, train_MAE=0.575, train_loss=0.627, val_MAE=0.646, val_loss=0.699]Epoch 14:   2%|▏         | 15/1000 [14:20<13:17:24, 48.57s/it, lr=0.0005, test_MAE=0.701, time=47.2, train_MAE=0.575, train_loss=0.627, val_MAE=0.646, val_loss=0.699]Epoch 15:   2%|▏         | 15/1000 [14:20<13:17:24, 48.57s/it, lr=0.0005, test_MAE=0.701, time=47.2, train_MAE=0.575, train_loss=0.627, val_MAE=0.646, val_loss=0.699]Epoch 15:   2%|▏         | 15/1000 [15:08<13:17:24, 48.57s/it, lr=0.0005, test_MAE=0.717, time=48, train_MAE=0.58, train_loss=0.632, val_MAE=0.653, val_loss=0.705]   Epoch    16: reducing learning rate of group 0 to 2.5000e-04.
Epoch 15:   2%|▏         | 16/1000 [15:08<13:13:47, 48.40s/it, lr=0.0005, test_MAE=0.717, time=48, train_MAE=0.58, train_loss=0.632, val_MAE=0.653, val_loss=0.705]Epoch 16:   2%|▏         | 16/1000 [15:08<13:13:47, 48.40s/it, lr=0.0005, test_MAE=0.717, time=48, train_MAE=0.58, train_loss=0.632, val_MAE=0.653, val_loss=0.705]Epoch 16:   2%|▏         | 16/1000 [15:56<13:13:47, 48.40s/it, lr=0.00025, test_MAE=0.677, time=47.7, train_MAE=0.557, train_loss=0.607, val_MAE=0.621, val_loss=0.669]Epoch 16:   2%|▏         | 17/1000 [15:56<13:09:32, 48.19s/it, lr=0.00025, test_MAE=0.677, time=47.7, train_MAE=0.557, train_loss=0.607, val_MAE=0.621, val_loss=0.669]Epoch 17:   2%|▏         | 17/1000 [15:56<13:09:32, 48.19s/it, lr=0.00025, test_MAE=0.677, time=47.7, train_MAE=0.557, train_loss=0.607, val_MAE=0.621, val_loss=0.669]Epoch 17:   2%|▏         | 17/1000 [16:43<13:09:32, 48.19s/it, lr=0.00025, test_MAE=0.693, time=47.4, train_MAE=0.551, train_loss=0.599, val_MAE=0.634, val_loss=0.682]Epoch 17:   2%|▏         | 18/1000 [16:43<13:04:53, 47.96s/it, lr=0.00025, test_MAE=0.693, time=47.4, train_MAE=0.551, train_loss=0.599, val_MAE=0.634, val_loss=0.682]Epoch 18:   2%|▏         | 18/1000 [16:43<13:04:53, 47.96s/it, lr=0.00025, test_MAE=0.693, time=47.4, train_MAE=0.551, train_loss=0.599, val_MAE=0.634, val_loss=0.682]Epoch 18:   2%|▏         | 18/1000 [17:31<13:04:53, 47.96s/it, lr=0.00025, test_MAE=0.699, time=47.8, train_MAE=0.55, train_loss=0.598, val_MAE=0.63, val_loss=0.678]  Epoch 18:   2%|▏         | 19/1000 [17:31<13:03:34, 47.93s/it, lr=0.00025, test_MAE=0.699, time=47.8, train_MAE=0.55, train_loss=0.598, val_MAE=0.63, val_loss=0.678]Epoch 19:   2%|▏         | 19/1000 [17:31<13:03:34, 47.93s/it, lr=0.00025, test_MAE=0.699, time=47.8, train_MAE=0.55, train_loss=0.598, val_MAE=0.63, val_loss=0.678]Epoch 19:   2%|▏         | 19/1000 [18:19<13:03:34, 47.93s/it, lr=0.00025, test_MAE=0.675, time=47.7, train_MAE=0.549, train_loss=0.598, val_MAE=0.619, val_loss=0.668]Epoch 19:   2%|▏         | 20/1000 [18:19<13:01:30, 47.85s/it, lr=0.00025, test_MAE=0.675, time=47.7, train_MAE=0.549, train_loss=0.598, val_MAE=0.619, val_loss=0.668]Epoch 20:   2%|▏         | 20/1000 [18:19<13:01:30, 47.85s/it, lr=0.00025, test_MAE=0.675, time=47.7, train_MAE=0.549, train_loss=0.598, val_MAE=0.619, val_loss=0.668]Epoch 20:   2%|▏         | 20/1000 [19:07<13:01:30, 47.85s/it, lr=0.00025, test_MAE=0.679, time=47.7, train_MAE=0.54, train_loss=0.589, val_MAE=0.62, val_loss=0.669]  Epoch 20:   2%|▏         | 21/1000 [19:07<13:00:11, 47.82s/it, lr=0.00025, test_MAE=0.679, time=47.7, train_MAE=0.54, train_loss=0.589, val_MAE=0.62, val_loss=0.669]Epoch 21:   2%|▏         | 21/1000 [19:07<13:00:11, 47.82s/it, lr=0.00025, test_MAE=0.679, time=47.7, train_MAE=0.54, train_loss=0.589, val_MAE=0.62, val_loss=0.669]Epoch 21:   2%|▏         | 21/1000 [19:55<13:00:11, 47.82s/it, lr=0.00025, test_MAE=0.675, time=48, train_MAE=0.542, train_loss=0.592, val_MAE=0.618, val_loss=0.668]Epoch 21:   2%|▏         | 22/1000 [19:55<13:00:13, 47.87s/it, lr=0.00025, test_MAE=0.675, time=48, train_MAE=0.542, train_loss=0.592, val_MAE=0.618, val_loss=0.668]Epoch 22:   2%|▏         | 22/1000 [19:55<13:00:13, 47.87s/it, lr=0.00025, test_MAE=0.675, time=48, train_MAE=0.542, train_loss=0.592, val_MAE=0.618, val_loss=0.668]Epoch 22:   2%|▏         | 22/1000 [20:42<13:00:13, 47.87s/it, lr=0.00025, test_MAE=0.679, time=47.3, train_MAE=0.536, train_loss=0.586, val_MAE=0.618, val_loss=0.668]Epoch 22:   2%|▏         | 23/1000 [20:42<12:56:33, 47.69s/it, lr=0.00025, test_MAE=0.679, time=47.3, train_MAE=0.536, train_loss=0.586, val_MAE=0.618, val_loss=0.668]Epoch 23:   2%|▏         | 23/1000 [20:42<12:56:33, 47.69s/it, lr=0.00025, test_MAE=0.679, time=47.3, train_MAE=0.536, train_loss=0.586, val_MAE=0.618, val_loss=0.668]Epoch 23:   2%|▏         | 23/1000 [21:30<12:56:33, 47.69s/it, lr=0.00025, test_MAE=0.686, time=48, train_MAE=0.534, train_loss=0.584, val_MAE=0.639, val_loss=0.69]   Epoch 23:   2%|▏         | 24/1000 [21:30<12:57:12, 47.78s/it, lr=0.00025, test_MAE=0.686, time=48, train_MAE=0.534, train_loss=0.584, val_MAE=0.639, val_loss=0.69]Epoch 24:   2%|▏         | 24/1000 [21:30<12:57:12, 47.78s/it, lr=0.00025, test_MAE=0.686, time=48, train_MAE=0.534, train_loss=0.584, val_MAE=0.639, val_loss=0.69]Epoch 24:   2%|▏         | 24/1000 [22:18<12:57:12, 47.78s/it, lr=0.00025, test_MAE=0.68, time=47.7, train_MAE=0.533, train_loss=0.584, val_MAE=0.617, val_loss=0.668]Epoch 24:   2%|▎         | 25/1000 [22:18<12:56:01, 47.75s/it, lr=0.00025, test_MAE=0.68, time=47.7, train_MAE=0.533, train_loss=0.584, val_MAE=0.617, val_loss=0.668]Epoch 25:   2%|▎         | 25/1000 [22:18<12:56:01, 47.75s/it, lr=0.00025, test_MAE=0.68, time=47.7, train_MAE=0.533, train_loss=0.584, val_MAE=0.617, val_loss=0.668]Epoch 25:   2%|▎         | 25/1000 [23:05<12:56:01, 47.75s/it, lr=0.00025, test_MAE=0.712, time=47.7, train_MAE=0.526, train_loss=0.577, val_MAE=0.65, val_loss=0.702]Epoch    26: reducing learning rate of group 0 to 1.2500e-04.
Epoch 25:   3%|▎         | 26/1000 [23:05<12:54:49, 47.73s/it, lr=0.00025, test_MAE=0.712, time=47.7, train_MAE=0.526, train_loss=0.577, val_MAE=0.65, val_loss=0.702]Epoch 26:   3%|▎         | 26/1000 [23:05<12:54:49, 47.73s/it, lr=0.00025, test_MAE=0.712, time=47.7, train_MAE=0.526, train_loss=0.577, val_MAE=0.65, val_loss=0.702]Epoch 26:   3%|▎         | 26/1000 [23:53<12:54:49, 47.73s/it, lr=0.000125, test_MAE=0.681, time=47.9, train_MAE=0.512, train_loss=0.562, val_MAE=0.625, val_loss=0.674]Epoch 26:   3%|▎         | 27/1000 [23:53<12:55:08, 47.80s/it, lr=0.000125, test_MAE=0.681, time=47.9, train_MAE=0.512, train_loss=0.562, val_MAE=0.625, val_loss=0.674]Epoch 27:   3%|▎         | 27/1000 [23:53<12:55:08, 47.80s/it, lr=0.000125, test_MAE=0.681, time=47.9, train_MAE=0.512, train_loss=0.562, val_MAE=0.625, val_loss=0.674]Epoch 27:   3%|▎         | 27/1000 [24:41<12:55:08, 47.80s/it, lr=0.000125, test_MAE=0.67, time=47.7, train_MAE=0.512, train_loss=0.561, val_MAE=0.619, val_loss=0.668] Epoch 27:   3%|▎         | 28/1000 [24:41<12:53:55, 47.77s/it, lr=0.000125, test_MAE=0.67, time=47.7, train_MAE=0.512, train_loss=0.561, val_MAE=0.619, val_loss=0.668]Epoch 28:   3%|▎         | 28/1000 [24:41<12:53:55, 47.77s/it, lr=0.000125, test_MAE=0.67, time=47.7, train_MAE=0.512, train_loss=0.561, val_MAE=0.619, val_loss=0.668]Epoch 28:   3%|▎         | 28/1000 [25:28<12:53:55, 47.77s/it, lr=0.000125, test_MAE=0.675, time=47.3, train_MAE=0.508, train_loss=0.557, val_MAE=0.629, val_loss=0.678]Epoch 28:   3%|▎         | 29/1000 [25:28<12:50:41, 47.62s/it, lr=0.000125, test_MAE=0.675, time=47.3, train_MAE=0.508, train_loss=0.557, val_MAE=0.629, val_loss=0.678]Epoch 29:   3%|▎         | 29/1000 [25:28<12:50:41, 47.62s/it, lr=0.000125, test_MAE=0.675, time=47.3, train_MAE=0.508, train_loss=0.557, val_MAE=0.629, val_loss=0.678]Epoch 29:   3%|▎         | 29/1000 [26:16<12:50:41, 47.62s/it, lr=0.000125, test_MAE=0.68, time=48.3, train_MAE=0.501, train_loss=0.55, val_MAE=0.623, val_loss=0.672]  Epoch 29:   3%|▎         | 30/1000 [26:16<12:53:06, 47.82s/it, lr=0.000125, test_MAE=0.68, time=48.3, train_MAE=0.501, train_loss=0.55, val_MAE=0.623, val_loss=0.672]Epoch 30:   3%|▎         | 30/1000 [26:16<12:53:06, 47.82s/it, lr=0.000125, test_MAE=0.68, time=48.3, train_MAE=0.501, train_loss=0.55, val_MAE=0.623, val_loss=0.672]Epoch 30:   3%|▎         | 30/1000 [27:04<12:53:06, 47.82s/it, lr=0.000125, test_MAE=0.676, time=47.7, train_MAE=0.497, train_loss=0.547, val_MAE=0.624, val_loss=0.673]Epoch 30:   3%|▎         | 31/1000 [27:04<12:51:34, 47.78s/it, lr=0.000125, test_MAE=0.676, time=47.7, train_MAE=0.497, train_loss=0.547, val_MAE=0.624, val_loss=0.673]Epoch 31:   3%|▎         | 31/1000 [27:04<12:51:34, 47.78s/it, lr=0.000125, test_MAE=0.676, time=47.7, train_MAE=0.497, train_loss=0.547, val_MAE=0.624, val_loss=0.673]Epoch 31:   3%|▎         | 31/1000 [27:51<12:51:34, 47.78s/it, lr=0.000125, test_MAE=0.673, time=47.3, train_MAE=0.502, train_loss=0.551, val_MAE=0.62, val_loss=0.669] Epoch    32: reducing learning rate of group 0 to 6.2500e-05.
Epoch 31:   3%|▎         | 32/1000 [27:51<12:48:33, 47.64s/it, lr=0.000125, test_MAE=0.673, time=47.3, train_MAE=0.502, train_loss=0.551, val_MAE=0.62, val_loss=0.669]Epoch 32:   3%|▎         | 32/1000 [27:51<12:48:33, 47.64s/it, lr=0.000125, test_MAE=0.673, time=47.3, train_MAE=0.502, train_loss=0.551, val_MAE=0.62, val_loss=0.669]Epoch 32:   3%|▎         | 32/1000 [28:39<12:48:33, 47.64s/it, lr=6.25e-5, test_MAE=0.676, time=48, train_MAE=0.491, train_loss=0.54, val_MAE=0.626, val_loss=0.674]   Epoch 32:   3%|▎         | 33/1000 [28:39<12:49:42, 47.76s/it, lr=6.25e-5, test_MAE=0.676, time=48, train_MAE=0.491, train_loss=0.54, val_MAE=0.626, val_loss=0.674]Epoch 33:   3%|▎         | 33/1000 [28:39<12:49:42, 47.76s/it, lr=6.25e-5, test_MAE=0.676, time=48, train_MAE=0.491, train_loss=0.54, val_MAE=0.626, val_loss=0.674]Epoch 33:   3%|▎         | 33/1000 [29:27<12:49:42, 47.76s/it, lr=6.25e-5, test_MAE=0.672, time=47.7, train_MAE=0.494, train_loss=0.542, val_MAE=0.622, val_loss=0.67]Epoch 33:   3%|▎         | 34/1000 [29:27<12:48:26, 47.73s/it, lr=6.25e-5, test_MAE=0.672, time=47.7, train_MAE=0.494, train_loss=0.542, val_MAE=0.622, val_loss=0.67]Epoch 34:   3%|▎         | 34/1000 [29:27<12:48:26, 47.73s/it, lr=6.25e-5, test_MAE=0.672, time=47.7, train_MAE=0.494, train_loss=0.542, val_MAE=0.622, val_loss=0.67]Epoch 34:   3%|▎         | 34/1000 [30:14<12:48:26, 47.73s/it, lr=6.25e-5, test_MAE=0.676, time=47.4, train_MAE=0.485, train_loss=0.533, val_MAE=0.623, val_loss=0.671]Epoch 34:   4%|▎         | 35/1000 [30:14<12:45:51, 47.62s/it, lr=6.25e-5, test_MAE=0.676, time=47.4, train_MAE=0.485, train_loss=0.533, val_MAE=0.623, val_loss=0.671]Epoch 35:   4%|▎         | 35/1000 [30:14<12:45:51, 47.62s/it, lr=6.25e-5, test_MAE=0.676, time=47.4, train_MAE=0.485, train_loss=0.533, val_MAE=0.623, val_loss=0.671]Epoch 35:   4%|▎         | 35/1000 [31:02<12:45:51, 47.62s/it, lr=6.25e-5, test_MAE=0.679, time=48, train_MAE=0.483, train_loss=0.531, val_MAE=0.628, val_loss=0.676]  Epoch 35:   4%|▎         | 36/1000 [31:02<12:46:54, 47.73s/it, lr=6.25e-5, test_MAE=0.679, time=48, train_MAE=0.483, train_loss=0.531, val_MAE=0.628, val_loss=0.676]Epoch 36:   4%|▎         | 36/1000 [31:02<12:46:54, 47.73s/it, lr=6.25e-5, test_MAE=0.679, time=48, train_MAE=0.483, train_loss=0.531, val_MAE=0.628, val_loss=0.676]Epoch 36:   4%|▎         | 36/1000 [31:50<12:46:54, 47.73s/it, lr=6.25e-5, test_MAE=0.674, time=47.7, train_MAE=0.488, train_loss=0.536, val_MAE=0.627, val_loss=0.674]Epoch 36:   4%|▎         | 37/1000 [31:50<12:46:00, 47.73s/it, lr=6.25e-5, test_MAE=0.674, time=47.7, train_MAE=0.488, train_loss=0.536, val_MAE=0.627, val_loss=0.674]Epoch 37:   4%|▎         | 37/1000 [31:50<12:46:00, 47.73s/it, lr=6.25e-5, test_MAE=0.674, time=47.7, train_MAE=0.488, train_loss=0.536, val_MAE=0.627, val_loss=0.674]Epoch 37:   4%|▎         | 37/1000 [32:38<12:46:00, 47.73s/it, lr=6.25e-5, test_MAE=0.675, time=47.3, train_MAE=0.486, train_loss=0.534, val_MAE=0.622, val_loss=0.67] Epoch    38: reducing learning rate of group 0 to 3.1250e-05.
Epoch 37:   4%|▍         | 38/1000 [32:38<12:43:25, 47.62s/it, lr=6.25e-5, test_MAE=0.675, time=47.3, train_MAE=0.486, train_loss=0.534, val_MAE=0.622, val_loss=0.67]Epoch 38:   4%|▍         | 38/1000 [32:38<12:43:25, 47.62s/it, lr=6.25e-5, test_MAE=0.675, time=47.3, train_MAE=0.486, train_loss=0.534, val_MAE=0.622, val_loss=0.67]Epoch 38:   4%|▍         | 38/1000 [33:26<12:43:25, 47.62s/it, lr=3.13e-5, test_MAE=0.672, time=48, train_MAE=0.492, train_loss=0.539, val_MAE=0.627, val_loss=0.674] Epoch 38:   4%|▍         | 39/1000 [33:26<12:44:32, 47.73s/it, lr=3.13e-5, test_MAE=0.672, time=48, train_MAE=0.492, train_loss=0.539, val_MAE=0.627, val_loss=0.674]Epoch 39:   4%|▍         | 39/1000 [33:26<12:44:32, 47.73s/it, lr=3.13e-5, test_MAE=0.672, time=48, train_MAE=0.492, train_loss=0.539, val_MAE=0.627, val_loss=0.674]Epoch 39:   4%|▍         | 39/1000 [34:13<12:44:32, 47.73s/it, lr=3.13e-5, test_MAE=0.678, time=47.6, train_MAE=0.476, train_loss=0.523, val_MAE=0.628, val_loss=0.676]Epoch 39:   4%|▍         | 40/1000 [34:13<12:43:15, 47.70s/it, lr=3.13e-5, test_MAE=0.678, time=47.6, train_MAE=0.476, train_loss=0.523, val_MAE=0.628, val_loss=0.676]Epoch 40:   4%|▍         | 40/1000 [34:13<12:43:15, 47.70s/it, lr=3.13e-5, test_MAE=0.678, time=47.6, train_MAE=0.476, train_loss=0.523, val_MAE=0.628, val_loss=0.676]Epoch 40:   4%|▍         | 40/1000 [35:01<12:43:15, 47.70s/it, lr=3.13e-5, test_MAE=0.675, time=47.6, train_MAE=0.48, train_loss=0.528, val_MAE=0.626, val_loss=0.673] Epoch 40:   4%|▍         | 41/1000 [35:01<12:42:14, 47.69s/it, lr=3.13e-5, test_MAE=0.675, time=47.6, train_MAE=0.48, train_loss=0.528, val_MAE=0.626, val_loss=0.673]Epoch 41:   4%|▍         | 41/1000 [35:01<12:42:14, 47.69s/it, lr=3.13e-5, test_MAE=0.675, time=47.6, train_MAE=0.48, train_loss=0.528, val_MAE=0.626, val_loss=0.673]Epoch 41:   4%|▍         | 41/1000 [35:49<12:42:14, 47.69s/it, lr=3.13e-5, test_MAE=0.679, time=48, train_MAE=0.487, train_loss=0.534, val_MAE=0.63, val_loss=0.677]  Epoch 41:   4%|▍         | 42/1000 [35:49<12:42:58, 47.79s/it, lr=3.13e-5, test_MAE=0.679, time=48, train_MAE=0.487, train_loss=0.534, val_MAE=0.63, val_loss=0.677]Epoch 42:   4%|▍         | 42/1000 [35:49<12:42:58, 47.79s/it, lr=3.13e-5, test_MAE=0.679, time=48, train_MAE=0.487, train_loss=0.534, val_MAE=0.63, val_loss=0.677]Epoch 42:   4%|▍         | 42/1000 [36:36<12:42:58, 47.79s/it, lr=3.13e-5, test_MAE=0.672, time=47.3, train_MAE=0.479, train_loss=0.526, val_MAE=0.624, val_loss=0.672]Epoch 42:   4%|▍         | 43/1000 [36:36<12:40:02, 47.65s/it, lr=3.13e-5, test_MAE=0.672, time=47.3, train_MAE=0.479, train_loss=0.526, val_MAE=0.624, val_loss=0.672]Epoch 43:   4%|▍         | 43/1000 [36:36<12:40:02, 47.65s/it, lr=3.13e-5, test_MAE=0.672, time=47.3, train_MAE=0.479, train_loss=0.526, val_MAE=0.624, val_loss=0.672]Epoch 43:   4%|▍         | 43/1000 [37:24<12:40:02, 47.65s/it, lr=3.13e-5, test_MAE=0.675, time=47.9, train_MAE=0.477, train_loss=0.525, val_MAE=0.627, val_loss=0.674]Epoch    44: reducing learning rate of group 0 to 1.5625e-05.
Epoch 43:   4%|▍         | 44/1000 [37:24<12:40:41, 47.74s/it, lr=3.13e-5, test_MAE=0.675, time=47.9, train_MAE=0.477, train_loss=0.525, val_MAE=0.627, val_loss=0.674]Epoch 44:   4%|▍         | 44/1000 [37:24<12:40:41, 47.74s/it, lr=3.13e-5, test_MAE=0.675, time=47.9, train_MAE=0.477, train_loss=0.525, val_MAE=0.627, val_loss=0.674]Epoch 44:   4%|▍         | 44/1000 [38:12<12:40:41, 47.74s/it, lr=1.56e-5, test_MAE=0.673, time=47.7, train_MAE=0.467, train_loss=0.514, val_MAE=0.623, val_loss=0.67] Epoch 44:   4%|▍         | 45/1000 [38:12<12:39:44, 47.73s/it, lr=1.56e-5, test_MAE=0.673, time=47.7, train_MAE=0.467, train_loss=0.514, val_MAE=0.623, val_loss=0.67]Epoch 45:   4%|▍         | 45/1000 [38:12<12:39:44, 47.73s/it, lr=1.56e-5, test_MAE=0.673, time=47.7, train_MAE=0.467, train_loss=0.514, val_MAE=0.623, val_loss=0.67]Epoch 45:   4%|▍         | 45/1000 [38:59<12:39:44, 47.73s/it, lr=1.56e-5, test_MAE=0.671, time=47.6, train_MAE=0.477, train_loss=0.524, val_MAE=0.623, val_loss=0.67]Epoch 45:   5%|▍         | 46/1000 [39:00<12:38:31, 47.71s/it, lr=1.56e-5, test_MAE=0.671, time=47.6, train_MAE=0.477, train_loss=0.524, val_MAE=0.623, val_loss=0.67]Epoch 46:   5%|▍         | 46/1000 [39:00<12:38:31, 47.71s/it, lr=1.56e-5, test_MAE=0.671, time=47.6, train_MAE=0.477, train_loss=0.524, val_MAE=0.623, val_loss=0.67]Epoch 46:   5%|▍         | 46/1000 [39:48<12:38:31, 47.71s/it, lr=1.56e-5, test_MAE=0.671, time=48, train_MAE=0.47, train_loss=0.517, val_MAE=0.625, val_loss=0.671]  Epoch 46:   5%|▍         | 47/1000 [39:48<12:39:19, 47.81s/it, lr=1.56e-5, test_MAE=0.671, time=48, train_MAE=0.47, train_loss=0.517, val_MAE=0.625, val_loss=0.671]Epoch 47:   5%|▍         | 47/1000 [39:48<12:39:19, 47.81s/it, lr=1.56e-5, test_MAE=0.671, time=48, train_MAE=0.47, train_loss=0.517, val_MAE=0.625, val_loss=0.671]Epoch 47:   5%|▍         | 47/1000 [40:35<12:39:19, 47.81s/it, lr=1.56e-5, test_MAE=0.676, time=47.6, train_MAE=0.476, train_loss=0.523, val_MAE=0.627, val_loss=0.674]Epoch 47:   5%|▍         | 48/1000 [40:35<12:37:38, 47.75s/it, lr=1.56e-5, test_MAE=0.676, time=47.6, train_MAE=0.476, train_loss=0.523, val_MAE=0.627, val_loss=0.674]Epoch 48:   5%|▍         | 48/1000 [40:35<12:37:38, 47.75s/it, lr=1.56e-5, test_MAE=0.676, time=47.6, train_MAE=0.476, train_loss=0.523, val_MAE=0.627, val_loss=0.674]Epoch 48:   5%|▍         | 48/1000 [41:23<12:37:38, 47.75s/it, lr=1.56e-5, test_MAE=0.673, time=47.3, train_MAE=0.47, train_loss=0.516, val_MAE=0.625, val_loss=0.672] Epoch 48:   5%|▍         | 49/1000 [41:23<12:34:58, 47.63s/it, lr=1.56e-5, test_MAE=0.673, time=47.3, train_MAE=0.47, train_loss=0.516, val_MAE=0.625, val_loss=0.672]Epoch 49:   5%|▍         | 49/1000 [41:23<12:34:58, 47.63s/it, lr=1.56e-5, test_MAE=0.673, time=47.3, train_MAE=0.47, train_loss=0.516, val_MAE=0.625, val_loss=0.672]Epoch 49:   5%|▍         | 49/1000 [42:11<12:34:58, 47.63s/it, lr=1.56e-5, test_MAE=0.675, time=48.3, train_MAE=0.472, train_loss=0.519, val_MAE=0.628, val_loss=0.675]Epoch    50: reducing learning rate of group 0 to 7.8125e-06.

!! LR EQUAL TO MIN LR SET.
Epoch 49:   5%|▍         | 49/1000 [42:11<13:38:48, 51.66s/it, lr=1.56e-5, test_MAE=0.675, time=48.3, train_MAE=0.472, train_loss=0.519, val_MAE=0.628, val_loss=0.675]
Test MAE: 0.6754
Train MAE: 0.4460
Convergence Time (Epochs): 49.0000
TOTAL TIME TAKEN: 2564.2493s
AVG TIME PER EPOCH: 50.6165s

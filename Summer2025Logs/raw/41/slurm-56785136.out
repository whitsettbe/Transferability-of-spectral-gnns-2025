I'm echoing to stdout
I'm echoing to stderr
My JobID is 56785136
I have 8 CPUs on node r108u25n01
Using backend: pytorch
cuda not available
[I] Loading dataset ZINC...
train, test, val sizes : 10000 1000 1000
[I] Finished loading.
[I] Data load time: 5.0474s
Dataset: ZINC,
Model: EigvalFilters

params={'seed': 41, 'epochs': 1000, 'batch_size': 128, 'init_lr': 0.001, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 5, 'min_lr': 1e-05, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 48}

net_params={'L': 4, 'hidden_dim': 106, 'out_dim': 106, 'residual': True, 'readout': 'mean', 'k': 2, 'in_feat_dropout': 0.0, 'dropout': 0.0, 'graph_norm': True, 'batch_norm': True, 'self_loop': False, 'subtype': 'parallel_dense_simp', 'normalized_laplacian': False, 'post_normalized': False, 'eigval_norm': 'scale(0,2)_all', 'num_eigs': 64, 'bias_mode': '', 'eigval_hidden_dim': 5, 'eigval_num_hidden_layer': 3, 'l1_reg': 0.0, 'l2_reg': 0.0, 'gen_reg': 5.0, 'eigmod': 'import_csv', 'eigInFiles': {'train': 'supp_data/molecules/zinc_train_rec_full.csv', 'test': 'supp_data/molecules/zinc_test_rec_full.csv', 'val': 'supp_data/molecules/zinc_val_rec_full.csv'}, 'fixMissingPhi1': False, 'extraOrtho': True, 'device': device(type='cpu'), 'gpu_id': 0, 'batch_size': 128, 'biases': False, 'num_atom_type': 28, 'num_bond_type': 4, 'total_param': -1}


Total Parameters: -1


Training Graphs:  10000
Validation Graphs:  1000
Test Graphs:  1000
  0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [03:00<?, ?it/s, lr=0.001, test_MAE=1.3, time=181, train_MAE=0.958, train_loss=1.02, val_MAE=1.23, val_loss=1.28]Epoch 0:   0%|          | 1/1000 [03:00<50:11:34, 180.88s/it, lr=0.001, test_MAE=1.3, time=181, train_MAE=0.958, train_loss=1.02, val_MAE=1.23, val_loss=1.28]Epoch 1:   0%|          | 1/1000 [03:00<50:11:34, 180.88s/it, lr=0.001, test_MAE=1.3, time=181, train_MAE=0.958, train_loss=1.02, val_MAE=1.23, val_loss=1.28]Epoch 1:   0%|          | 1/1000 [03:37<50:11:34, 180.88s/it, lr=0.001, test_MAE=0.875, time=37.1, train_MAE=0.685, train_loss=0.739, val_MAE=0.852, val_loss=0.898]Epoch 1:   0%|          | 2/1000 [03:37<38:11:02, 137.74s/it, lr=0.001, test_MAE=0.875, time=37.1, train_MAE=0.685, train_loss=0.739, val_MAE=0.852, val_loss=0.898]Epoch 2:   0%|          | 2/1000 [03:37<38:11:02, 137.74s/it, lr=0.001, test_MAE=0.875, time=37.1, train_MAE=0.685, train_loss=0.739, val_MAE=0.852, val_loss=0.898]Epoch 2:   0%|          | 2/1000 [04:15<38:11:02, 137.74s/it, lr=0.001, test_MAE=0.706, time=37.8, train_MAE=0.665, train_loss=0.71, val_MAE=0.654, val_loss=0.695] Epoch 2:   0%|          | 3/1000 [04:15<29:50:23, 107.75s/it, lr=0.001, test_MAE=0.706, time=37.8, train_MAE=0.665, train_loss=0.71, val_MAE=0.654, val_loss=0.695]Epoch 3:   0%|          | 3/1000 [04:15<29:50:23, 107.75s/it, lr=0.001, test_MAE=0.706, time=37.8, train_MAE=0.665, train_loss=0.71, val_MAE=0.654, val_loss=0.695]Epoch 3:   0%|          | 3/1000 [04:53<29:50:23, 107.75s/it, lr=0.001, test_MAE=0.714, time=37.7, train_MAE=0.647, train_loss=0.688, val_MAE=0.663, val_loss=0.704]Epoch 3:   0%|          | 4/1000 [04:53<23:59:44, 86.73s/it, lr=0.001, test_MAE=0.714, time=37.7, train_MAE=0.647, train_loss=0.688, val_MAE=0.663, val_loss=0.704] Epoch 4:   0%|          | 4/1000 [04:53<23:59:44, 86.73s/it, lr=0.001, test_MAE=0.714, time=37.7, train_MAE=0.647, train_loss=0.688, val_MAE=0.663, val_loss=0.704]Epoch 4:   0%|          | 4/1000 [05:30<23:59:44, 86.73s/it, lr=0.001, test_MAE=0.73, time=37.5, train_MAE=0.64, train_loss=0.681, val_MAE=0.695, val_loss=0.735]  Epoch 4:   0%|          | 5/1000 [05:30<19:53:15, 71.95s/it, lr=0.001, test_MAE=0.73, time=37.5, train_MAE=0.64, train_loss=0.681, val_MAE=0.695, val_loss=0.735]Epoch 5:   0%|          | 5/1000 [05:30<19:53:15, 71.95s/it, lr=0.001, test_MAE=0.73, time=37.5, train_MAE=0.64, train_loss=0.681, val_MAE=0.695, val_loss=0.735]Epoch 5:   0%|          | 5/1000 [06:08<19:53:15, 71.95s/it, lr=0.001, test_MAE=0.693, time=37.1, train_MAE=0.64, train_loss=0.681, val_MAE=0.658, val_loss=0.699]Epoch 5:   1%|          | 6/1000 [06:08<16:59:03, 61.51s/it, lr=0.001, test_MAE=0.693, time=37.1, train_MAE=0.64, train_loss=0.681, val_MAE=0.658, val_loss=0.699]Epoch 6:   1%|          | 6/1000 [06:08<16:59:03, 61.51s/it, lr=0.001, test_MAE=0.693, time=37.1, train_MAE=0.64, train_loss=0.681, val_MAE=0.658, val_loss=0.699]Epoch 6:   1%|          | 6/1000 [06:46<16:59:03, 61.51s/it, lr=0.001, test_MAE=0.735, time=38.1, train_MAE=0.631, train_loss=0.674, val_MAE=0.695, val_loss=0.737]Epoch 6:   1%|          | 7/1000 [06:46<15:01:36, 54.48s/it, lr=0.001, test_MAE=0.735, time=38.1, train_MAE=0.631, train_loss=0.674, val_MAE=0.695, val_loss=0.737]Epoch 7:   1%|          | 7/1000 [06:46<15:01:36, 54.48s/it, lr=0.001, test_MAE=0.735, time=38.1, train_MAE=0.631, train_loss=0.674, val_MAE=0.695, val_loss=0.737]Epoch 7:   1%|          | 7/1000 [07:23<15:01:36, 54.48s/it, lr=0.001, test_MAE=0.717, time=37.5, train_MAE=0.625, train_loss=0.669, val_MAE=0.679, val_loss=0.723]Epoch 7:   1%|          | 8/1000 [07:23<13:36:28, 49.38s/it, lr=0.001, test_MAE=0.717, time=37.5, train_MAE=0.625, train_loss=0.669, val_MAE=0.679, val_loss=0.723]Epoch 8:   1%|          | 8/1000 [07:23<13:36:28, 49.38s/it, lr=0.001, test_MAE=0.717, time=37.5, train_MAE=0.625, train_loss=0.669, val_MAE=0.679, val_loss=0.723]Epoch 8:   1%|          | 8/1000 [08:01<13:36:28, 49.38s/it, lr=0.001, test_MAE=0.707, time=37.6, train_MAE=0.623, train_loss=0.669, val_MAE=0.669, val_loss=0.712]Epoch     9: reducing learning rate of group 0 to 5.0000e-04.
Epoch 8:   1%|          | 9/1000 [08:01<12:37:26, 45.86s/it, lr=0.001, test_MAE=0.707, time=37.6, train_MAE=0.623, train_loss=0.669, val_MAE=0.669, val_loss=0.712]Epoch 9:   1%|          | 9/1000 [08:01<12:37:26, 45.86s/it, lr=0.001, test_MAE=0.707, time=37.6, train_MAE=0.623, train_loss=0.669, val_MAE=0.669, val_loss=0.712]Epoch 9:   1%|          | 9/1000 [08:39<12:37:26, 45.86s/it, lr=0.0005, test_MAE=0.698, time=37.8, train_MAE=0.595, train_loss=0.634, val_MAE=0.657, val_loss=0.695]Epoch 9:   1%|          | 10/1000 [08:39<11:56:49, 43.44s/it, lr=0.0005, test_MAE=0.698, time=37.8, train_MAE=0.595, train_loss=0.634, val_MAE=0.657, val_loss=0.695]Epoch 10:   1%|          | 10/1000 [08:39<11:56:49, 43.44s/it, lr=0.0005, test_MAE=0.698, time=37.8, train_MAE=0.595, train_loss=0.634, val_MAE=0.657, val_loss=0.695]Epoch 10:   1%|          | 10/1000 [09:17<11:56:49, 43.44s/it, lr=0.0005, test_MAE=0.677, time=38, train_MAE=0.599, train_loss=0.638, val_MAE=0.636, val_loss=0.676]  Epoch 10:   1%|          | 11/1000 [09:17<11:29:14, 41.81s/it, lr=0.0005, test_MAE=0.677, time=38, train_MAE=0.599, train_loss=0.638, val_MAE=0.636, val_loss=0.676]Epoch 11:   1%|          | 11/1000 [09:17<11:29:14, 41.81s/it, lr=0.0005, test_MAE=0.677, time=38, train_MAE=0.599, train_loss=0.638, val_MAE=0.636, val_loss=0.676]Epoch 11:   1%|          | 11/1000 [09:54<11:29:14, 41.81s/it, lr=0.0005, test_MAE=0.702, time=37.8, train_MAE=0.594, train_loss=0.634, val_MAE=0.668, val_loss=0.709]Epoch 11:   1%|          | 12/1000 [09:54<11:08:32, 40.60s/it, lr=0.0005, test_MAE=0.702, time=37.8, train_MAE=0.594, train_loss=0.634, val_MAE=0.668, val_loss=0.709]Epoch 12:   1%|          | 12/1000 [09:54<11:08:32, 40.60s/it, lr=0.0005, test_MAE=0.702, time=37.8, train_MAE=0.594, train_loss=0.634, val_MAE=0.668, val_loss=0.709]Epoch 12:   1%|          | 12/1000 [10:32<11:08:32, 40.60s/it, lr=0.0005, test_MAE=0.679, time=37.5, train_MAE=0.584, train_loss=0.626, val_MAE=0.639, val_loss=0.681]Epoch 12:   1%|▏         | 13/1000 [10:32<10:52:32, 39.67s/it, lr=0.0005, test_MAE=0.679, time=37.5, train_MAE=0.584, train_loss=0.626, val_MAE=0.639, val_loss=0.681]Epoch 13:   1%|▏         | 13/1000 [10:32<10:52:32, 39.67s/it, lr=0.0005, test_MAE=0.679, time=37.5, train_MAE=0.584, train_loss=0.626, val_MAE=0.639, val_loss=0.681]Epoch 13:   1%|▏         | 13/1000 [11:10<10:52:32, 39.67s/it, lr=0.0005, test_MAE=0.734, time=38.3, train_MAE=0.583, train_loss=0.625, val_MAE=0.694, val_loss=0.736]Epoch 13:   1%|▏         | 14/1000 [11:10<10:45:02, 39.25s/it, lr=0.0005, test_MAE=0.734, time=38.3, train_MAE=0.583, train_loss=0.625, val_MAE=0.694, val_loss=0.736]Epoch 14:   1%|▏         | 14/1000 [11:10<10:45:02, 39.25s/it, lr=0.0005, test_MAE=0.734, time=38.3, train_MAE=0.583, train_loss=0.625, val_MAE=0.694, val_loss=0.736]Epoch 14:   1%|▏         | 14/1000 [11:48<10:45:02, 39.25s/it, lr=0.0005, test_MAE=0.724, time=37.8, train_MAE=0.578, train_loss=0.621, val_MAE=0.677, val_loss=0.721]Epoch 14:   2%|▏         | 15/1000 [11:48<10:37:14, 38.82s/it, lr=0.0005, test_MAE=0.724, time=37.8, train_MAE=0.578, train_loss=0.621, val_MAE=0.677, val_loss=0.721]Epoch 15:   2%|▏         | 15/1000 [11:48<10:37:14, 38.82s/it, lr=0.0005, test_MAE=0.724, time=37.8, train_MAE=0.578, train_loss=0.621, val_MAE=0.677, val_loss=0.721]Epoch 15:   2%|▏         | 15/1000 [12:26<10:37:14, 38.82s/it, lr=0.0005, test_MAE=0.696, time=37.7, train_MAE=0.578, train_loss=0.622, val_MAE=0.656, val_loss=0.701]Epoch 15:   2%|▏         | 16/1000 [12:26<10:31:04, 38.48s/it, lr=0.0005, test_MAE=0.696, time=37.7, train_MAE=0.578, train_loss=0.622, val_MAE=0.656, val_loss=0.701]Epoch 16:   2%|▏         | 16/1000 [12:26<10:31:04, 38.48s/it, lr=0.0005, test_MAE=0.696, time=37.7, train_MAE=0.578, train_loss=0.622, val_MAE=0.656, val_loss=0.701]Epoch 16:   2%|▏         | 16/1000 [13:03<10:31:04, 38.48s/it, lr=0.0005, test_MAE=0.686, time=37.8, train_MAE=0.569, train_loss=0.615, val_MAE=0.645, val_loss=0.691]Epoch    17: reducing learning rate of group 0 to 2.5000e-04.
Epoch 16:   2%|▏         | 17/1000 [13:03<10:27:15, 38.29s/it, lr=0.0005, test_MAE=0.686, time=37.8, train_MAE=0.569, train_loss=0.615, val_MAE=0.645, val_loss=0.691]Epoch 17:   2%|▏         | 17/1000 [13:03<10:27:15, 38.29s/it, lr=0.0005, test_MAE=0.686, time=37.8, train_MAE=0.569, train_loss=0.615, val_MAE=0.645, val_loss=0.691]Epoch 17:   2%|▏         | 17/1000 [13:42<10:27:15, 38.29s/it, lr=0.00025, test_MAE=0.693, time=38.1, train_MAE=0.557, train_loss=0.601, val_MAE=0.659, val_loss=0.701]Epoch 17:   2%|▏         | 18/1000 [13:42<10:25:41, 38.23s/it, lr=0.00025, test_MAE=0.693, time=38.1, train_MAE=0.557, train_loss=0.601, val_MAE=0.659, val_loss=0.701]Epoch 18:   2%|▏         | 18/1000 [13:42<10:25:41, 38.23s/it, lr=0.00025, test_MAE=0.693, time=38.1, train_MAE=0.557, train_loss=0.601, val_MAE=0.659, val_loss=0.701]Epoch 18:   2%|▏         | 18/1000 [14:19<10:25:41, 38.23s/it, lr=0.00025, test_MAE=0.697, time=37.7, train_MAE=0.544, train_loss=0.587, val_MAE=0.662, val_loss=0.705]Epoch 18:   2%|▏         | 19/1000 [14:19<10:22:24, 38.07s/it, lr=0.00025, test_MAE=0.697, time=37.7, train_MAE=0.544, train_loss=0.587, val_MAE=0.662, val_loss=0.705]Epoch 19:   2%|▏         | 19/1000 [14:19<10:22:24, 38.07s/it, lr=0.00025, test_MAE=0.697, time=37.7, train_MAE=0.544, train_loss=0.587, val_MAE=0.662, val_loss=0.705]Epoch 19:   2%|▏         | 19/1000 [14:57<10:22:24, 38.07s/it, lr=0.00025, test_MAE=0.694, time=37.8, train_MAE=0.537, train_loss=0.58, val_MAE=0.661, val_loss=0.705] Epoch 19:   2%|▏         | 20/1000 [14:57<10:20:21, 37.98s/it, lr=0.00025, test_MAE=0.694, time=37.8, train_MAE=0.537, train_loss=0.58, val_MAE=0.661, val_loss=0.705]Epoch 20:   2%|▏         | 20/1000 [14:57<10:20:21, 37.98s/it, lr=0.00025, test_MAE=0.694, time=37.8, train_MAE=0.537, train_loss=0.58, val_MAE=0.661, val_loss=0.705]Epoch 20:   2%|▏         | 20/1000 [15:35<10:20:21, 37.98s/it, lr=0.00025, test_MAE=0.697, time=38, train_MAE=0.539, train_loss=0.583, val_MAE=0.659, val_loss=0.704] Epoch 20:   2%|▏         | 21/1000 [15:35<10:19:49, 37.99s/it, lr=0.00025, test_MAE=0.697, time=38, train_MAE=0.539, train_loss=0.583, val_MAE=0.659, val_loss=0.704]Epoch 21:   2%|▏         | 21/1000 [15:35<10:19:49, 37.99s/it, lr=0.00025, test_MAE=0.697, time=38, train_MAE=0.539, train_loss=0.583, val_MAE=0.659, val_loss=0.704]Epoch 21:   2%|▏         | 21/1000 [16:13<10:19:49, 37.99s/it, lr=0.00025, test_MAE=0.711, time=37.8, train_MAE=0.535, train_loss=0.579, val_MAE=0.676, val_loss=0.721]Epoch 21:   2%|▏         | 22/1000 [16:13<10:18:12, 37.93s/it, lr=0.00025, test_MAE=0.711, time=37.8, train_MAE=0.535, train_loss=0.579, val_MAE=0.676, val_loss=0.721]Epoch 22:   2%|▏         | 22/1000 [16:13<10:18:12, 37.93s/it, lr=0.00025, test_MAE=0.711, time=37.8, train_MAE=0.535, train_loss=0.579, val_MAE=0.676, val_loss=0.721]Epoch 22:   2%|▏         | 22/1000 [16:51<10:18:12, 37.93s/it, lr=0.00025, test_MAE=0.905, time=37.7, train_MAE=0.532, train_loss=0.577, val_MAE=0.873, val_loss=0.919]Epoch    23: reducing learning rate of group 0 to 1.2500e-04.
Epoch 22:   2%|▏         | 23/1000 [16:51<10:16:45, 37.88s/it, lr=0.00025, test_MAE=0.905, time=37.7, train_MAE=0.532, train_loss=0.577, val_MAE=0.873, val_loss=0.919]Epoch 23:   2%|▏         | 23/1000 [16:51<10:16:45, 37.88s/it, lr=0.00025, test_MAE=0.905, time=37.7, train_MAE=0.532, train_loss=0.577, val_MAE=0.873, val_loss=0.919]Epoch 23:   2%|▏         | 23/1000 [17:28<10:16:45, 37.88s/it, lr=0.000125, test_MAE=0.717, time=37.7, train_MAE=0.512, train_loss=0.556, val_MAE=0.675, val_loss=0.718]Epoch 23:   2%|▏         | 24/1000 [17:28<10:15:30, 37.84s/it, lr=0.000125, test_MAE=0.717, time=37.7, train_MAE=0.512, train_loss=0.556, val_MAE=0.675, val_loss=0.718]Epoch 24:   2%|▏         | 24/1000 [17:28<10:15:30, 37.84s/it, lr=0.000125, test_MAE=0.717, time=37.7, train_MAE=0.512, train_loss=0.556, val_MAE=0.675, val_loss=0.718]Epoch 24:   2%|▏         | 24/1000 [18:06<10:15:30, 37.84s/it, lr=0.000125, test_MAE=0.707, time=38.1, train_MAE=0.508, train_loss=0.552, val_MAE=0.661, val_loss=0.705]Epoch 24:   2%|▎         | 25/1000 [18:06<10:16:12, 37.92s/it, lr=0.000125, test_MAE=0.707, time=38.1, train_MAE=0.508, train_loss=0.552, val_MAE=0.661, val_loss=0.705]Epoch 25:   2%|▎         | 25/1000 [18:06<10:16:12, 37.92s/it, lr=0.000125, test_MAE=0.707, time=38.1, train_MAE=0.508, train_loss=0.552, val_MAE=0.661, val_loss=0.705]Epoch 25:   2%|▎         | 25/1000 [18:44<10:16:12, 37.92s/it, lr=0.000125, test_MAE=0.703, time=37.4, train_MAE=0.496, train_loss=0.539, val_MAE=0.661, val_loss=0.704]Epoch 25:   3%|▎         | 26/1000 [18:44<10:12:51, 37.75s/it, lr=0.000125, test_MAE=0.703, time=37.4, train_MAE=0.496, train_loss=0.539, val_MAE=0.661, val_loss=0.704]Epoch 26:   3%|▎         | 26/1000 [18:44<10:12:51, 37.75s/it, lr=0.000125, test_MAE=0.703, time=37.4, train_MAE=0.496, train_loss=0.539, val_MAE=0.661, val_loss=0.704]Epoch 26:   3%|▎         | 26/1000 [19:22<10:12:51, 37.75s/it, lr=0.000125, test_MAE=0.714, time=38.3, train_MAE=0.491, train_loss=0.535, val_MAE=0.683, val_loss=0.727]Epoch 26:   3%|▎         | 27/1000 [19:22<10:14:47, 37.91s/it, lr=0.000125, test_MAE=0.714, time=38.3, train_MAE=0.491, train_loss=0.535, val_MAE=0.683, val_loss=0.727]Epoch 27:   3%|▎         | 27/1000 [19:22<10:14:47, 37.91s/it, lr=0.000125, test_MAE=0.714, time=38.3, train_MAE=0.491, train_loss=0.535, val_MAE=0.683, val_loss=0.727]Epoch 27:   3%|▎         | 27/1000 [20:00<10:14:47, 37.91s/it, lr=0.000125, test_MAE=0.704, time=37.9, train_MAE=0.489, train_loss=0.533, val_MAE=0.669, val_loss=0.713]Epoch 27:   3%|▎         | 28/1000 [20:00<10:14:06, 37.91s/it, lr=0.000125, test_MAE=0.704, time=37.9, train_MAE=0.489, train_loss=0.533, val_MAE=0.669, val_loss=0.713]Epoch 28:   3%|▎         | 28/1000 [20:00<10:14:06, 37.91s/it, lr=0.000125, test_MAE=0.704, time=37.9, train_MAE=0.489, train_loss=0.533, val_MAE=0.669, val_loss=0.713]Epoch 28:   3%|▎         | 28/1000 [20:38<10:14:06, 37.91s/it, lr=0.000125, test_MAE=0.713, time=37.7, train_MAE=0.493, train_loss=0.537, val_MAE=0.682, val_loss=0.727]Epoch    29: reducing learning rate of group 0 to 6.2500e-05.
Epoch 28:   3%|▎         | 29/1000 [20:38<10:12:19, 37.84s/it, lr=0.000125, test_MAE=0.713, time=37.7, train_MAE=0.493, train_loss=0.537, val_MAE=0.682, val_loss=0.727]Epoch 29:   3%|▎         | 29/1000 [20:38<10:12:19, 37.84s/it, lr=0.000125, test_MAE=0.713, time=37.7, train_MAE=0.493, train_loss=0.537, val_MAE=0.682, val_loss=0.727]Epoch 29:   3%|▎         | 29/1000 [21:15<10:12:19, 37.84s/it, lr=6.25e-5, test_MAE=0.705, time=37.3, train_MAE=0.479, train_loss=0.524, val_MAE=0.663, val_loss=0.707] Epoch 29:   3%|▎         | 30/1000 [21:15<10:09:18, 37.69s/it, lr=6.25e-5, test_MAE=0.705, time=37.3, train_MAE=0.479, train_loss=0.524, val_MAE=0.663, val_loss=0.707]Epoch 30:   3%|▎         | 30/1000 [21:15<10:09:18, 37.69s/it, lr=6.25e-5, test_MAE=0.705, time=37.3, train_MAE=0.479, train_loss=0.524, val_MAE=0.663, val_loss=0.707]Epoch 30:   3%|▎         | 30/1000 [21:53<10:09:18, 37.69s/it, lr=6.25e-5, test_MAE=0.713, time=38.3, train_MAE=0.467, train_loss=0.51, val_MAE=0.672, val_loss=0.716] Epoch 30:   3%|▎         | 31/1000 [21:53<10:11:29, 37.86s/it, lr=6.25e-5, test_MAE=0.713, time=38.3, train_MAE=0.467, train_loss=0.51, val_MAE=0.672, val_loss=0.716]Epoch 31:   3%|▎         | 31/1000 [21:53<10:11:29, 37.86s/it, lr=6.25e-5, test_MAE=0.713, time=38.3, train_MAE=0.467, train_loss=0.51, val_MAE=0.672, val_loss=0.716]Epoch 31:   3%|▎         | 31/1000 [22:31<10:11:29, 37.86s/it, lr=6.25e-5, test_MAE=0.715, time=37.6, train_MAE=0.475, train_loss=0.519, val_MAE=0.673, val_loss=0.717]Epoch 31:   3%|▎         | 32/1000 [22:31<10:09:36, 37.79s/it, lr=6.25e-5, test_MAE=0.715, time=37.6, train_MAE=0.475, train_loss=0.519, val_MAE=0.673, val_loss=0.717]Epoch 32:   3%|▎         | 32/1000 [22:31<10:09:36, 37.79s/it, lr=6.25e-5, test_MAE=0.715, time=37.6, train_MAE=0.475, train_loss=0.519, val_MAE=0.673, val_loss=0.717]Epoch 32:   3%|▎         | 32/1000 [23:08<10:09:36, 37.79s/it, lr=6.25e-5, test_MAE=0.718, time=37.7, train_MAE=0.475, train_loss=0.518, val_MAE=0.68, val_loss=0.723] Epoch 32:   3%|▎         | 33/1000 [23:09<10:08:23, 37.75s/it, lr=6.25e-5, test_MAE=0.718, time=37.7, train_MAE=0.475, train_loss=0.518, val_MAE=0.68, val_loss=0.723]Epoch 33:   3%|▎         | 33/1000 [23:09<10:08:23, 37.75s/it, lr=6.25e-5, test_MAE=0.718, time=37.7, train_MAE=0.475, train_loss=0.518, val_MAE=0.68, val_loss=0.723]Epoch 33:   3%|▎         | 33/1000 [23:46<10:08:23, 37.75s/it, lr=6.25e-5, test_MAE=0.718, time=37.7, train_MAE=0.463, train_loss=0.507, val_MAE=0.684, val_loss=0.727]Epoch 33:   3%|▎         | 34/1000 [23:46<10:07:36, 37.74s/it, lr=6.25e-5, test_MAE=0.718, time=37.7, train_MAE=0.463, train_loss=0.507, val_MAE=0.684, val_loss=0.727]Epoch 34:   3%|▎         | 34/1000 [23:46<10:07:36, 37.74s/it, lr=6.25e-5, test_MAE=0.718, time=37.7, train_MAE=0.463, train_loss=0.507, val_MAE=0.684, val_loss=0.727]Epoch 34:   3%|▎         | 34/1000 [24:24<10:07:36, 37.74s/it, lr=6.25e-5, test_MAE=0.744, time=37.9, train_MAE=0.461, train_loss=0.505, val_MAE=0.703, val_loss=0.747]Epoch    35: reducing learning rate of group 0 to 3.1250e-05.
Epoch 34:   4%|▎         | 35/1000 [24:24<10:08:01, 37.80s/it, lr=6.25e-5, test_MAE=0.744, time=37.9, train_MAE=0.461, train_loss=0.505, val_MAE=0.703, val_loss=0.747]Epoch 35:   4%|▎         | 35/1000 [24:24<10:08:01, 37.80s/it, lr=6.25e-5, test_MAE=0.744, time=37.9, train_MAE=0.461, train_loss=0.505, val_MAE=0.703, val_loss=0.747]Epoch 35:   4%|▎         | 35/1000 [25:02<10:08:01, 37.80s/it, lr=3.13e-5, test_MAE=0.729, time=37.6, train_MAE=0.453, train_loss=0.496, val_MAE=0.689, val_loss=0.732]Epoch 35:   4%|▎         | 36/1000 [25:02<10:06:23, 37.74s/it, lr=3.13e-5, test_MAE=0.729, time=37.6, train_MAE=0.453, train_loss=0.496, val_MAE=0.689, val_loss=0.732]Epoch 36:   4%|▎         | 36/1000 [25:02<10:06:23, 37.74s/it, lr=3.13e-5, test_MAE=0.729, time=37.6, train_MAE=0.453, train_loss=0.496, val_MAE=0.689, val_loss=0.732]Epoch 36:   4%|▎         | 36/1000 [25:39<10:06:23, 37.74s/it, lr=3.13e-5, test_MAE=0.722, time=37.7, train_MAE=0.452, train_loss=0.495, val_MAE=0.681, val_loss=0.724]Epoch 36:   4%|▎         | 37/1000 [25:39<10:05:38, 37.73s/it, lr=3.13e-5, test_MAE=0.722, time=37.7, train_MAE=0.452, train_loss=0.495, val_MAE=0.681, val_loss=0.724]Epoch 37:   4%|▎         | 37/1000 [25:39<10:05:38, 37.73s/it, lr=3.13e-5, test_MAE=0.722, time=37.7, train_MAE=0.452, train_loss=0.495, val_MAE=0.681, val_loss=0.724]Epoch 37:   4%|▎         | 37/1000 [26:17<10:05:38, 37.73s/it, lr=3.13e-5, test_MAE=0.717, time=37.9, train_MAE=0.45, train_loss=0.493, val_MAE=0.677, val_loss=0.72]  Epoch 37:   4%|▍         | 38/1000 [26:17<10:05:58, 37.80s/it, lr=3.13e-5, test_MAE=0.717, time=37.9, train_MAE=0.45, train_loss=0.493, val_MAE=0.677, val_loss=0.72]Epoch 38:   4%|▍         | 38/1000 [26:17<10:05:58, 37.80s/it, lr=3.13e-5, test_MAE=0.717, time=37.9, train_MAE=0.45, train_loss=0.493, val_MAE=0.677, val_loss=0.72]Epoch 38:   4%|▍         | 38/1000 [26:55<10:05:58, 37.80s/it, lr=3.13e-5, test_MAE=0.729, time=37.6, train_MAE=0.453, train_loss=0.496, val_MAE=0.687, val_loss=0.73]Epoch 38:   4%|▍         | 39/1000 [26:55<10:04:35, 37.75s/it, lr=3.13e-5, test_MAE=0.729, time=37.6, train_MAE=0.453, train_loss=0.496, val_MAE=0.687, val_loss=0.73]Epoch 39:   4%|▍         | 39/1000 [26:55<10:04:35, 37.75s/it, lr=3.13e-5, test_MAE=0.729, time=37.6, train_MAE=0.453, train_loss=0.496, val_MAE=0.687, val_loss=0.73]Epoch 39:   4%|▍         | 39/1000 [27:33<10:04:35, 37.75s/it, lr=3.13e-5, test_MAE=0.72, time=37.6, train_MAE=0.45, train_loss=0.493, val_MAE=0.678, val_loss=0.721] Epoch 39:   4%|▍         | 40/1000 [27:33<10:03:32, 37.72s/it, lr=3.13e-5, test_MAE=0.72, time=37.6, train_MAE=0.45, train_loss=0.493, val_MAE=0.678, val_loss=0.721]Epoch 40:   4%|▍         | 40/1000 [27:33<10:03:32, 37.72s/it, lr=3.13e-5, test_MAE=0.72, time=37.6, train_MAE=0.45, train_loss=0.493, val_MAE=0.678, val_loss=0.721]Epoch 40:   4%|▍         | 40/1000 [28:10<10:03:32, 37.72s/it, lr=3.13e-5, test_MAE=0.717, time=37.7, train_MAE=0.453, train_loss=0.496, val_MAE=0.679, val_loss=0.722]Epoch    41: reducing learning rate of group 0 to 1.5625e-05.
Epoch 40:   4%|▍         | 41/1000 [28:10<10:02:51, 37.72s/it, lr=3.13e-5, test_MAE=0.717, time=37.7, train_MAE=0.453, train_loss=0.496, val_MAE=0.679, val_loss=0.722]Epoch 41:   4%|▍         | 41/1000 [28:10<10:02:51, 37.72s/it, lr=3.13e-5, test_MAE=0.717, time=37.7, train_MAE=0.453, train_loss=0.496, val_MAE=0.679, val_loss=0.722]Epoch 41:   4%|▍         | 41/1000 [28:48<10:02:51, 37.72s/it, lr=1.56e-5, test_MAE=0.723, time=38, train_MAE=0.438, train_loss=0.481, val_MAE=0.684, val_loss=0.727]  Epoch 41:   4%|▍         | 42/1000 [28:48<10:03:38, 37.81s/it, lr=1.56e-5, test_MAE=0.723, time=38, train_MAE=0.438, train_loss=0.481, val_MAE=0.684, val_loss=0.727]Epoch 42:   4%|▍         | 42/1000 [28:48<10:03:38, 37.81s/it, lr=1.56e-5, test_MAE=0.723, time=38, train_MAE=0.438, train_loss=0.481, val_MAE=0.684, val_loss=0.727]Epoch 42:   4%|▍         | 42/1000 [29:26<10:03:38, 37.81s/it, lr=1.56e-5, test_MAE=0.722, time=37.6, train_MAE=0.45, train_loss=0.493, val_MAE=0.681, val_loss=0.724]Epoch 42:   4%|▍         | 43/1000 [29:26<10:02:15, 37.76s/it, lr=1.56e-5, test_MAE=0.722, time=37.6, train_MAE=0.45, train_loss=0.493, val_MAE=0.681, val_loss=0.724]Epoch 43:   4%|▍         | 43/1000 [29:26<10:02:15, 37.76s/it, lr=1.56e-5, test_MAE=0.722, time=37.6, train_MAE=0.45, train_loss=0.493, val_MAE=0.681, val_loss=0.724]Epoch 43:   4%|▍         | 43/1000 [30:04<10:02:15, 37.76s/it, lr=1.56e-5, test_MAE=0.721, time=37.7, train_MAE=0.441, train_loss=0.484, val_MAE=0.68, val_loss=0.723]Epoch 43:   4%|▍         | 44/1000 [30:04<10:01:10, 37.73s/it, lr=1.56e-5, test_MAE=0.721, time=37.7, train_MAE=0.441, train_loss=0.484, val_MAE=0.68, val_loss=0.723]Epoch 44:   4%|▍         | 44/1000 [30:04<10:01:10, 37.73s/it, lr=1.56e-5, test_MAE=0.721, time=37.7, train_MAE=0.441, train_loss=0.484, val_MAE=0.68, val_loss=0.723]Epoch 44:   4%|▍         | 44/1000 [30:42<10:01:10, 37.73s/it, lr=1.56e-5, test_MAE=0.731, time=38, train_MAE=0.438, train_loss=0.481, val_MAE=0.69, val_loss=0.733]  Epoch 44:   4%|▍         | 45/1000 [30:42<10:01:41, 37.80s/it, lr=1.56e-5, test_MAE=0.731, time=38, train_MAE=0.438, train_loss=0.481, val_MAE=0.69, val_loss=0.733]Epoch 45:   4%|▍         | 45/1000 [30:42<10:01:41, 37.80s/it, lr=1.56e-5, test_MAE=0.731, time=38, train_MAE=0.438, train_loss=0.481, val_MAE=0.69, val_loss=0.733]Epoch 45:   4%|▍         | 45/1000 [31:19<10:01:41, 37.80s/it, lr=1.56e-5, test_MAE=0.722, time=37.6, train_MAE=0.433, train_loss=0.476, val_MAE=0.683, val_loss=0.726]Epoch 45:   5%|▍         | 46/1000 [31:19<10:00:16, 37.75s/it, lr=1.56e-5, test_MAE=0.722, time=37.6, train_MAE=0.433, train_loss=0.476, val_MAE=0.683, val_loss=0.726]Epoch 46:   5%|▍         | 46/1000 [31:19<10:00:16, 37.75s/it, lr=1.56e-5, test_MAE=0.722, time=37.6, train_MAE=0.433, train_loss=0.476, val_MAE=0.683, val_loss=0.726]Epoch 46:   5%|▍         | 46/1000 [31:57<10:00:16, 37.75s/it, lr=1.56e-5, test_MAE=0.726, time=37.6, train_MAE=0.433, train_loss=0.476, val_MAE=0.689, val_loss=0.732]Epoch    47: reducing learning rate of group 0 to 7.8125e-06.

!! LR EQUAL TO MIN LR SET.
Epoch 46:   5%|▍         | 46/1000 [31:57<11:02:48, 41.69s/it, lr=1.56e-5, test_MAE=0.726, time=37.6, train_MAE=0.433, train_loss=0.476, val_MAE=0.689, val_loss=0.732]
Test MAE: 0.7261
Train MAE: 0.4077
Convergence Time (Epochs): 46.0000
TOTAL TIME TAKEN: 1941.2163s
AVG TIME PER EPOCH: 40.7884s

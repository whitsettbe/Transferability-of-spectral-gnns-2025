I'm echoing to stdout
I'm echoing to stderr
My JobID is 56855328
I have 4 CPUs on node r108u25n01
Using backend: pytorch
cuda not available
[I] Loading dataset SBM_CLUSTER...
train, test, val sizes : 10000 1000 1000
[I] Finished loading.
[I] Data load time: 7.2751s
Dataset: SBM_CLUSTER,
Model: EigvalFilters

params={'seed': 41, 'epochs': 1000, 'batch_size': 128, 'init_lr': 0.001, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 5, 'min_lr': 1e-05, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 48}

net_params={'L': 4, 'hidden_dim': 70, 'out_dim': 70, 'residual': True, 'k': 5, 'readout': 'mean', 'in_feat_dropout': 0.0, 'dropout': 0.0, 'graph_norm': True, 'batch_norm': True, 'self_loop': False, 'subtype': 'cheb02_vec', 'normalized_laplacian': True, 'post_normalized': False, 'eigval_norm': '', 'num_eigs': 190, 'device': device(type='cpu'), 'gpu_id': 0, 'batch_size': 128, 'biases': False, 'in_dim': 7, 'n_classes': 6, 'total_param': -1}


Total Parameters: -1


Training Graphs:  10000
Validation Graphs:  1000
Test Graphs:  1000
Number of Classes:  6
  0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s]/vast/palmer/pi/krishnaswamy_smita/bsw38/GraphML/Transferability-of-spectral-gnns-2025/Benchmark-gnn/nets/SBMs_node_classification/ChebNet.py:74: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629427478/work/torch/csrc/utils/python_arg_parser.cpp:766.)
  label_count = label_count[label_count.nonzero()].squeeze()
Epoch 0:   0%|          | 0/1000 [03:17<?, ?it/s, lr=0.001, test_acc=37.5, time=197, train_acc=50.9, train_loss=1.27, val_acc=37.4, val_loss=1.6]Epoch 0:   0%|          | 1/1000 [03:17<54:44:21, 197.26s/it, lr=0.001, test_acc=37.5, time=197, train_acc=50.9, train_loss=1.27, val_acc=37.4, val_loss=1.6]Epoch 1:   0%|          | 1/1000 [03:17<54:44:21, 197.26s/it, lr=0.001, test_acc=37.5, time=197, train_acc=50.9, train_loss=1.27, val_acc=37.4, val_loss=1.6]Epoch 1:   0%|          | 1/1000 [05:29<54:44:21, 197.26s/it, lr=0.001, test_acc=27.2, time=132, train_acc=68.4, train_loss=0.867, val_acc=26.9, val_loss=1.77]Epoch 1:   0%|          | 2/1000 [05:29<49:17:57, 177.83s/it, lr=0.001, test_acc=27.2, time=132, train_acc=68.4, train_loss=0.867, val_acc=26.9, val_loss=1.77]Epoch 2:   0%|          | 2/1000 [05:29<49:17:57, 177.83s/it, lr=0.001, test_acc=27.2, time=132, train_acc=68.4, train_loss=0.867, val_acc=26.9, val_loss=1.77]Epoch 2:   0%|          | 2/1000 [07:41<49:17:57, 177.83s/it, lr=0.001, test_acc=34.3, time=132, train_acc=70.1, train_loss=0.818, val_acc=34.2, val_loss=2.57]Epoch 2:   0%|          | 3/1000 [07:41<45:27:02, 164.11s/it, lr=0.001, test_acc=34.3, time=132, train_acc=70.1, train_loss=0.818, val_acc=34.2, val_loss=2.57]Epoch 3:   0%|          | 3/1000 [07:41<45:27:02, 164.11s/it, lr=0.001, test_acc=34.3, time=132, train_acc=70.1, train_loss=0.818, val_acc=34.2, val_loss=2.57]Epoch 3:   0%|          | 3/1000 [09:53<45:27:02, 164.11s/it, lr=0.001, test_acc=17.8, time=132, train_acc=70.8, train_loss=0.797, val_acc=17.9, val_loss=2.03]Epoch 3:   0%|          | 4/1000 [09:53<42:44:12, 154.47s/it, lr=0.001, test_acc=17.8, time=132, train_acc=70.8, train_loss=0.797, val_acc=17.9, val_loss=2.03]Epoch 4:   0%|          | 4/1000 [09:53<42:44:12, 154.47s/it, lr=0.001, test_acc=17.8, time=132, train_acc=70.8, train_loss=0.797, val_acc=17.9, val_loss=2.03]Epoch 4:   0%|          | 4/1000 [12:05<42:44:12, 154.47s/it, lr=0.001, test_acc=39.5, time=132, train_acc=71.4, train_loss=0.78, val_acc=39.1, val_loss=2.21] Epoch 4:   0%|          | 5/1000 [12:05<40:50:05, 147.74s/it, lr=0.001, test_acc=39.5, time=132, train_acc=71.4, train_loss=0.78, val_acc=39.1, val_loss=2.21]Epoch 5:   0%|          | 5/1000 [12:05<40:50:05, 147.74s/it, lr=0.001, test_acc=39.5, time=132, train_acc=71.4, train_loss=0.78, val_acc=39.1, val_loss=2.21]Epoch 5:   0%|          | 5/1000 [14:17<40:50:05, 147.74s/it, lr=0.001, test_acc=37.7, time=132, train_acc=71.7, train_loss=0.771, val_acc=37.7, val_loss=2.03]Epoch 5:   1%|          | 6/1000 [14:17<39:29:53, 143.05s/it, lr=0.001, test_acc=37.7, time=132, train_acc=71.7, train_loss=0.771, val_acc=37.7, val_loss=2.03]Epoch 6:   1%|          | 6/1000 [14:17<39:29:53, 143.05s/it, lr=0.001, test_acc=37.7, time=132, train_acc=71.7, train_loss=0.771, val_acc=37.7, val_loss=2.03]Epoch 6:   1%|          | 6/1000 [16:30<39:29:53, 143.05s/it, lr=0.001, test_acc=34.1, time=133, train_acc=72.2, train_loss=0.759, val_acc=33.7, val_loss=2.11]Epoch     7: reducing learning rate of group 0 to 5.0000e-04.
Epoch 6:   1%|          | 7/1000 [16:30<38:35:24, 139.90s/it, lr=0.001, test_acc=34.1, time=133, train_acc=72.2, train_loss=0.759, val_acc=33.7, val_loss=2.11]Epoch 7:   1%|          | 7/1000 [16:30<38:35:24, 139.90s/it, lr=0.001, test_acc=34.1, time=133, train_acc=72.2, train_loss=0.759, val_acc=33.7, val_loss=2.11]Epoch 7:   1%|          | 7/1000 [18:42<38:35:24, 139.90s/it, lr=0.0005, test_acc=50.6, time=132, train_acc=72.8, train_loss=0.742, val_acc=50.2, val_loss=1.57]Epoch 7:   1%|          | 8/1000 [18:42<37:54:44, 137.59s/it, lr=0.0005, test_acc=50.6, time=132, train_acc=72.8, train_loss=0.742, val_acc=50.2, val_loss=1.57]Epoch 8:   1%|          | 8/1000 [18:42<37:54:44, 137.59s/it, lr=0.0005, test_acc=50.6, time=132, train_acc=72.8, train_loss=0.742, val_acc=50.2, val_loss=1.57]Epoch 8:   1%|          | 8/1000 [20:54<37:54:44, 137.59s/it, lr=0.0005, test_acc=42.2, time=132, train_acc=73.1, train_loss=0.736, val_acc=41.8, val_loss=1.8] Epoch 8:   1%|          | 9/1000 [20:54<37:25:58, 135.98s/it, lr=0.0005, test_acc=42.2, time=132, train_acc=73.1, train_loss=0.736, val_acc=41.8, val_loss=1.8]Epoch 9:   1%|          | 9/1000 [20:54<37:25:58, 135.98s/it, lr=0.0005, test_acc=42.2, time=132, train_acc=73.1, train_loss=0.736, val_acc=41.8, val_loss=1.8]Epoch 9:   1%|          | 9/1000 [23:06<37:25:58, 135.98s/it, lr=0.0005, test_acc=60.8, time=132, train_acc=73.2, train_loss=0.731, val_acc=60.9, val_loss=1.27]Epoch 9:   1%|          | 10/1000 [23:06<37:03:58, 134.79s/it, lr=0.0005, test_acc=60.8, time=132, train_acc=73.2, train_loss=0.731, val_acc=60.9, val_loss=1.27]Epoch 10:   1%|          | 10/1000 [23:06<37:03:58, 134.79s/it, lr=0.0005, test_acc=60.8, time=132, train_acc=73.2, train_loss=0.731, val_acc=60.9, val_loss=1.27]Epoch 10:   1%|          | 10/1000 [25:19<37:03:58, 134.79s/it, lr=0.0005, test_acc=38.2, time=132, train_acc=73.2, train_loss=0.731, val_acc=37.7, val_loss=1.57]Epoch 10:   1%|          | 11/1000 [25:19<36:49:02, 134.02s/it, lr=0.0005, test_acc=38.2, time=132, train_acc=73.2, train_loss=0.731, val_acc=37.7, val_loss=1.57]Epoch 11:   1%|          | 11/1000 [25:19<36:49:02, 134.02s/it, lr=0.0005, test_acc=38.2, time=132, train_acc=73.2, train_loss=0.731, val_acc=37.7, val_loss=1.57]Epoch 11:   1%|          | 11/1000 [27:31<36:49:02, 134.02s/it, lr=0.0005, test_acc=54.6, time=132, train_acc=73.4, train_loss=0.727, val_acc=53.9, val_loss=1.38]Epoch 11:   1%|          | 12/1000 [27:31<36:37:21, 133.44s/it, lr=0.0005, test_acc=54.6, time=132, train_acc=73.4, train_loss=0.727, val_acc=53.9, val_loss=1.38]Epoch 12:   1%|          | 12/1000 [27:31<36:37:21, 133.44s/it, lr=0.0005, test_acc=54.6, time=132, train_acc=73.4, train_loss=0.727, val_acc=53.9, val_loss=1.38]Epoch 12:   1%|          | 12/1000 [29:43<36:37:21, 133.44s/it, lr=0.0005, test_acc=48.3, time=132, train_acc=73.5, train_loss=0.724, val_acc=47.1, val_loss=1.51]Epoch 12:   1%|▏         | 13/1000 [29:43<36:27:33, 132.98s/it, lr=0.0005, test_acc=48.3, time=132, train_acc=73.5, train_loss=0.724, val_acc=47.1, val_loss=1.51]Epoch 13:   1%|▏         | 13/1000 [29:43<36:27:33, 132.98s/it, lr=0.0005, test_acc=48.3, time=132, train_acc=73.5, train_loss=0.724, val_acc=47.1, val_loss=1.51]Epoch 13:   1%|▏         | 13/1000 [31:55<36:27:33, 132.98s/it, lr=0.0005, test_acc=45.3, time=132, train_acc=73.5, train_loss=0.723, val_acc=44.7, val_loss=1.54]Epoch 13:   1%|▏         | 14/1000 [31:55<36:21:33, 132.75s/it, lr=0.0005, test_acc=45.3, time=132, train_acc=73.5, train_loss=0.723, val_acc=44.7, val_loss=1.54]Epoch 14:   1%|▏         | 14/1000 [31:55<36:21:33, 132.75s/it, lr=0.0005, test_acc=45.3, time=132, train_acc=73.5, train_loss=0.723, val_acc=44.7, val_loss=1.54]Epoch 14:   1%|▏         | 14/1000 [34:07<36:21:33, 132.75s/it, lr=0.0005, test_acc=38.2, time=132, train_acc=73.5, train_loss=0.722, val_acc=37.4, val_loss=2.81]Epoch 14:   2%|▏         | 15/1000 [34:07<36:14:50, 132.48s/it, lr=0.0005, test_acc=38.2, time=132, train_acc=73.5, train_loss=0.722, val_acc=37.4, val_loss=2.81]Epoch 15:   2%|▏         | 15/1000 [34:07<36:14:50, 132.48s/it, lr=0.0005, test_acc=38.2, time=132, train_acc=73.5, train_loss=0.722, val_acc=37.4, val_loss=2.81]Epoch 15:   2%|▏         | 15/1000 [36:19<36:14:50, 132.48s/it, lr=0.0005, test_acc=45.4, time=132, train_acc=73.5, train_loss=0.722, val_acc=44.5, val_loss=1.43]Epoch    16: reducing learning rate of group 0 to 2.5000e-04.
Epoch 15:   2%|▏         | 16/1000 [36:19<36:09:23, 132.28s/it, lr=0.0005, test_acc=45.4, time=132, train_acc=73.5, train_loss=0.722, val_acc=44.5, val_loss=1.43]Epoch 16:   2%|▏         | 16/1000 [36:19<36:09:23, 132.28s/it, lr=0.0005, test_acc=45.4, time=132, train_acc=73.5, train_loss=0.722, val_acc=44.5, val_loss=1.43]Epoch 16:   2%|▏         | 16/1000 [38:30<36:09:23, 132.28s/it, lr=0.00025, test_acc=69.3, time=132, train_acc=74, train_loss=0.71, val_acc=69.1, val_loss=0.86]  Epoch 16:   2%|▏         | 17/1000 [38:31<36:05:30, 132.18s/it, lr=0.00025, test_acc=69.3, time=132, train_acc=74, train_loss=0.71, val_acc=69.1, val_loss=0.86]Epoch 17:   2%|▏         | 17/1000 [38:31<36:05:30, 132.18s/it, lr=0.00025, test_acc=69.3, time=132, train_acc=74, train_loss=0.71, val_acc=69.1, val_loss=0.86]Epoch 17:   2%|▏         | 17/1000 [40:43<36:05:30, 132.18s/it, lr=0.00025, test_acc=48.8, time=132, train_acc=74.2, train_loss=0.705, val_acc=48, val_loss=1.45]Epoch 17:   2%|▏         | 18/1000 [40:43<36:02:32, 132.13s/it, lr=0.00025, test_acc=48.8, time=132, train_acc=74.2, train_loss=0.705, val_acc=48, val_loss=1.45]Epoch 18:   2%|▏         | 18/1000 [40:43<36:02:32, 132.13s/it, lr=0.00025, test_acc=48.8, time=132, train_acc=74.2, train_loss=0.705, val_acc=48, val_loss=1.45]Epoch 18:   2%|▏         | 18/1000 [42:55<36:02:32, 132.13s/it, lr=0.00025, test_acc=58.7, time=133, train_acc=74.3, train_loss=0.702, val_acc=58.1, val_loss=1.12]Epoch 18:   2%|▏         | 19/1000 [42:55<36:02:44, 132.28s/it, lr=0.00025, test_acc=58.7, time=133, train_acc=74.3, train_loss=0.702, val_acc=58.1, val_loss=1.12]Epoch 19:   2%|▏         | 19/1000 [42:55<36:02:44, 132.28s/it, lr=0.00025, test_acc=58.7, time=133, train_acc=74.3, train_loss=0.702, val_acc=58.1, val_loss=1.12]Epoch 19:   2%|▏         | 19/1000 [45:08<36:02:44, 132.28s/it, lr=0.00025, test_acc=59.7, time=133, train_acc=74.3, train_loss=0.702, val_acc=59.5, val_loss=1.11]Epoch 19:   2%|▏         | 20/1000 [45:08<36:02:54, 132.42s/it, lr=0.00025, test_acc=59.7, time=133, train_acc=74.3, train_loss=0.702, val_acc=59.5, val_loss=1.11]Epoch 20:   2%|▏         | 20/1000 [45:08<36:02:54, 132.42s/it, lr=0.00025, test_acc=59.7, time=133, train_acc=74.3, train_loss=0.702, val_acc=59.5, val_loss=1.11]Epoch 20:   2%|▏         | 20/1000 [47:21<36:02:54, 132.42s/it, lr=0.00025, test_acc=58.6, time=133, train_acc=74.4, train_loss=0.7, val_acc=58.1, val_loss=1.2]   Epoch 20:   2%|▏         | 21/1000 [47:21<36:02:20, 132.52s/it, lr=0.00025, test_acc=58.6, time=133, train_acc=74.4, train_loss=0.7, val_acc=58.1, val_loss=1.2]Epoch 21:   2%|▏         | 21/1000 [47:21<36:02:20, 132.52s/it, lr=0.00025, test_acc=58.6, time=133, train_acc=74.4, train_loss=0.7, val_acc=58.1, val_loss=1.2]Epoch 21:   2%|▏         | 21/1000 [49:33<36:02:20, 132.52s/it, lr=0.00025, test_acc=52.3, time=132, train_acc=74.3, train_loss=0.701, val_acc=51.2, val_loss=1.5]Epoch 21:   2%|▏         | 22/1000 [49:33<35:58:24, 132.42s/it, lr=0.00025, test_acc=52.3, time=132, train_acc=74.3, train_loss=0.701, val_acc=51.2, val_loss=1.5]Epoch 22:   2%|▏         | 22/1000 [49:33<35:58:24, 132.42s/it, lr=0.00025, test_acc=52.3, time=132, train_acc=74.3, train_loss=0.701, val_acc=51.2, val_loss=1.5]Epoch 22:   2%|▏         | 22/1000 [51:45<35:58:24, 132.42s/it, lr=0.00025, test_acc=57.6, time=132, train_acc=74.3, train_loss=0.7, val_acc=57.2, val_loss=1.16] Epoch    23: reducing learning rate of group 0 to 1.2500e-04.
Epoch 22:   2%|▏         | 23/1000 [51:45<35:53:43, 132.27s/it, lr=0.00025, test_acc=57.6, time=132, train_acc=74.3, train_loss=0.7, val_acc=57.2, val_loss=1.16]Epoch 23:   2%|▏         | 23/1000 [51:45<35:53:43, 132.27s/it, lr=0.00025, test_acc=57.6, time=132, train_acc=74.3, train_loss=0.7, val_acc=57.2, val_loss=1.16]Epoch 23:   2%|▏         | 23/1000 [53:55<35:53:43, 132.27s/it, lr=0.000125, test_acc=64.9, time=130, train_acc=74.7, train_loss=0.691, val_acc=64.9, val_loss=0.994]Epoch 23:   2%|▏         | 24/1000 [53:55<35:42:25, 131.71s/it, lr=0.000125, test_acc=64.9, time=130, train_acc=74.7, train_loss=0.691, val_acc=64.9, val_loss=0.994]Epoch 24:   2%|▏         | 24/1000 [53:55<35:42:25, 131.71s/it, lr=0.000125, test_acc=64.9, time=130, train_acc=74.7, train_loss=0.691, val_acc=64.9, val_loss=0.994]Epoch 24:   2%|▏         | 24/1000 [56:05<35:42:25, 131.71s/it, lr=0.000125, test_acc=64.8, time=130, train_acc=74.8, train_loss=0.69, val_acc=65, val_loss=0.983]   Epoch 24:   2%|▎         | 25/1000 [56:05<35:30:23, 131.10s/it, lr=0.000125, test_acc=64.8, time=130, train_acc=74.8, train_loss=0.69, val_acc=65, val_loss=0.983]Epoch 25:   2%|▎         | 25/1000 [56:05<35:30:23, 131.10s/it, lr=0.000125, test_acc=64.8, time=130, train_acc=74.8, train_loss=0.69, val_acc=65, val_loss=0.983]Epoch 25:   2%|▎         | 25/1000 [58:14<35:30:23, 131.10s/it, lr=0.000125, test_acc=62.9, time=129, train_acc=74.8, train_loss=0.687, val_acc=62.8, val_loss=1.07]Epoch 25:   3%|▎         | 26/1000 [58:14<35:18:59, 130.53s/it, lr=0.000125, test_acc=62.9, time=129, train_acc=74.8, train_loss=0.687, val_acc=62.8, val_loss=1.07]Epoch 26:   3%|▎         | 26/1000 [58:14<35:18:59, 130.53s/it, lr=0.000125, test_acc=62.9, time=129, train_acc=74.8, train_loss=0.687, val_acc=62.8, val_loss=1.07]Epoch 26:   3%|▎         | 26/1000 [1:00:23<35:18:59, 130.53s/it, lr=0.000125, test_acc=65.5, time=129, train_acc=74.8, train_loss=0.688, val_acc=65.4, val_loss=0.93]Epoch 26:   3%|▎         | 27/1000 [1:00:23<35:07:35, 129.96s/it, lr=0.000125, test_acc=65.5, time=129, train_acc=74.8, train_loss=0.688, val_acc=65.4, val_loss=0.93]Epoch 27:   3%|▎         | 27/1000 [1:00:23<35:07:35, 129.96s/it, lr=0.000125, test_acc=65.5, time=129, train_acc=74.8, train_loss=0.688, val_acc=65.4, val_loss=0.93]Epoch 27:   3%|▎         | 27/1000 [1:02:31<35:07:35, 129.96s/it, lr=0.000125, test_acc=62.8, time=128, train_acc=74.8, train_loss=0.687, val_acc=62, val_loss=1.09]  Epoch 27:   3%|▎         | 28/1000 [1:02:31<34:55:33, 129.36s/it, lr=0.000125, test_acc=62.8, time=128, train_acc=74.8, train_loss=0.687, val_acc=62, val_loss=1.09]Epoch 28:   3%|▎         | 28/1000 [1:02:31<34:55:33, 129.36s/it, lr=0.000125, test_acc=62.8, time=128, train_acc=74.8, train_loss=0.687, val_acc=62, val_loss=1.09]Epoch 28:   3%|▎         | 28/1000 [1:04:37<34:55:33, 129.36s/it, lr=0.000125, test_acc=60.9, time=126, train_acc=74.8, train_loss=0.687, val_acc=61, val_loss=1.08]Epoch    29: reducing learning rate of group 0 to 6.2500e-05.
Epoch 28:   3%|▎         | 29/1000 [1:04:37<34:39:08, 128.47s/it, lr=0.000125, test_acc=60.9, time=126, train_acc=74.8, train_loss=0.687, val_acc=61, val_loss=1.08]Epoch 29:   3%|▎         | 29/1000 [1:04:37<34:39:08, 128.47s/it, lr=0.000125, test_acc=60.9, time=126, train_acc=74.8, train_loss=0.687, val_acc=61, val_loss=1.08]Epoch 29:   3%|▎         | 29/1000 [1:06:43<34:39:08, 128.47s/it, lr=6.25e-5, test_acc=67.7, time=126, train_acc=75, train_loss=0.682, val_acc=67.5, val_loss=0.883]Epoch 29:   3%|▎         | 30/1000 [1:06:43<34:23:44, 127.65s/it, lr=6.25e-5, test_acc=67.7, time=126, train_acc=75, train_loss=0.682, val_acc=67.5, val_loss=0.883]Epoch 30:   3%|▎         | 30/1000 [1:06:43<34:23:44, 127.65s/it, lr=6.25e-5, test_acc=67.7, time=126, train_acc=75, train_loss=0.682, val_acc=67.5, val_loss=0.883]Epoch 30:   3%|▎         | 30/1000 [1:08:49<34:23:44, 127.65s/it, lr=6.25e-5, test_acc=70.8, time=126, train_acc=75.1, train_loss=0.682, val_acc=70.6, val_loss=0.807]Epoch 30:   3%|▎         | 31/1000 [1:08:49<34:13:29, 127.15s/it, lr=6.25e-5, test_acc=70.8, time=126, train_acc=75.1, train_loss=0.682, val_acc=70.6, val_loss=0.807]Epoch 31:   3%|▎         | 31/1000 [1:08:49<34:13:29, 127.15s/it, lr=6.25e-5, test_acc=70.8, time=126, train_acc=75.1, train_loss=0.682, val_acc=70.6, val_loss=0.807]Epoch 31:   3%|▎         | 31/1000 [1:10:53<34:13:29, 127.15s/it, lr=6.25e-5, test_acc=71.5, time=124, train_acc=75.1, train_loss=0.682, val_acc=71.3, val_loss=0.783]Epoch 31:   3%|▎         | 32/1000 [1:10:53<33:56:59, 126.26s/it, lr=6.25e-5, test_acc=71.5, time=124, train_acc=75.1, train_loss=0.682, val_acc=71.3, val_loss=0.783]Epoch 32:   3%|▎         | 32/1000 [1:10:53<33:56:59, 126.26s/it, lr=6.25e-5, test_acc=71.5, time=124, train_acc=75.1, train_loss=0.682, val_acc=71.3, val_loss=0.783]Epoch 32:   3%|▎         | 32/1000 [1:12:56<33:56:59, 126.26s/it, lr=6.25e-5, test_acc=68, time=123, train_acc=75.1, train_loss=0.681, val_acc=67.9, val_loss=0.873]  Epoch 32:   3%|▎         | 33/1000 [1:12:56<33:40:16, 125.35s/it, lr=6.25e-5, test_acc=68, time=123, train_acc=75.1, train_loss=0.681, val_acc=67.9, val_loss=0.873]Epoch 33:   3%|▎         | 33/1000 [1:12:56<33:40:16, 125.35s/it, lr=6.25e-5, test_acc=68, time=123, train_acc=75.1, train_loss=0.681, val_acc=67.9, val_loss=0.873]Epoch 33:   3%|▎         | 33/1000 [1:14:59<33:40:16, 125.35s/it, lr=6.25e-5, test_acc=68.5, time=122, train_acc=75.1, train_loss=0.681, val_acc=68.1, val_loss=0.908]Epoch 33:   3%|▎         | 34/1000 [1:14:59<33:23:41, 124.45s/it, lr=6.25e-5, test_acc=68.5, time=122, train_acc=75.1, train_loss=0.681, val_acc=68.1, val_loss=0.908]Epoch 34:   3%|▎         | 34/1000 [1:14:59<33:23:41, 124.45s/it, lr=6.25e-5, test_acc=68.5, time=122, train_acc=75.1, train_loss=0.681, val_acc=68.1, val_loss=0.908]Epoch 34:   3%|▎         | 34/1000 [1:17:00<33:23:41, 124.45s/it, lr=6.25e-5, test_acc=71.4, time=121, train_acc=75, train_loss=0.682, val_acc=71, val_loss=0.797]    Epoch 34:   4%|▎         | 35/1000 [1:17:00<33:06:33, 123.52s/it, lr=6.25e-5, test_acc=71.4, time=121, train_acc=75, train_loss=0.682, val_acc=71, val_loss=0.797]Epoch 35:   4%|▎         | 35/1000 [1:17:00<33:06:33, 123.52s/it, lr=6.25e-5, test_acc=71.4, time=121, train_acc=75, train_loss=0.682, val_acc=71, val_loss=0.797]Epoch 35:   4%|▎         | 35/1000 [1:19:00<33:06:33, 123.52s/it, lr=6.25e-5, test_acc=63.1, time=120, train_acc=75.1, train_loss=0.68, val_acc=63.1, val_loss=1.14]Epoch 35:   4%|▎         | 36/1000 [1:19:00<32:48:57, 122.55s/it, lr=6.25e-5, test_acc=63.1, time=120, train_acc=75.1, train_loss=0.68, val_acc=63.1, val_loss=1.14]Epoch 36:   4%|▎         | 36/1000 [1:19:00<32:48:57, 122.55s/it, lr=6.25e-5, test_acc=63.1, time=120, train_acc=75.1, train_loss=0.68, val_acc=63.1, val_loss=1.14]Epoch 36:   4%|▎         | 36/1000 [1:20:58<32:48:57, 122.55s/it, lr=6.25e-5, test_acc=72.8, time=118, train_acc=75.1, train_loss=0.682, val_acc=72.6, val_loss=0.75]Epoch 36:   4%|▎         | 37/1000 [1:20:58<32:24:05, 121.13s/it, lr=6.25e-5, test_acc=72.8, time=118, train_acc=75.1, train_loss=0.682, val_acc=72.6, val_loss=0.75]Epoch 37:   4%|▎         | 37/1000 [1:20:58<32:24:05, 121.13s/it, lr=6.25e-5, test_acc=72.8, time=118, train_acc=75.1, train_loss=0.682, val_acc=72.6, val_loss=0.75]Epoch 37:   4%|▎         | 37/1000 [1:22:55<32:24:05, 121.13s/it, lr=6.25e-5, test_acc=69.1, time=117, train_acc=75.1, train_loss=0.68, val_acc=68.9, val_loss=0.854]Epoch 37:   4%|▍         | 38/1000 [1:22:55<32:03:31, 119.97s/it, lr=6.25e-5, test_acc=69.1, time=117, train_acc=75.1, train_loss=0.68, val_acc=68.9, val_loss=0.854]Epoch 38:   4%|▍         | 38/1000 [1:22:55<32:03:31, 119.97s/it, lr=6.25e-5, test_acc=69.1, time=117, train_acc=75.1, train_loss=0.68, val_acc=68.9, val_loss=0.854]Epoch 38:   4%|▍         | 38/1000 [1:24:52<32:03:31, 119.97s/it, lr=6.25e-5, test_acc=70.8, time=117, train_acc=75.2, train_loss=0.678, val_acc=70.6, val_loss=0.803]Epoch 38:   4%|▍         | 39/1000 [1:24:52<31:48:28, 119.16s/it, lr=6.25e-5, test_acc=70.8, time=117, train_acc=75.2, train_loss=0.678, val_acc=70.6, val_loss=0.803]Epoch 39:   4%|▍         | 39/1000 [1:24:52<31:48:28, 119.16s/it, lr=6.25e-5, test_acc=70.8, time=117, train_acc=75.2, train_loss=0.678, val_acc=70.6, val_loss=0.803]Epoch 39:   4%|▍         | 39/1000 [1:26:50<31:48:28, 119.16s/it, lr=6.25e-5, test_acc=70.4, time=117, train_acc=75.1, train_loss=0.679, val_acc=70, val_loss=0.819]  Epoch 39:   4%|▍         | 40/1000 [1:26:50<31:37:59, 118.62s/it, lr=6.25e-5, test_acc=70.4, time=117, train_acc=75.1, train_loss=0.679, val_acc=70, val_loss=0.819]Epoch 40:   4%|▍         | 40/1000 [1:26:50<31:37:59, 118.62s/it, lr=6.25e-5, test_acc=70.4, time=117, train_acc=75.1, train_loss=0.679, val_acc=70, val_loss=0.819]Epoch 40:   4%|▍         | 40/1000 [1:28:47<31:37:59, 118.62s/it, lr=6.25e-5, test_acc=69.8, time=117, train_acc=75.2, train_loss=0.678, val_acc=69.4, val_loss=0.863]Epoch 40:   4%|▍         | 41/1000 [1:28:47<31:29:21, 118.21s/it, lr=6.25e-5, test_acc=69.8, time=117, train_acc=75.2, train_loss=0.678, val_acc=69.4, val_loss=0.863]Epoch 41:   4%|▍         | 41/1000 [1:28:47<31:29:21, 118.21s/it, lr=6.25e-5, test_acc=69.8, time=117, train_acc=75.2, train_loss=0.678, val_acc=69.4, val_loss=0.863]Epoch 41:   4%|▍         | 41/1000 [1:30:45<31:29:21, 118.21s/it, lr=6.25e-5, test_acc=69.1, time=117, train_acc=75.2, train_loss=0.678, val_acc=69, val_loss=0.844]  Epoch 41:   4%|▍         | 42/1000 [1:30:45<31:23:49, 117.98s/it, lr=6.25e-5, test_acc=69.1, time=117, train_acc=75.2, train_loss=0.678, val_acc=69, val_loss=0.844]Epoch 42:   4%|▍         | 42/1000 [1:30:45<31:23:49, 117.98s/it, lr=6.25e-5, test_acc=69.1, time=117, train_acc=75.2, train_loss=0.678, val_acc=69, val_loss=0.844]Epoch 42:   4%|▍         | 42/1000 [1:32:41<31:23:49, 117.98s/it, lr=6.25e-5, test_acc=70.7, time=117, train_acc=75.2, train_loss=0.679, val_acc=70.6, val_loss=0.812]Epoch    43: reducing learning rate of group 0 to 3.1250e-05.
Epoch 42:   4%|▍         | 43/1000 [1:32:42<31:16:51, 117.67s/it, lr=6.25e-5, test_acc=70.7, time=117, train_acc=75.2, train_loss=0.679, val_acc=70.6, val_loss=0.812]Epoch 43:   4%|▍         | 43/1000 [1:32:42<31:16:51, 117.67s/it, lr=6.25e-5, test_acc=70.7, time=117, train_acc=75.2, train_loss=0.679, val_acc=70.6, val_loss=0.812]Epoch 43:   4%|▍         | 43/1000 [1:34:38<31:16:51, 117.67s/it, lr=3.13e-5, test_acc=72.2, time=116, train_acc=75.2, train_loss=0.676, val_acc=72.1, val_loss=0.771]Epoch 43:   4%|▍         | 44/1000 [1:34:38<31:08:35, 117.28s/it, lr=3.13e-5, test_acc=72.2, time=116, train_acc=75.2, train_loss=0.676, val_acc=72.1, val_loss=0.771]Epoch 44:   4%|▍         | 44/1000 [1:34:38<31:08:35, 117.28s/it, lr=3.13e-5, test_acc=72.2, time=116, train_acc=75.2, train_loss=0.676, val_acc=72.1, val_loss=0.771]Epoch 44:   4%|▍         | 44/1000 [1:36:34<31:08:35, 117.28s/it, lr=3.13e-5, test_acc=71.6, time=116, train_acc=75.3, train_loss=0.673, val_acc=71.5, val_loss=0.78] Epoch 44:   4%|▍         | 45/1000 [1:36:34<31:01:54, 116.98s/it, lr=3.13e-5, test_acc=71.6, time=116, train_acc=75.3, train_loss=0.673, val_acc=71.5, val_loss=0.78]Epoch 45:   4%|▍         | 45/1000 [1:36:34<31:01:54, 116.98s/it, lr=3.13e-5, test_acc=71.6, time=116, train_acc=75.3, train_loss=0.673, val_acc=71.5, val_loss=0.78]Epoch 45:   4%|▍         | 45/1000 [1:38:31<31:01:54, 116.98s/it, lr=3.13e-5, test_acc=71.5, time=116, train_acc=75.4, train_loss=0.674, val_acc=71.3, val_loss=0.786]Epoch 45:   5%|▍         | 46/1000 [1:38:31<30:57:03, 116.80s/it, lr=3.13e-5, test_acc=71.5, time=116, train_acc=75.4, train_loss=0.674, val_acc=71.3, val_loss=0.786]Epoch 46:   5%|▍         | 46/1000 [1:38:31<30:57:03, 116.80s/it, lr=3.13e-5, test_acc=71.5, time=116, train_acc=75.4, train_loss=0.674, val_acc=71.3, val_loss=0.786]Epoch 46:   5%|▍         | 46/1000 [1:40:27<30:57:03, 116.80s/it, lr=3.13e-5, test_acc=71.9, time=116, train_acc=75.3, train_loss=0.674, val_acc=71.4, val_loss=0.784]Epoch 46:   5%|▍         | 47/1000 [1:40:27<30:52:43, 116.65s/it, lr=3.13e-5, test_acc=71.9, time=116, train_acc=75.3, train_loss=0.674, val_acc=71.4, val_loss=0.784]Epoch 47:   5%|▍         | 47/1000 [1:40:27<30:52:43, 116.65s/it, lr=3.13e-5, test_acc=71.9, time=116, train_acc=75.3, train_loss=0.674, val_acc=71.4, val_loss=0.784]Epoch 47:   5%|▍         | 47/1000 [1:42:23<30:52:43, 116.65s/it, lr=3.13e-5, test_acc=71.9, time=116, train_acc=75.3, train_loss=0.674, val_acc=72.3, val_loss=0.758]Epoch 47:   5%|▍         | 48/1000 [1:42:23<30:48:58, 116.53s/it, lr=3.13e-5, test_acc=71.9, time=116, train_acc=75.3, train_loss=0.674, val_acc=72.3, val_loss=0.758]Epoch 48:   5%|▍         | 48/1000 [1:42:23<30:48:58, 116.53s/it, lr=3.13e-5, test_acc=71.9, time=116, train_acc=75.3, train_loss=0.674, val_acc=72.3, val_loss=0.758]Epoch 48:   5%|▍         | 48/1000 [1:44:19<30:48:58, 116.53s/it, lr=3.13e-5, test_acc=72.7, time=116, train_acc=75.3, train_loss=0.676, val_acc=72.6, val_loss=0.75] Epoch    49: reducing learning rate of group 0 to 1.5625e-05.
Epoch 48:   5%|▍         | 49/1000 [1:44:19<30:46:11, 116.48s/it, lr=3.13e-5, test_acc=72.7, time=116, train_acc=75.3, train_loss=0.676, val_acc=72.6, val_loss=0.75]Epoch 49:   5%|▍         | 49/1000 [1:44:19<30:46:11, 116.48s/it, lr=3.13e-5, test_acc=72.7, time=116, train_acc=75.3, train_loss=0.676, val_acc=72.6, val_loss=0.75]Epoch 49:   5%|▍         | 49/1000 [1:46:16<30:46:11, 116.48s/it, lr=1.56e-5, test_acc=73.5, time=116, train_acc=75.4, train_loss=0.672, val_acc=73.5, val_loss=0.731]Epoch 49:   5%|▌         | 50/1000 [1:46:16<30:43:26, 116.43s/it, lr=1.56e-5, test_acc=73.5, time=116, train_acc=75.4, train_loss=0.672, val_acc=73.5, val_loss=0.731]Epoch 50:   5%|▌         | 50/1000 [1:46:16<30:43:26, 116.43s/it, lr=1.56e-5, test_acc=73.5, time=116, train_acc=75.4, train_loss=0.672, val_acc=73.5, val_loss=0.731]Epoch 50:   5%|▌         | 50/1000 [1:48:12<30:43:26, 116.43s/it, lr=1.56e-5, test_acc=73.8, time=116, train_acc=75.3, train_loss=0.674, val_acc=73.5, val_loss=0.727]Epoch 50:   5%|▌         | 51/1000 [1:48:12<30:41:14, 116.41s/it, lr=1.56e-5, test_acc=73.8, time=116, train_acc=75.3, train_loss=0.674, val_acc=73.5, val_loss=0.727]Epoch 51:   5%|▌         | 51/1000 [1:48:12<30:41:14, 116.41s/it, lr=1.56e-5, test_acc=73.8, time=116, train_acc=75.3, train_loss=0.674, val_acc=73.5, val_loss=0.727]Epoch 51:   5%|▌         | 51/1000 [1:50:09<30:41:14, 116.41s/it, lr=1.56e-5, test_acc=73.7, time=117, train_acc=75.4, train_loss=0.672, val_acc=73.7, val_loss=0.72] Epoch 51:   5%|▌         | 52/1000 [1:50:09<30:39:56, 116.45s/it, lr=1.56e-5, test_acc=73.7, time=117, train_acc=75.4, train_loss=0.672, val_acc=73.7, val_loss=0.72]Epoch 52:   5%|▌         | 52/1000 [1:50:09<30:39:56, 116.45s/it, lr=1.56e-5, test_acc=73.7, time=117, train_acc=75.4, train_loss=0.672, val_acc=73.7, val_loss=0.72]Epoch 52:   5%|▌         | 52/1000 [1:52:05<30:39:56, 116.45s/it, lr=1.56e-5, test_acc=72.9, time=116, train_acc=75.3, train_loss=0.674, val_acc=72.8, val_loss=0.746]Epoch 52:   5%|▌         | 53/1000 [1:52:05<30:37:05, 116.39s/it, lr=1.56e-5, test_acc=72.9, time=116, train_acc=75.3, train_loss=0.674, val_acc=72.8, val_loss=0.746]Epoch 53:   5%|▌         | 53/1000 [1:52:05<30:37:05, 116.39s/it, lr=1.56e-5, test_acc=72.9, time=116, train_acc=75.3, train_loss=0.674, val_acc=72.8, val_loss=0.746]Epoch 53:   5%|▌         | 53/1000 [1:54:01<30:37:05, 116.39s/it, lr=1.56e-5, test_acc=73.6, time=116, train_acc=75.4, train_loss=0.672, val_acc=73.4, val_loss=0.728]Epoch 53:   5%|▌         | 54/1000 [1:54:01<30:35:07, 116.39s/it, lr=1.56e-5, test_acc=73.6, time=116, train_acc=75.4, train_loss=0.672, val_acc=73.4, val_loss=0.728]Epoch 54:   5%|▌         | 54/1000 [1:54:01<30:35:07, 116.39s/it, lr=1.56e-5, test_acc=73.6, time=116, train_acc=75.4, train_loss=0.672, val_acc=73.4, val_loss=0.728]Epoch 54:   5%|▌         | 54/1000 [1:55:58<30:35:07, 116.39s/it, lr=1.56e-5, test_acc=73.5, time=116, train_acc=75.4, train_loss=0.671, val_acc=73.3, val_loss=0.73] Epoch 54:   6%|▌         | 55/1000 [1:55:58<30:32:51, 116.37s/it, lr=1.56e-5, test_acc=73.5, time=116, train_acc=75.4, train_loss=0.671, val_acc=73.3, val_loss=0.73]Epoch 55:   6%|▌         | 55/1000 [1:55:58<30:32:51, 116.37s/it, lr=1.56e-5, test_acc=73.5, time=116, train_acc=75.4, train_loss=0.671, val_acc=73.3, val_loss=0.73]Epoch 55:   6%|▌         | 55/1000 [1:57:54<30:32:51, 116.37s/it, lr=1.56e-5, test_acc=73.4, time=116, train_acc=75.4, train_loss=0.672, val_acc=73.2, val_loss=0.735]Epoch 55:   6%|▌         | 56/1000 [1:57:54<30:31:07, 116.39s/it, lr=1.56e-5, test_acc=73.4, time=116, train_acc=75.4, train_loss=0.672, val_acc=73.2, val_loss=0.735]Epoch 56:   6%|▌         | 56/1000 [1:57:54<30:31:07, 116.39s/it, lr=1.56e-5, test_acc=73.4, time=116, train_acc=75.4, train_loss=0.672, val_acc=73.2, val_loss=0.735]Epoch 56:   6%|▌         | 56/1000 [1:59:50<30:31:07, 116.39s/it, lr=1.56e-5, test_acc=73.5, time=116, train_acc=75.4, train_loss=0.672, val_acc=73.4, val_loss=0.728]Epoch 56:   6%|▌         | 57/1000 [1:59:50<30:29:06, 116.38s/it, lr=1.56e-5, test_acc=73.5, time=116, train_acc=75.4, train_loss=0.672, val_acc=73.4, val_loss=0.728]Epoch 57:   6%|▌         | 57/1000 [1:59:50<30:29:06, 116.38s/it, lr=1.56e-5, test_acc=73.5, time=116, train_acc=75.4, train_loss=0.672, val_acc=73.4, val_loss=0.728]Epoch 57:   6%|▌         | 57/1000 [2:01:47<30:29:06, 116.38s/it, lr=1.56e-5, test_acc=73.5, time=116, train_acc=75.4, train_loss=0.671, val_acc=73.3, val_loss=0.731]Epoch    58: reducing learning rate of group 0 to 7.8125e-06.

!! LR SMALLER OR EQUAL TO MIN LR THRESHOLD.
Epoch 57:   6%|▌         | 57/1000 [2:01:47<33:34:49, 128.20s/it, lr=1.56e-5, test_acc=73.5, time=116, train_acc=75.4, train_loss=0.671, val_acc=73.3, val_loss=0.731]
Test Accuracy: 73.4817
Train Accuracy: 75.0085
Convergence Time (Epochs): 57.0000
TOTAL TIME TAKEN: 7387.9645s
AVG TIME PER EPOCH: 125.9753s

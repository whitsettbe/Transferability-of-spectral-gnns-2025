I'm echoing to stdout
I'm echoing to stderr
My JobID is 57142821
I have 8 CPUs on node r108u25n01
Using backend: pytorch
cuda not available
[I] Loading dataset ZINC...
train, test, val sizes : 10000 1000 1000
[I] Finished loading.
[I] Data load time: 5.3309s
Dataset: ZINC,
Model: EigvalFilters

params={'seed': 41, 'epochs': 1000, 'batch_size': 128, 'init_lr': 0.001, 'lr_reduce_factor': 0.5, 'lr_schedule_patience': 5, 'min_lr': 1e-05, 'weight_decay': 0.0, 'print_epoch_interval': 5, 'max_time': 48}

net_params={'L': 4, 'hidden_dim': 106, 'out_dim': 106, 'residual': True, 'readout': 'mean', 'k': 4, 'in_feat_dropout': 0.0, 'dropout': 0.0, 'graph_norm': True, 'batch_norm': True, 'self_loop': False, 'subtype': 'rational_vec', 'normalized_laplacian': False, 'post_normalized': True, 'eigval_norm': 'scale(0,2)_all', 'num_eigs': 64, 'bias_mode': 'spatial', 'eigval_hidden_dim': 5, 'eigval_num_hidden_layer': 3, 'l1_reg': 0.0, 'l2_reg': 0.0, 'gen_reg': 2.0, 'eigmod': 'import_csv', 'eigInFiles': {'train': 'supp_data/molecules/zinc_train_haar.csv', 'test': 'supp_data/molecules/zinc_test_haar.csv', 'val': 'supp_data/molecules/zinc_val_haar.csv'}, 'fixMissingPhi1': False, 'extraOrtho': False, 'doublePrecision': True, 'device': device(type='cpu'), 'gpu_id': 0, 'batch_size': 128, 'biases': False, 'num_atom_type': 28, 'num_bond_type': 4, 'total_param': -1}


Total Parameters: -1


Training Graphs:  10000
Validation Graphs:  1000
Test Graphs:  1000
  0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/1000 [10:13<?, ?it/s, lr=0.001, test_MAE=1.71, time=614, train_MAE=1.26, train_loss=1.31, val_MAE=1.67, val_loss=1.73]Epoch 0:   0%|          | 1/1000 [10:13<170:20:14, 613.83s/it, lr=0.001, test_MAE=1.71, time=614, train_MAE=1.26, train_loss=1.31, val_MAE=1.67, val_loss=1.73]Epoch 1:   0%|          | 1/1000 [10:13<170:20:14, 613.83s/it, lr=0.001, test_MAE=1.71, time=614, train_MAE=1.26, train_loss=1.31, val_MAE=1.67, val_loss=1.73]Epoch 1:   0%|          | 1/1000 [20:06<170:20:14, 613.83s/it, lr=0.001, test_MAE=1.42, time=593, train_MAE=0.838, train_loss=0.9, val_MAE=1.35, val_loss=1.41]Epoch 1:   0%|          | 2/1000 [20:06<168:24:46, 607.50s/it, lr=0.001, test_MAE=1.42, time=593, train_MAE=0.838, train_loss=0.9, val_MAE=1.35, val_loss=1.41]Epoch 2:   0%|          | 2/1000 [20:06<168:24:46, 607.50s/it, lr=0.001, test_MAE=1.42, time=593, train_MAE=0.838, train_loss=0.9, val_MAE=1.35, val_loss=1.41]Epoch 2:   0%|          | 2/1000 [30:01<168:24:46, 607.50s/it, lr=0.001, test_MAE=1.73, time=595, train_MAE=0.716, train_loss=0.782, val_MAE=1.64, val_loss=1.7]Epoch 2:   0%|          | 3/1000 [30:01<167:14:05, 603.86s/it, lr=0.001, test_MAE=1.73, time=595, train_MAE=0.716, train_loss=0.782, val_MAE=1.64, val_loss=1.7]Epoch 3:   0%|          | 3/1000 [30:01<167:14:05, 603.86s/it, lr=0.001, test_MAE=1.73, time=595, train_MAE=0.716, train_loss=0.782, val_MAE=1.64, val_loss=1.7]Epoch 3:   0%|          | 3/1000 [39:42<167:14:05, 603.86s/it, lr=0.001, test_MAE=0.964, time=581, train_MAE=0.685, train_loss=0.753, val_MAE=0.902, val_loss=0.971]Epoch 3:   0%|          | 4/1000 [39:42<165:08:47, 596.92s/it, lr=0.001, test_MAE=0.964, time=581, train_MAE=0.685, train_loss=0.753, val_MAE=0.902, val_loss=0.971]Epoch 4:   0%|          | 4/1000 [39:42<165:08:47, 596.92s/it, lr=0.001, test_MAE=0.964, time=581, train_MAE=0.685, train_loss=0.753, val_MAE=0.902, val_loss=0.971]Epoch 4:   0%|          | 4/1000 [49:16<165:08:47, 596.92s/it, lr=0.001, test_MAE=0.973, time=574, train_MAE=0.654, train_loss=0.722, val_MAE=0.918, val_loss=0.987]Epoch 4:   0%|          | 5/1000 [49:16<163:03:15, 589.94s/it, lr=0.001, test_MAE=0.973, time=574, train_MAE=0.654, train_loss=0.722, val_MAE=0.918, val_loss=0.987]Epoch 5:   0%|          | 5/1000 [49:16<163:03:15, 589.94s/it, lr=0.001, test_MAE=0.973, time=574, train_MAE=0.654, train_loss=0.722, val_MAE=0.918, val_loss=0.987]Epoch 5:   0%|          | 5/1000 [58:49<163:03:15, 589.94s/it, lr=0.001, test_MAE=0.77, time=573, train_MAE=0.642, train_loss=0.711, val_MAE=0.693, val_loss=0.763] Epoch 5:   1%|          | 6/1000 [58:49<161:31:39, 585.01s/it, lr=0.001, test_MAE=0.77, time=573, train_MAE=0.642, train_loss=0.711, val_MAE=0.693, val_loss=0.763]Epoch 6:   1%|          | 6/1000 [58:49<161:31:39, 585.01s/it, lr=0.001, test_MAE=0.77, time=573, train_MAE=0.642, train_loss=0.711, val_MAE=0.693, val_loss=0.763]Epoch 6:   1%|          | 6/1000 [1:08:24<161:31:39, 585.01s/it, lr=0.001, test_MAE=0.88, time=575, train_MAE=0.621, train_loss=0.692, val_MAE=0.701, val_loss=0.772]Epoch 6:   1%|          | 7/1000 [1:08:24<160:31:25, 581.96s/it, lr=0.001, test_MAE=0.88, time=575, train_MAE=0.621, train_loss=0.692, val_MAE=0.701, val_loss=0.772]Epoch 7:   1%|          | 7/1000 [1:08:24<160:31:25, 581.96s/it, lr=0.001, test_MAE=0.88, time=575, train_MAE=0.621, train_loss=0.692, val_MAE=0.701, val_loss=0.772]Epoch 7:   1%|          | 7/1000 [1:18:12<160:31:25, 581.96s/it, lr=0.001, test_MAE=0.741, time=588, train_MAE=0.636, train_loss=0.707, val_MAE=0.68, val_loss=0.752]Epoch 7:   1%|          | 8/1000 [1:18:12<160:52:50, 583.84s/it, lr=0.001, test_MAE=0.741, time=588, train_MAE=0.636, train_loss=0.707, val_MAE=0.68, val_loss=0.752]Epoch 8:   1%|          | 8/1000 [1:18:12<160:52:50, 583.84s/it, lr=0.001, test_MAE=0.741, time=588, train_MAE=0.636, train_loss=0.707, val_MAE=0.68, val_loss=0.752]Epoch 8:   1%|          | 8/1000 [1:28:00<160:52:50, 583.84s/it, lr=0.001, test_MAE=0.746, time=588, train_MAE=0.621, train_loss=0.694, val_MAE=0.707, val_loss=0.779]Epoch 8:   1%|          | 9/1000 [1:28:00<161:02:30, 585.02s/it, lr=0.001, test_MAE=0.746, time=588, train_MAE=0.621, train_loss=0.694, val_MAE=0.707, val_loss=0.779]Epoch 9:   1%|          | 9/1000 [1:28:00<161:02:30, 585.02s/it, lr=0.001, test_MAE=0.746, time=588, train_MAE=0.621, train_loss=0.694, val_MAE=0.707, val_loss=0.779]Epoch 9:   1%|          | 9/1000 [1:38:01<161:02:30, 585.02s/it, lr=0.001, test_MAE=0.713, time=601, train_MAE=0.606, train_loss=0.679, val_MAE=0.649, val_loss=0.723]Epoch 9:   1%|          | 10/1000 [1:38:01<162:13:32, 589.91s/it, lr=0.001, test_MAE=0.713, time=601, train_MAE=0.606, train_loss=0.679, val_MAE=0.649, val_loss=0.723]Epoch 10:   1%|          | 10/1000 [1:38:01<162:13:32, 589.91s/it, lr=0.001, test_MAE=0.713, time=601, train_MAE=0.606, train_loss=0.679, val_MAE=0.649, val_loss=0.723]Epoch 10:   1%|          | 10/1000 [1:48:01<162:13:32, 589.91s/it, lr=0.001, test_MAE=0.736, time=599, train_MAE=0.61, train_loss=0.684, val_MAE=0.691, val_loss=0.766] Epoch 10:   1%|          | 11/1000 [1:48:01<162:51:14, 592.80s/it, lr=0.001, test_MAE=0.736, time=599, train_MAE=0.61, train_loss=0.684, val_MAE=0.691, val_loss=0.766]Epoch 11:   1%|          | 11/1000 [1:48:01<162:51:14, 592.80s/it, lr=0.001, test_MAE=0.736, time=599, train_MAE=0.61, train_loss=0.684, val_MAE=0.691, val_loss=0.766]Epoch 11:   1%|          | 11/1000 [1:58:00<162:51:14, 592.80s/it, lr=0.001, test_MAE=0.657, time=599, train_MAE=0.599, train_loss=0.674, val_MAE=0.607, val_loss=0.683]Epoch 11:   1%|          | 12/1000 [1:58:00<163:13:43, 594.76s/it, lr=0.001, test_MAE=0.657, time=599, train_MAE=0.599, train_loss=0.674, val_MAE=0.607, val_loss=0.683]Epoch 12:   1%|          | 12/1000 [1:58:00<163:13:43, 594.76s/it, lr=0.001, test_MAE=0.657, time=599, train_MAE=0.599, train_loss=0.674, val_MAE=0.607, val_loss=0.683]Epoch 12:   1%|          | 12/1000 [2:07:48<163:13:43, 594.76s/it, lr=0.001, test_MAE=0.726, time=588, train_MAE=0.601, train_loss=0.677, val_MAE=0.668, val_loss=0.744]Epoch 12:   1%|▏         | 13/1000 [2:07:48<162:29:34, 592.68s/it, lr=0.001, test_MAE=0.726, time=588, train_MAE=0.601, train_loss=0.677, val_MAE=0.668, val_loss=0.744]Epoch 13:   1%|▏         | 13/1000 [2:07:48<162:29:34, 592.68s/it, lr=0.001, test_MAE=0.726, time=588, train_MAE=0.601, train_loss=0.677, val_MAE=0.668, val_loss=0.744]Epoch 13:   1%|▏         | 13/1000 [2:17:33<162:29:34, 592.68s/it, lr=0.001, test_MAE=0.643, time=585, train_MAE=0.596, train_loss=0.673, val_MAE=0.596, val_loss=0.673]Epoch 13:   1%|▏         | 14/1000 [2:17:33<161:42:39, 590.43s/it, lr=0.001, test_MAE=0.643, time=585, train_MAE=0.596, train_loss=0.673, val_MAE=0.596, val_loss=0.673]Epoch 14:   1%|▏         | 14/1000 [2:17:33<161:42:39, 590.43s/it, lr=0.001, test_MAE=0.643, time=585, train_MAE=0.596, train_loss=0.673, val_MAE=0.596, val_loss=0.673]Epoch 14:   1%|▏         | 14/1000 [2:27:17<161:42:39, 590.43s/it, lr=0.001, test_MAE=0.751, time=584, train_MAE=0.585, train_loss=0.662, val_MAE=0.706, val_loss=0.783]Epoch 14:   2%|▏         | 15/1000 [2:27:17<161:00:08, 588.44s/it, lr=0.001, test_MAE=0.751, time=584, train_MAE=0.585, train_loss=0.662, val_MAE=0.706, val_loss=0.783]Epoch 15:   2%|▏         | 15/1000 [2:27:17<161:00:08, 588.44s/it, lr=0.001, test_MAE=0.751, time=584, train_MAE=0.585, train_loss=0.662, val_MAE=0.706, val_loss=0.783]Epoch 15:   2%|▏         | 15/1000 [2:37:01<161:00:08, 588.44s/it, lr=0.001, test_MAE=0.743, time=584, train_MAE=0.586, train_loss=0.664, val_MAE=0.682, val_loss=0.761]Epoch 15:   2%|▏         | 16/1000 [2:37:01<160:28:33, 587.11s/it, lr=0.001, test_MAE=0.743, time=584, train_MAE=0.586, train_loss=0.664, val_MAE=0.682, val_loss=0.761]Epoch 16:   2%|▏         | 16/1000 [2:37:01<160:28:33, 587.11s/it, lr=0.001, test_MAE=0.743, time=584, train_MAE=0.586, train_loss=0.664, val_MAE=0.682, val_loss=0.761]Epoch 16:   2%|▏         | 16/1000 [2:46:45<160:28:33, 587.11s/it, lr=0.001, test_MAE=0.672, time=583, train_MAE=0.594, train_loss=0.674, val_MAE=0.636, val_loss=0.715]Epoch 16:   2%|▏         | 17/1000 [2:46:45<160:00:38, 586.00s/it, lr=0.001, test_MAE=0.672, time=583, train_MAE=0.594, train_loss=0.674, val_MAE=0.636, val_loss=0.715]Epoch 17:   2%|▏         | 17/1000 [2:46:45<160:00:38, 586.00s/it, lr=0.001, test_MAE=0.672, time=583, train_MAE=0.594, train_loss=0.674, val_MAE=0.636, val_loss=0.715]Epoch 17:   2%|▏         | 17/1000 [2:56:30<160:00:38, 586.00s/it, lr=0.001, test_MAE=0.71, time=585, train_MAE=0.585, train_loss=0.665, val_MAE=0.679, val_loss=0.76]  Epoch 17:   2%|▏         | 18/1000 [2:56:30<159:47:34, 585.80s/it, lr=0.001, test_MAE=0.71, time=585, train_MAE=0.585, train_loss=0.665, val_MAE=0.679, val_loss=0.76]Epoch 18:   2%|▏         | 18/1000 [2:56:30<159:47:34, 585.80s/it, lr=0.001, test_MAE=0.71, time=585, train_MAE=0.585, train_loss=0.665, val_MAE=0.679, val_loss=0.76]Epoch 18:   2%|▏         | 18/1000 [3:06:16<159:47:34, 585.80s/it, lr=0.001, test_MAE=0.675, time=586, train_MAE=0.583, train_loss=0.664, val_MAE=0.641, val_loss=0.722]Epoch 18:   2%|▏         | 19/1000 [3:06:16<159:38:38, 585.85s/it, lr=0.001, test_MAE=0.675, time=586, train_MAE=0.583, train_loss=0.664, val_MAE=0.641, val_loss=0.722]Epoch 19:   2%|▏         | 19/1000 [3:06:16<159:38:38, 585.85s/it, lr=0.001, test_MAE=0.675, time=586, train_MAE=0.583, train_loss=0.664, val_MAE=0.641, val_loss=0.722]Epoch 19:   2%|▏         | 19/1000 [3:16:02<159:38:38, 585.85s/it, lr=0.001, test_MAE=0.958, time=586, train_MAE=0.576, train_loss=0.658, val_MAE=0.906, val_loss=0.988]Epoch    20: reducing learning rate of group 0 to 5.0000e-04.
Epoch 19:   2%|▏         | 20/1000 [3:16:02<159:31:11, 585.99s/it, lr=0.001, test_MAE=0.958, time=586, train_MAE=0.576, train_loss=0.658, val_MAE=0.906, val_loss=0.988]Epoch 20:   2%|▏         | 20/1000 [3:16:02<159:31:11, 585.99s/it, lr=0.001, test_MAE=0.958, time=586, train_MAE=0.576, train_loss=0.658, val_MAE=0.906, val_loss=0.988]Epoch 20:   2%|▏         | 20/1000 [3:25:51<159:31:11, 585.99s/it, lr=0.0005, test_MAE=0.62, time=589, train_MAE=0.556, train_loss=0.638, val_MAE=0.584, val_loss=0.665]Epoch 20:   2%|▏         | 21/1000 [3:25:51<159:37:37, 586.98s/it, lr=0.0005, test_MAE=0.62, time=589, train_MAE=0.556, train_loss=0.638, val_MAE=0.584, val_loss=0.665]Epoch 21:   2%|▏         | 21/1000 [3:25:51<159:37:37, 586.98s/it, lr=0.0005, test_MAE=0.62, time=589, train_MAE=0.556, train_loss=0.638, val_MAE=0.584, val_loss=0.665]Epoch 21:   2%|▏         | 21/1000 [3:35:41<159:37:37, 586.98s/it, lr=0.0005, test_MAE=1.34, time=590, train_MAE=0.55, train_loss=0.631, val_MAE=1.35, val_loss=1.43]   Epoch 21:   2%|▏         | 22/1000 [3:35:41<159:41:35, 587.83s/it, lr=0.0005, test_MAE=1.34, time=590, train_MAE=0.55, train_loss=0.631, val_MAE=1.35, val_loss=1.43]Epoch 22:   2%|▏         | 22/1000 [3:35:41<159:41:35, 587.83s/it, lr=0.0005, test_MAE=1.34, time=590, train_MAE=0.55, train_loss=0.631, val_MAE=1.35, val_loss=1.43]Epoch 22:   2%|▏         | 22/1000 [3:45:30<159:41:35, 587.83s/it, lr=0.0005, test_MAE=0.605, time=589, train_MAE=0.553, train_loss=0.634, val_MAE=0.551, val_loss=0.632]Epoch 22:   2%|▏         | 23/1000 [3:45:30<159:37:29, 588.18s/it, lr=0.0005, test_MAE=0.605, time=589, train_MAE=0.553, train_loss=0.634, val_MAE=0.551, val_loss=0.632]Epoch 23:   2%|▏         | 23/1000 [3:45:30<159:37:29, 588.18s/it, lr=0.0005, test_MAE=0.605, time=589, train_MAE=0.553, train_loss=0.634, val_MAE=0.551, val_loss=0.632]Epoch 23:   2%|▏         | 23/1000 [3:55:19<159:37:29, 588.18s/it, lr=0.0005, test_MAE=0.681, time=589, train_MAE=0.551, train_loss=0.631, val_MAE=0.639, val_loss=0.72] Epoch 23:   2%|▏         | 24/1000 [3:55:19<159:30:29, 588.35s/it, lr=0.0005, test_MAE=0.681, time=589, train_MAE=0.551, train_loss=0.631, val_MAE=0.639, val_loss=0.72]Epoch 24:   2%|▏         | 24/1000 [3:55:19<159:30:29, 588.35s/it, lr=0.0005, test_MAE=0.681, time=589, train_MAE=0.551, train_loss=0.631, val_MAE=0.639, val_loss=0.72]Epoch 24:   2%|▏         | 24/1000 [4:05:08<159:30:29, 588.35s/it, lr=0.0005, test_MAE=0.613, time=589, train_MAE=0.55, train_loss=0.63, val_MAE=0.563, val_loss=0.644] Epoch 24:   2%|▎         | 25/1000 [4:05:08<159:22:56, 588.49s/it, lr=0.0005, test_MAE=0.613, time=589, train_MAE=0.55, train_loss=0.63, val_MAE=0.563, val_loss=0.644]Epoch 25:   2%|▎         | 25/1000 [4:05:08<159:22:56, 588.49s/it, lr=0.0005, test_MAE=0.613, time=589, train_MAE=0.55, train_loss=0.63, val_MAE=0.563, val_loss=0.644]Epoch 25:   2%|▎         | 25/1000 [4:14:53<159:22:56, 588.49s/it, lr=0.0005, test_MAE=0.643, time=585, train_MAE=0.55, train_loss=0.631, val_MAE=0.601, val_loss=0.681]Epoch 25:   3%|▎         | 26/1000 [4:14:53<158:57:46, 587.54s/it, lr=0.0005, test_MAE=0.643, time=585, train_MAE=0.55, train_loss=0.631, val_MAE=0.601, val_loss=0.681]Epoch 26:   3%|▎         | 26/1000 [4:14:53<158:57:46, 587.54s/it, lr=0.0005, test_MAE=0.643, time=585, train_MAE=0.55, train_loss=0.631, val_MAE=0.601, val_loss=0.681]Epoch 26:   3%|▎         | 26/1000 [4:24:46<158:57:46, 587.54s/it, lr=0.0005, test_MAE=0.851, time=593, train_MAE=0.552, train_loss=0.632, val_MAE=0.789, val_loss=0.87]Epoch 26:   3%|▎         | 27/1000 [4:24:46<159:13:24, 589.11s/it, lr=0.0005, test_MAE=0.851, time=593, train_MAE=0.552, train_loss=0.632, val_MAE=0.789, val_loss=0.87]Epoch 27:   3%|▎         | 27/1000 [4:24:46<159:13:24, 589.11s/it, lr=0.0005, test_MAE=0.851, time=593, train_MAE=0.552, train_loss=0.632, val_MAE=0.789, val_loss=0.87]Epoch 27:   3%|▎         | 27/1000 [4:34:33<159:13:24, 589.11s/it, lr=0.0005, test_MAE=0.827, time=587, train_MAE=0.548, train_loss=0.629, val_MAE=0.791, val_loss=0.872]Epoch 27:   3%|▎         | 28/1000 [4:34:33<158:54:07, 588.53s/it, lr=0.0005, test_MAE=0.827, time=587, train_MAE=0.548, train_loss=0.629, val_MAE=0.791, val_loss=0.872]Epoch 28:   3%|▎         | 28/1000 [4:34:33<158:54:07, 588.53s/it, lr=0.0005, test_MAE=0.827, time=587, train_MAE=0.548, train_loss=0.629, val_MAE=0.791, val_loss=0.872]Epoch 28:   3%|▎         | 28/1000 [4:44:17<158:54:07, 588.53s/it, lr=0.0005, test_MAE=0.791, time=584, train_MAE=0.545, train_loss=0.625, val_MAE=0.74, val_loss=0.821] Epoch    29: reducing learning rate of group 0 to 2.5000e-04.
Epoch 28:   3%|▎         | 29/1000 [4:44:17<158:22:12, 587.16s/it, lr=0.0005, test_MAE=0.791, time=584, train_MAE=0.545, train_loss=0.625, val_MAE=0.74, val_loss=0.821]Epoch 29:   3%|▎         | 29/1000 [4:44:17<158:22:12, 587.16s/it, lr=0.0005, test_MAE=0.791, time=584, train_MAE=0.545, train_loss=0.625, val_MAE=0.74, val_loss=0.821]Epoch 29:   3%|▎         | 29/1000 [4:53:58<158:22:12, 587.16s/it, lr=0.00025, test_MAE=0.646, time=581, train_MAE=0.528, train_loss=0.609, val_MAE=0.616, val_loss=0.697]Epoch 29:   3%|▎         | 30/1000 [4:53:58<157:43:13, 585.35s/it, lr=0.00025, test_MAE=0.646, time=581, train_MAE=0.528, train_loss=0.609, val_MAE=0.616, val_loss=0.697]Epoch 30:   3%|▎         | 30/1000 [4:53:58<157:43:13, 585.35s/it, lr=0.00025, test_MAE=0.646, time=581, train_MAE=0.528, train_loss=0.609, val_MAE=0.616, val_loss=0.697]Epoch 30:   3%|▎         | 30/1000 [5:03:49<157:43:13, 585.35s/it, lr=0.00025, test_MAE=0.621, time=590, train_MAE=0.519, train_loss=0.599, val_MAE=0.576, val_loss=0.656]Epoch 30:   3%|▎         | 30/1000 [5:03:49<163:43:26, 607.64s/it, lr=0.00025, test_MAE=0.621, time=590, train_MAE=0.519, train_loss=0.599, val_MAE=0.576, val_loss=0.656]
Traceback (most recent call last):
  File "main_molecules_graph_regression.py", line 506, in <module>
    main()
  File "main_molecules_graph_regression.py", line 503, in main
    train_val_pipeline(MODEL_NAME, dataset, params, net_params, dirs)
  File "main_molecules_graph_regression.py", line 214, in train_val_pipeline
    torch.save(model.state_dict(), '{}.pkl'.format(ckpt_dir + "/epoch_" + str(epoch)))
  File "/home/bsw38/.conda/envs/benchmark_gnn_2/lib/python3.7/site-packages/torch/serialization.py", line 361, in save
    with _open_file_like(f, 'wb') as opened_file:
  File "/home/bsw38/.conda/envs/benchmark_gnn_2/lib/python3.7/site-packages/torch/serialization.py", line 229, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/bsw38/.conda/envs/benchmark_gnn_2/lib/python3.7/site-packages/torch/serialization.py", line 210, in __init__
    super(_open_file, self).__init__(open(name, mode))
OSError: [Errno 122] Disk quota exceeded: 'out/molecules_graph_regression/checkpoints/EigvalFilters_ZINC_GPU0_41_11h19m16s_on_Aug_06_2025/RUN_/epoch_30.pkl'
